{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Soften - Old nets policy notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cmCpRMBKgvDB",
        "wuDM6Ydokv05",
        "bvD4EPGJLNZn",
        "jICRWSARLUuD"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb7fc96f17684b5eb1e2f0b44825548a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9677b3275f984177aeae18936ebc40eb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dce3ecb022d44118993c7781c0d7c2d6",
              "IPY_MODEL_239a7d7b1a114836a0e343f0254a4755"
            ]
          }
        },
        "9677b3275f984177aeae18936ebc40eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dce3ecb022d44118993c7781c0d7c2d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7ff63a4c204b4a0391ba36c1fe609139",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_62020032795d4af491d2c6ac95689c87"
          }
        },
        "239a7d7b1a114836a0e343f0254a4755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eaf9c1fdf3504b82be1a3df12a3667a7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:20&lt;00:00, 34022156.91it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ea216c3a70d24fd8a7043d7c3c23412e"
          }
        },
        "7ff63a4c204b4a0391ba36c1fe609139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "62020032795d4af491d2c6ac95689c87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eaf9c1fdf3504b82be1a3df12a3667a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ea216c3a70d24fd8a7043d7c3c23412e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manuelemacchia/incremental-learning-image-classification/blob/master/dist_targets_analisys.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NzqxHIh4OCdW"
      },
      "source": [
        "# Incremental learning on image classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAmDjtS9LyH2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "2f2a1eb2-81b9-48da-bd2a-e18ffcb6e789"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jul  7 18:27:20 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wBHSznCZxpNB"
      },
      "source": [
        "## Libraries and packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4eQ6O12jxMFf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "outputId": "099ee4c8-d720-4340-c77b-a470478a58f1"
      },
      "source": [
        "!pip3 install 'torch==1.4.0'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 22kB/s \n",
            "\u001b[31mERROR: torchvision 0.6.1+cu101 has requirement torch==1.5.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "  Found existing installation: torch 1.5.1+cu101\n",
            "    Uninstalling torch-1.5.1+cu101:\n",
            "      Successfully uninstalled torch-1.5.1+cu101\n",
            "Successfully installed torch-1.4.0\n",
            "Collecting torchvision==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/90/6141bf41f5655c78e24f40f710fdd4f8a8aff6c8b7c6f0328240f649bdbe/torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (7.0.0)\n",
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.4.0)\n",
            "Installing collected packages: torchvision\n",
            "  Found existing installation: torchvision 0.6.1+cu101\n",
            "    Uninstalling torchvision-0.6.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.6.1+cu101\n",
            "Successfully installed torchvision-0.5.0\n",
            "Collecting Pillow-SIMD\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/6a/30d21c886293cca3755b8e55de34137a5068b77eba1c0644d3632080516b/Pillow-SIMD-7.0.0.post3.tar.gz (630kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 7.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Pillow-SIMD\n",
            "  Building wheel for Pillow-SIMD (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pillow-SIMD: filename=Pillow_SIMD-7.0.0.post3-cp36-cp36m-linux_x86_64.whl size=1110226 sha256=e703058402555d695475699e7d6bbbfb5cfde76ef6fe7e075081a992ac6d2451\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/ac/4f/4cdf8febba528e5f1b09fc58d5181e1c12ed1e8655dcd583b8\n",
            "Successfully built Pillow-SIMD\n",
            "Installing collected packages: Pillow-SIMD\n",
            "Successfully installed Pillow-SIMD-7.0.0.post3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xAYXtIdpx0Yy",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import urllib\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, Subset, DataLoader, ConcatDataset\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet34\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "09iWc_oCotu2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "3a67cc56-1b37-44b2-a65f-2ecee2843c88"
      },
      "source": [
        "# GitHub credentials for cloning private repository\n",
        "username = ''\n",
        "password = ''\n",
        "\n",
        "# Download packages from repository\n",
        "password = urllib.parse.quote(password)\n",
        "!git clone https://$username:$password@github.com/manuelemacchia/incremental-learning-image-classification.git\n",
        "password = ''\n",
        "\n",
        "!mv -v incremental-learning-image-classification/* .\n",
        "!rm -rf incremental-learning-image-classification README.md"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'incremental-learning-image-classification'...\n",
            "remote: Enumerating objects: 181, done.\u001b[K\n",
            "remote: Counting objects: 100% (181/181), done.\u001b[K\n",
            "remote: Compressing objects: 100% (162/162), done.\u001b[K\n",
            "remote: Total 1070 (delta 96), reused 51 (delta 17), pack-reused 889\u001b[K\n",
            "Receiving objects: 100% (1070/1070), 6.47 MiB | 10.41 MiB/s, done.\n",
            "Resolving deltas: 100% (597/597), done.\n",
            "renamed 'incremental-learning-image-classification/data' -> './data'\n",
            "renamed 'incremental-learning-image-classification/dist_targets_analisys_notebook.ipynb' -> './dist_targets_analisys_notebook.ipynb'\n",
            "renamed 'incremental-learning-image-classification/icarlSVM.ipynb' -> './icarlSVM.ipynb'\n",
            "renamed 'incremental-learning-image-classification/joint_training.ipynb' -> './joint_training.ipynb'\n",
            "renamed 'incremental-learning-image-classification/losses' -> './losses'\n",
            "renamed 'incremental-learning-image-classification/model' -> './model'\n",
            "renamed 'incremental-learning-image-classification/notebook.ipynb' -> './notebook.ipynb'\n",
            "renamed 'incremental-learning-image-classification/point5_notebook.ipynb' -> './point5_notebook.ipynb'\n",
            "renamed 'incremental-learning-image-classification/README.md' -> './README.md'\n",
            "renamed 'incremental-learning-image-classification/report' -> './report'\n",
            "renamed 'incremental-learning-image-classification/representation_drift_analisys.ipynb' -> './representation_drift_analisys.ipynb'\n",
            "renamed 'incremental-learning-image-classification/results' -> './results'\n",
            "renamed 'incremental-learning-image-classification/studies-classifier.ipynb' -> './studies-classifier.ipynb'\n",
            "renamed 'incremental-learning-image-classification/tsne notebook.ipynb' -> './tsne notebook.ipynb'\n",
            "renamed 'incremental-learning-image-classification/utils' -> './utils'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QPLViftqtC3I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8fe35454-d665-42c4-de44-eaf219eeb3b0"
      },
      "source": [
        "from data.cifar100 import Cifar100\n",
        "from model.resnet_cifar import resnet32\n",
        "from model.manager import Manager\n",
        "from model.icarl import Exemplars\n",
        "# from model.icarl import iCaRL\n",
        "from utils import plot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j12pgffMR6Qv"
      },
      "source": [
        "## Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JwE0x8gkSisn",
        "colab": {}
      },
      "source": [
        "# Directories\n",
        "DATA_DIR = 'data'       # Directory where the dataset will be downloaded\n",
        "\n",
        "# Settings\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# Dataset\n",
        "\n",
        "RANDOM_STATE = None\n",
        "\n",
        "RANDOM_STATES = [658, 423, 422]      # For reproducibility of results                        \n",
        "                                     # Note: different random states give very different\n",
        "                                     # splits and therefore very different results.\n",
        "\n",
        "NUM_CLASSES = 100       # Total number of classes\n",
        "NUM_BATCHES = 10\n",
        "CLASS_BATCH_SIZE = 10   # Size of batch of classes for incremental learning\n",
        "\n",
        "VAL_SIZE = 0.1          # Proportion of validation set with respect to training set (between 0 and 1)\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE = 64         # Batch size (iCaRL sets this to 128)\n",
        "LR = 2                  # Initial learning rate\n",
        "                       \n",
        "MOMENTUM = 0.9          # Momentum for stochastic gradient descent (SGD)\n",
        "WEIGHT_DECAY = 1e-5     # Weight decay from iCaRL\n",
        "\n",
        "NUM_RUNS = 3            # Number of runs of every method\n",
        "                        # Note: this should be at least 3 to have a fair benchmark\n",
        "\n",
        "NUM_EPOCHS = 70         # Total number of training epochs\n",
        "MILESTONES = [49, 63]   # Step down policy from iCaRL (MultiStepLR)\n",
        "                        # Decrease the learning rate by gamma at each milestone\n",
        "GAMMA = 0.2             # Gamma factor from iCaRL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF0ypxGognNR",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y9Oq44dxgmPN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "eb7fc96f17684b5eb1e2f0b44825548a",
            "9677b3275f984177aeae18936ebc40eb",
            "dce3ecb022d44118993c7781c0d7c2d6",
            "239a7d7b1a114836a0e343f0254a4755",
            "7ff63a4c204b4a0391ba36c1fe609139",
            "62020032795d4af491d2c6ac95689c87",
            "eaf9c1fdf3504b82be1a3df12a3667a7",
            "ea216c3a70d24fd8a7043d7c3c23412e"
          ]
        },
        "outputId": "939b0131-2c1e-4de2-a562-e51d7fac24cb"
      },
      "source": [
        "train_subsets = [[] for i in range(NUM_RUNS)]\n",
        "val_subsets = [[] for i in range(NUM_RUNS)]\n",
        "test_subsets = [[] for i in range(NUM_RUNS)]\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "    for split_i in range(CLASS_BATCH_SIZE):\n",
        "        if run_i+split_i == 0: # Download dataset only at first instantiation\n",
        "            download = True\n",
        "        else:\n",
        "            download = False\n",
        "\n",
        "        # Create CIFAR100 dataset\n",
        "        train_dataset = Cifar100(DATA_DIR, train=True, download=download, random_state=RANDOM_STATES[run_i], transform=train_transform)\n",
        "        test_dataset = Cifar100(DATA_DIR, train=False, download=False, random_state=RANDOM_STATES[run_i], transform=test_transform)\n",
        "    \n",
        "        # Subspace of CIFAR100 of 10 classes\n",
        "        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i]) \n",
        "        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n",
        "\n",
        "        # Define train and validation indices\n",
        "        train_indices, val_indices = train_dataset.train_val_split(VAL_SIZE, RANDOM_STATES[run_i])\n",
        "\n",
        "        # Define subsets\n",
        "        train_subsets[run_i].append(Subset(train_dataset, train_indices))\n",
        "        val_subsets[run_i].append(Subset(train_dataset, val_indices))\n",
        "        test_subsets[run_i].append(test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb7fc96f17684b5eb1e2f0b44825548a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4mf04UjEgmPG",
        "colab": {}
      },
      "source": [
        "# Transformations for Learning Without Forgetting\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwyiPrAZw3L9",
        "colab_type": "text"
      },
      "source": [
        "## iCaRL for Distillation targets analisys"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h-Fy3D0xAqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import numpy as np\n",
        "import numpy.ma as ma\n",
        "from math import floor\n",
        "from copy import deepcopy\n",
        "import random\n",
        "\n",
        "sigmoid = nn.Sigmoid() # Sigmoid function\n",
        "\n",
        "NUM_EXAM_EXEMPLARS = 10\n",
        "\n",
        "class Exemplars(torch.utils.data.Dataset):\n",
        "    def __init__(self, exemplars, transform=None):\n",
        "        # exemplars = [\n",
        "        #     [ex0_class0, ex1_class0, ex2_class0, ...],\n",
        "        #     [ex0_class1, ex1_class1, ex2_class1, ...],\n",
        "        #     ...\n",
        "        #     [ex0_classN, ex1_classN, ex2_classN, ...]\n",
        "        # ]\n",
        "\n",
        "        self.dataset = []\n",
        "        self.targets = []\n",
        "\n",
        "        for y, exemplar_y in enumerate(exemplars):\n",
        "            self.dataset.extend(exemplar_y)\n",
        "            self.targets.extend([y] * len(exemplar_y))\n",
        "\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image = self.dataset[index]\n",
        "        target = self.targets[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "class iCaRL:\n",
        "    \"\"\"Implement iCaRL, a strategy for simultaneously learning classifiers and a\n",
        "    feature representation in the class-incremental setting.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device, net, lr, momentum, weight_decay, milestones, gamma, num_epochs, batch_size, train_transform, test_transform):\n",
        "        self.device = device\n",
        "        self.net = net\n",
        "\n",
        "        # Set hyper-parameters\n",
        "        self.LR = lr\n",
        "        self.MOMENTUM = momentum\n",
        "        self.WEIGHT_DECAY = weight_decay\n",
        "        self.MILESTONES = milestones\n",
        "        self.GAMMA = gamma\n",
        "        self.NUM_EPOCHS = num_epochs\n",
        "        self.BATCH_SIZE = batch_size\n",
        "        \n",
        "        # Set transformations\n",
        "        self.train_transform = train_transform\n",
        "        self.test_transform = test_transform\n",
        "\n",
        "        # List of exemplar sets. Each set contains memory_size/num_classes exemplars\n",
        "        # with num_classes the number of classes seen until now by the network.\n",
        "        self.exemplars = []\n",
        "\n",
        "        self.exemplars_rand = []\n",
        "\n",
        "        # Initialize the copy of the old network, used to compute outputs of the\n",
        "        # previous network for the distillation loss, to None. This is useful to\n",
        "        # correctly apply the first function when training the network for the\n",
        "        # first time.\n",
        "        self.old_net = None\n",
        "        self.old_nets = []\n",
        "\n",
        "        # store saved features in a dictionary where key = split,\n",
        "        # value = [last_net_features, old_nets_features]\n",
        "        self.examined_exemplars_loader = None\n",
        "        self.examined_exemplars_features = dict()\n",
        "\n",
        "        # Maximum number of exemplars\n",
        "        self.memory_size = 2000\n",
        "    \n",
        "        # Loss function\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        # If True, test on the best model found (e.g., minimize loss). If False,\n",
        "        # test on the last model build (of the last epoch).\n",
        "        self.VALIDATE = False\n",
        "\n",
        "    def classify(self, batch, train_dataset=None):\n",
        "        \"\"\"Mean-of-exemplars classifier used to classify images into the set of\n",
        "        classes observed so far.\n",
        "\n",
        "        Args:\n",
        "            batch (torch.tensor): batch to classify\n",
        "        Returns:\n",
        "            label (int): class label assigned to the image\n",
        "        \"\"\"\n",
        "\n",
        "        batch_features = self.extract_features(batch) # (batch size, 64)\n",
        "        for i in range(batch_features.size(0)):\n",
        "            batch_features[i] = batch_features[i]/batch_features[i].norm() # Normalize sample feature representation\n",
        "        batch_features = batch_features.to(self.device)\n",
        "\n",
        "        if self.cached_means is None:\n",
        "            print(\"Computing mean of exemplars... \", end=\"\")\n",
        "\n",
        "            self.cached_means = []\n",
        "\n",
        "            # Number of known classes\n",
        "            num_classes = len(self.exemplars)\n",
        "\n",
        "            # Compute the means of classes with all the data available,\n",
        "            # including training data which contains samples belonging to\n",
        "            # the latest 10 classes. This will remove noise from the mean\n",
        "            # estimate, improving the results.\n",
        "            if train_dataset is not None:\n",
        "                train_features_list = [[] for _ in range(10)]\n",
        "\n",
        "                for train_sample, label in train_dataset:\n",
        "                    features = self.extract_features(train_sample, batch=False, transform=self.test_transform)\n",
        "                    features = features/features.norm()\n",
        "                    train_features_list[label % 10].append(features)\n",
        "\n",
        "            # Compute means of exemplars for all known classes\n",
        "            for y in range(num_classes):\n",
        "                if (train_dataset is not None) and (y in range(num_classes-10, num_classes)):\n",
        "                    features_list = train_features_list[y % 10]\n",
        "                else:\n",
        "                    features_list = []\n",
        "\n",
        "                for exemplar in self.exemplars[y]:\n",
        "                    features = self.extract_features(exemplar, batch=False, transform=self.test_transform)\n",
        "                    features = features/features.norm() # Normalize the feature representation of the exemplar\n",
        "                    features_list.append(features)\n",
        "                \n",
        "                features_list = torch.stack(features_list)\n",
        "                class_means = features_list.mean(dim=0)\n",
        "                class_means = class_means/class_means.norm() # Normalize the class means\n",
        "\n",
        "                self.cached_means.append(class_means)\n",
        "            \n",
        "            self.cached_means = torch.stack(self.cached_means).to(self.device)\n",
        "            print(\"done\")\n",
        "\n",
        "        preds = []\n",
        "        for i in range(batch_features.size(0)):\n",
        "            f_arg = torch.norm(batch_features[i] - self.cached_means, dim=1)\n",
        "            preds.append(torch.argmin(f_arg))\n",
        "        \n",
        "        return torch.stack(preds)\n",
        "    \n",
        "    def classify_rand(self, batch, train_dataset=None):\n",
        "        \"\"\"Mean-of-exemplars classifier used to classify images into the set of\n",
        "        classes observed so far.\n",
        "\n",
        "        Args:\n",
        "            batch (torch.tensor): batch to classify\n",
        "        Returns:\n",
        "            label (int): class label assigned to the image\n",
        "        \"\"\"\n",
        "\n",
        "        batch_features = self.extract_features(batch) # (batch size, 64)\n",
        "        for i in range(batch_features.size(0)):\n",
        "            batch_features[i] = batch_features[i]/batch_features[i].norm() # Normalize sample feature representation\n",
        "        batch_features = batch_features.to(self.device)\n",
        "\n",
        "        if self.cached_means is None:\n",
        "            print(\"Computing mean of exemplars... \", end=\"\")\n",
        "\n",
        "            self.cached_means = []\n",
        "\n",
        "            # Number of known classes\n",
        "            num_classes = len(self.exemplars_rand)\n",
        "\n",
        "            # Compute the means of classes with all the data available,\n",
        "            # including training data which contains samples belonging to\n",
        "            # the latest 10 classes. This will remove noise from the mean\n",
        "            # estimate, improving the results.\n",
        "            if train_dataset is not None:\n",
        "                train_features_list = [[] for _ in range(10)]\n",
        "\n",
        "                for train_sample, label in train_dataset:\n",
        "                    features = self.extract_features(train_sample, batch=False, transform=self.test_transform)\n",
        "                    features = features/features.norm()\n",
        "                    train_features_list[label % 10].append(features)\n",
        "\n",
        "            # Compute means of exemplars for all known classes\n",
        "            for y in range(num_classes):\n",
        "                if (train_dataset is not None) and (y in range(num_classes-10, num_classes)):\n",
        "                    features_list = train_features_list[y % 10]\n",
        "                else:\n",
        "                    features_list = []\n",
        "\n",
        "                for exemplar in self.exemplars_rand[y]:\n",
        "                    features = self.extract_features(exemplar, batch=False, transform=self.test_transform)\n",
        "                    features = features/features.norm() # Normalize the feature representation of the exemplar\n",
        "                    features_list.append(features)\n",
        "                \n",
        "                features_list = torch.stack(features_list)\n",
        "                class_means = features_list.mean(dim=0)\n",
        "                class_means = class_means/class_means.norm() # Normalize the class means\n",
        "\n",
        "                self.cached_means.append(class_means)\n",
        "            \n",
        "            self.cached_means = torch.stack(self.cached_means).to(self.device)\n",
        "            print(\"done\")\n",
        "\n",
        "        preds = []\n",
        "        for i in range(batch_features.size(0)):\n",
        "            f_arg = torch.norm(batch_features[i] - self.cached_means, dim=1)\n",
        "            preds.append(torch.argmin(f_arg))\n",
        "        \n",
        "        return torch.stack(preds)\n",
        "    \n",
        "    def extract_features(self, sample, batch=True, transform=None):\n",
        "        \"\"\"Extract features from single sample or from batch.\n",
        "        \n",
        "        Args:\n",
        "            sample (PIL image or torch.tensor): sample(s) from which to\n",
        "                extract features\n",
        "            batch (bool): if True, sample is a torch.tensor containing a batch\n",
        "                of images with dimensions (batch_size, 3, 32, 32)\n",
        "            transform: transformations to apply to the PIL image before\n",
        "                processing\n",
        "        Returns:\n",
        "            features: torch.tensor, 1-D of dimension 64 for single samples or\n",
        "                2-D of dimension (batch_size, 64) for batch\n",
        "        \"\"\"\n",
        "\n",
        "        assert not (batch is False and transform is None), \"if a PIL image is passed to extract_features, a transform must be defined\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)\n",
        "        if self.old_net is not None: \n",
        "          self.old_net.train(False)\n",
        "          for i in range(len(self.old_nets)):\n",
        "            self.old_nets[i] = self.old_nets[i].train(False)\n",
        "\n",
        "\n",
        "        if batch is False: # Treat sample as single PIL image\n",
        "            sample = transform(sample)\n",
        "            sample = sample.unsqueeze(0) # https://stackoverflow.com/a/59566009/6486336\n",
        "\n",
        "        sample = sample.to(self.device)\n",
        "\n",
        "        if self.VALIDATE:\n",
        "            features = self.best_net.features(sample)\n",
        "        else:\n",
        "            features = self.net.features(sample)\n",
        "\n",
        "        if batch is False:\n",
        "            features = features[0]\n",
        "\n",
        "        return features\n",
        "\n",
        "    def incremental_train(self, split, train_dataset, val_dataset):\n",
        "        \"\"\"Adjust internal knowledge based on the additional information\n",
        "        available in the new observations.\n",
        "\n",
        "        Args:\n",
        "            split (int): current split number, counting from zero\n",
        "            train_dataset: dataset for training the model\n",
        "            val_dataset: dataset for validating the model\n",
        "        Returns:\n",
        "            train_logs: tuple of four metrics (train_loss, train_accuracy,\n",
        "            val_loss, val_accuracy) obtained during network training\n",
        "        \"\"\"\n",
        "\n",
        "        self.split = split\n",
        "\n",
        "        if split is not 0:\n",
        "            # Increment the number of output nodes for the new network by 10\n",
        "            self.increment_classes(10)\n",
        "            last_net_preds, old_nets_preds= self.save_exemplars_features()\n",
        "            # update features dictionary for analisys\n",
        "            self.examined_exemplars_features[self.split] = [last_net_preds, old_nets_preds]\n",
        "\n",
        "        # Improve network parameters upon receiving new classes. Effectively\n",
        "        # train a new network starting from the current network parameters.\n",
        "        train_logs = self.update_representation(train_dataset, val_dataset)\n",
        "\n",
        "        # Compute the number of exemplars per class\n",
        "        num_classes = self.output_neurons_count()\n",
        "        m = floor(self.memory_size / num_classes)\n",
        "\n",
        "        print(f\"Target number of exemplars per class: {m}\")\n",
        "        print(f\"Target total number of exemplars: {m*num_classes}\")\n",
        "\n",
        "        # Reduce pre-existing exemplar sets in order to fit new exemplars\n",
        "        for y in range(len(self.exemplars)):\n",
        "            self.exemplars[y] = self.reduce_exemplar_set(self.exemplars[y], m)\n",
        "\n",
        "        for y in range(len(self.exemplars_rand)):\n",
        "            self.exemplars_rand[y] = self.reduce_exemplar_set(self.exemplars[y], m)\n",
        "\n",
        "        # Construct exemplar set for new classes\n",
        "        new_exemplars = self.construct_exemplar_set(train_dataset, m)\n",
        "        self.exemplars.extend(new_exemplars)\n",
        "        \n",
        "\n",
        "        new_exemplars = self.construct_exemplar_set_rand(train_dataset, m)\n",
        "        self.exemplars_rand.extend(new_exemplars)\n",
        "\n",
        "        return train_logs\n",
        "\n",
        "\n",
        "    def save_exemplars_features(self):\n",
        "      # Take predictions on old_classes (num_classes-10) with\n",
        "      # 1. self.old_net\n",
        "      # 2. self.old_nets, keeping prediction of each net over the classes it's been trained on\n",
        "      # Predictions are taken on exemplars of a single class, self.examined_exemplas, which are the same\n",
        "      # across all the training procedure from split 0 to split 9\n",
        "      # Predictions must be passed into a sigmoid since we want to compare the targets seen by the\n",
        "      # currently trained net\n",
        "\n",
        "      dataiter = iter(self.examined_exemplars_loader)\n",
        "      images, _ = dataiter.next()\n",
        "      images = images.to(self.device)\n",
        "\n",
        "      last_net_preds = sigmoid(self.old_net(images))\n",
        "\n",
        "      old_nets_preds = sigmoid(self.old_nets[0](images))\n",
        "      \n",
        "      for i in range(1, len(self.old_nets[1:])):\n",
        "        old_nets_preds = torch.cat( (old_nets_preds, sigmoid(self.old_nets[i](images))[:, i*10:] ), dim=1)\n",
        "      \n",
        "      if(old_nets_preds.size() != last_net_preds.size()):\n",
        "        print(\"Error: The two predictions matrices must be of the same size\")\n",
        "\n",
        "\n",
        "      return last_net_preds, old_nets_preds\n",
        "      \n",
        "\n",
        "    def update_representation(self, train_dataset, val_dataset):\n",
        "        \"\"\"Update the parameters of the network.\n",
        "\n",
        "        Args:\n",
        "            train_dataset: dataset for training the model\n",
        "            val_dataset: dataset for validating the model\n",
        "        Returns:\n",
        "            train_logs: tuple of four metrics (train_loss, train_accuracy,\n",
        "            val_loss, val_accuracy) obtained during network training\n",
        "        \"\"\"\n",
        "\n",
        "        # Combine the new training data with existing exemplars.\n",
        "        print(f\"Length of exemplars set: {sum([len(self.exemplars[y]) for y in range(len(self.exemplars))])}\")\n",
        "        exemplars_dataset = Exemplars(self.exemplars, self.train_transform)\n",
        "        train_dataset_with_exemplars = ConcatDataset([exemplars_dataset, train_dataset])\n",
        "\n",
        "        # Train the network on combined dataset\n",
        "        train_logs = self.train(train_dataset_with_exemplars, val_dataset) # @todo: include exemplars in validation set?\n",
        "\n",
        "        # Keep a copy of the current network in order to compute its outputs for\n",
        "        # the distillation loss while the new network is being trained.\n",
        "        self.old_net = deepcopy(self.net)\n",
        "        self.old_nets.append(self.old_net)\n",
        "\n",
        "\n",
        "        return train_logs\n",
        "\n",
        "    def construct_exemplar_set(self, dataset, m):\n",
        "        \"\"\"...\n",
        "\n",
        "        Args:\n",
        "            dataset: dataset containing a split (samples from 10 classes) from\n",
        "                which to take exemplars\n",
        "            m (int): target number of exemplars per class\n",
        "        Returns:\n",
        "            exemplars: list of samples extracted from the dataset\n",
        "        \"\"\"\n",
        "\n",
        "        dataset.dataset.disable_transform()\n",
        "\n",
        "        samples = [[] for _ in range(10)]\n",
        "        for image, label in dataset:\n",
        "            label = label % 10 # Map labels to 0-9 range\n",
        "            samples[label].append(image)\n",
        "\n",
        "        dataset.dataset.enable_transform()\n",
        "\n",
        "        # Initialize exemplar sets\n",
        "        exemplars = [[] for _ in range(10)]\n",
        "\n",
        "        # Iterate over classes\n",
        "        for y in range(10):\n",
        "            print(f\"Extracting exemplars from class {y} of current split... \", end=\"\")\n",
        "\n",
        "\n",
        "            # Transform samples to tensors and apply normalization\n",
        "            transformed_samples = torch.zeros((len(samples[y]), 3, 32, 32)).to(self.device)\n",
        "            for i in range(len(transformed_samples)):\n",
        "                transformed_samples[i] = self.test_transform(samples[y][i])\n",
        "\n",
        "            # Extract features from samples\n",
        "            samples_features = self.extract_features(transformed_samples).to(self.device)\n",
        "\n",
        "            # Compute the feature mean of the current class\n",
        "            features_mean = samples_features.mean(dim=0)\n",
        "\n",
        "            # Initializes indices vector, containing the index of each exemplar chosen\n",
        "            idx = []\n",
        "\n",
        "            # See iCaRL algorithm 4\n",
        "            for k in range(1, m+1): # k = 1, ..., m -- Choose m exemplars\n",
        "                if k == 1: # No exemplars chosen yet, sum to 0 vector\n",
        "                    f_sum = torch.zeros(64).to(self.device)\n",
        "                else: # Sum of features of all exemplars chosen until now (j = 1, ..., k-1)\n",
        "                    f_sum = samples_features[idx].sum(dim=0)\n",
        "\n",
        "                # Compute argument of argmin function\n",
        "                f_arg = torch.norm(features_mean - 1/k * samples_features + f_sum, dim=1)\n",
        "\n",
        "                # Mask exemplars that were already taken, as we do not want to store the\n",
        "                # same exemplar more than once\n",
        "                mask = np.zeros(len(f_arg), int)\n",
        "                mask[idx] = 1\n",
        "                f_arg_masked = ma.masked_array(f_arg.cpu().detach().numpy(), mask=mask)\n",
        "\n",
        "                # Compute the nearest available exemplar\n",
        "                exemplar_idx = np.argmin(f_arg_masked)\n",
        "\n",
        "                idx.append(exemplar_idx)\n",
        "            \n",
        "            # Save exemplars to exemplar set\n",
        "            for i in idx:\n",
        "                exemplars[y].append(samples[y][i])\n",
        "\n",
        "            print(f\"Extracted {len(exemplars[y])} exemplars.\")\n",
        "                \n",
        "            # take first ten exemplars of first class, only once during the\n",
        "            # whole incremental training procedure\n",
        "            if self.split == 0 and y == 0:\n",
        "              examined_exemplars = [exemplars[y][:NUM_EXAM_EXEMPLARS]] # add a dimension\n",
        "\n",
        "              examined_exemplars_dataset = Exemplars(examined_exemplars, self.train_transform)\n",
        "              self.examined_exemplars_loader = DataLoader(examined_exemplars_dataset,\n",
        "                                                          batch_size = NUM_EXAM_EXEMPLARS)\n",
        "              print(\"Saved {} exemplars for features analysis\".format(len(examined_exemplars[0])))\n",
        "                \n",
        "            \n",
        "            \n",
        "        return exemplars\n",
        "\n",
        "    def construct_exemplar_set_rand(self, dataset, m):\n",
        "        \"\"\"Randomly sample m elements from a dataset without replacement.\n",
        "\n",
        "        Args:\n",
        "            dataset: dataset containing a split (samples from 10 classes) from\n",
        "                which to take exemplars\n",
        "            m (int): target number of exemplars per class\n",
        "        Returns:\n",
        "            exemplars: list of samples extracted from the dataset\n",
        "        \"\"\"\n",
        "\n",
        "        dataset.dataset.disable_transform()\n",
        "\n",
        "        samples = [[] for _ in range(10)]\n",
        "        for image, label in dataset:\n",
        "            label = label % 10 # Map labels to 0-9 range\n",
        "            samples[label].append(image)\n",
        "\n",
        "        dataset.dataset.enable_transform()\n",
        "\n",
        "        exemplars = [[] for _ in range(10)]\n",
        "\n",
        "        for y in range(10):\n",
        "            print(f\"Randomly extracting exemplars from class {y} of current split... \", end=\"\")\n",
        "\n",
        "            # Randomly choose m samples from samples[y] without replacement\n",
        "            exemplars[y] = random.sample(samples[y], m)\n",
        "\n",
        "            print(f\"Extracted {len(exemplars[y])} exemplars.\")\n",
        "\n",
        "        return exemplars\n",
        "\n",
        "    def reduce_exemplar_set(self, exemplar_set, m):\n",
        "        \"\"\"Procedure for removing exemplars from a given set.\n",
        "\n",
        "        Args:\n",
        "            exemplar_set (set): set of exemplars belonging to a certain class\n",
        "            m (int): target number of exemplars\n",
        "        Returns:\n",
        "            exemplar_set: reduced exemplar set\n",
        "        \"\"\"\n",
        "\n",
        "        return exemplar_set[:m]\n",
        "\n",
        "    def train(self, train_dataset, val_dataset):\n",
        "        \"\"\"Train the network for a specified number of epochs, and save\n",
        "        the best performing model on the validation set.\n",
        "        \n",
        "        Args:\n",
        "            train_dataset: dataset for training the model\n",
        "            val_dataset: dataset for validating the model\n",
        "        Returns: train_logs: tuple of four metrics (train_loss, train_accuracy,\n",
        "            val_loss, val_accuracy) obtained during network training. If\n",
        "            validation is enabled, return scores of the best epoch, otherwise\n",
        "            return scores of the last epoch.\n",
        "        \"\"\"\n",
        "\n",
        "        # Define the optimization algorithm\n",
        "        parameters_to_optimize = self.net.parameters()\n",
        "        self.optimizer = optim.SGD(parameters_to_optimize, \n",
        "                                   lr=self.LR,\n",
        "                                   momentum=self.MOMENTUM,\n",
        "                                   weight_decay=self.WEIGHT_DECAY)\n",
        "        \n",
        "        # Define the learning rate decaying policy\n",
        "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer,\n",
        "                                                        milestones=self.MILESTONES,\n",
        "                                                        gamma=self.GAMMA)\n",
        "\n",
        "        # Create DataLoaders for training and validation\n",
        "        self.train_dataloader = DataLoader(train_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "        self.val_dataloader = DataLoader(val_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "        # Send networks to chosen device\n",
        "        self.net = self.net.to(self.device)\n",
        "        if self.old_net is not None: \n",
        "          self.old_net = self.old_net.to(self.device)\n",
        "          for i in range(len(self.old_nets)):\n",
        "            self.old_nets[i] = self.old_nets[i].to(self.device)\n",
        "\n",
        "        cudnn.benchmark  # Calling this optimizes runtime\n",
        "\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.best_val_accuracy = 0\n",
        "        self.best_train_loss = float('inf')\n",
        "        self.best_train_accuracy = 0\n",
        "        \n",
        "        self.best_net = None\n",
        "        self.best_epoch = -1\n",
        "\n",
        "        for epoch in range(self.NUM_EPOCHS):\n",
        "            # Run an epoch (start counting form 1)\n",
        "            train_loss, train_accuracy = self.do_epoch(epoch+1)\n",
        "        \n",
        "            # Validate after each epoch \n",
        "            val_loss, val_accuracy = self.validate()    \n",
        "\n",
        "            # Validation criterion: best net is the one that minimizes the loss\n",
        "            # on the validation set.\n",
        "            if self.VALIDATE and val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.best_val_accuracy = val_accuracy\n",
        "                self.best_train_loss = train_loss\n",
        "                self.best_train_accuracy = train_accuracy\n",
        "\n",
        "                self.best_net = deepcopy(self.net)\n",
        "                self.best_epoch = epoch\n",
        "                print(\"Best model updated\")\n",
        "\n",
        "        if self.VALIDATE:\n",
        "            val_loss = self.best_val_loss\n",
        "            val_accuracy = self.best_val_accuracy\n",
        "            train_loss = self.best_train_loss\n",
        "            train_accuracy = self.best_train_accuracy\n",
        "\n",
        "            print(f\"Best model found at epoch {self.best_epoch+1}\")\n",
        "\n",
        "        return train_loss, train_accuracy, val_loss, val_accuracy\n",
        "    \n",
        "    def do_epoch(self, current_epoch):\n",
        "        \"\"\"Trains model for one epoch.\n",
        "        \n",
        "        Args:\n",
        "            current_epoch (int): current epoch number (begins from 1)\n",
        "        Returns:\n",
        "            train_loss: average training loss over all batches of the\n",
        "                current epoch.\n",
        "            train_accuracy: training accuracy of the current epoch over\n",
        "                all samples.\n",
        "        \"\"\"\n",
        "\n",
        "        # Set the current network in training mode\n",
        "        self.net.train()\n",
        "        if self.old_net is not None: \n",
        "          self.old_net.train(False)\n",
        "          for i in range(len(self.old_nets)):\n",
        "            self.old_nets[i] = self.old_nets[i].train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)\n",
        "\n",
        "        running_train_loss = 0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        batch_idx = 0\n",
        "\n",
        "        print(f\"Epoch: {current_epoch}, LR: {self.scheduler.get_last_lr()}\")\n",
        "\n",
        "        for images, labels in self.train_dataloader:\n",
        "            loss, corrects = self.do_batch(images, labels)\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "            running_corrects += corrects\n",
        "            total += labels.size(0)\n",
        "            batch_idx += 1\n",
        "\n",
        "        self.scheduler.step()\n",
        "\n",
        "        # Calculate average scores\n",
        "        train_loss = running_train_loss / batch_idx # Average over all batches\n",
        "        train_accuracy = running_corrects / float(total) # Average over all samples\n",
        "\n",
        "        print(f\"Train loss: {train_loss}, Train accuracy: {train_accuracy}\")\n",
        "\n",
        "        return train_loss, train_accuracy\n",
        "\n",
        "    def do_batch(self, batch, labels):\n",
        "        \"\"\"Train network for a batch. Loss is applied here.\n",
        "\n",
        "        Args:\n",
        "            batch: batch of data used for training the network\n",
        "            labels: targets of the batch\n",
        "        Returns:\n",
        "            loss: output of the criterion applied\n",
        "            running_corrects: number of correctly classified elements\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        # Zero-ing the gradients\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        # One-hot encoding of labels of the new training data (new classes)\n",
        "        # Size: batch size (rows) by number of classes seen until now (columns)\n",
        "        #\n",
        "        # e.g., suppose we have four images in a batch, and each incremental\n",
        "        #   step adds three new classes. At the second step, the one-hot\n",
        "        #   encoding may return the following tensor:\n",
        "        #\n",
        "        #       tensor([[0., 0., 0., 1., 0., 0.],   # image 0 (label 3)\n",
        "        #               [0., 0., 0., 0., 1., 0.],   # image 1 (label 4)\n",
        "        #               [0., 0., 0., 0., 0., 1.],   # image 2 (label 5)\n",
        "        #               [0., 0., 0., 0., 1., 0.]])  # image 3 (label 4)\n",
        "        #\n",
        "        #   The first three elements of each vector will always be 0, as the\n",
        "        #   new training batch does not contain images belonging to classes\n",
        "        #   already seen in previous steps.\n",
        "        #\n",
        "        #   The last three elements of each vector will contain the actual\n",
        "        #   information about the class of each image (one-hot encoding of the\n",
        "        #   label). Therefore, we slice the tensor and remove the columns \n",
        "        #   related to old classes (all zeros).\n",
        "        num_classes = self.output_neurons_count() # Number of classes seen until now, including new classes\n",
        "        one_hot_labels = self.to_onehot(labels)[:, num_classes-10:num_classes]\n",
        "\n",
        "        if self.old_net is None:\n",
        "            # Network is training for the first time, so we only apply the\n",
        "            # classification loss.\n",
        "            targets = one_hot_labels\n",
        "\n",
        "        else:\n",
        "            # Old net forward pass. We compute the outputs of the old network\n",
        "            # and apply a sigmoid function. These are used in the distillation\n",
        "            # loss. We discard the output of the new neurons, as they are not\n",
        "            # considered in the distillation loss.\n",
        "            old_net_outputs = sigmoid(self.old_net(batch))[:, :num_classes-10]\n",
        "\n",
        "\n",
        "            # Concatenate the outputs of the old network and the one-hot encoded\n",
        "            # labels along dimension 1 (columns).\n",
        "            # \n",
        "            # Each row refers to an image in the training set, and contains:\n",
        "            # - the output of the old network for that image, used by the\n",
        "            #   distillation loss\n",
        "            # - the one-hot label of the image, used by the classification loss\n",
        "            targets = torch.cat((old_net_outputs, one_hot_labels), dim=1)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = self.net(batch)\n",
        "        loss = self.criterion(outputs, targets)\n",
        "\n",
        "        # Get predictions\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Accuracy over NEW IMAGES, not over all images\n",
        "        running_corrects = torch.sum(preds == labels.data).data.item() \n",
        "\n",
        "        # Backward pass: computes gradients\n",
        "        loss.backward()\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss, running_corrects\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\"Validate the model.\n",
        "        \n",
        "        Returns:\n",
        "            val_loss: average loss function computed on the network outputs\n",
        "                of the validation set (val_dataloader).\n",
        "            val_accuracy: accuracy computed on the validation set.\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)\n",
        "\n",
        "        running_val_loss = 0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        batch_idx = 0\n",
        "\n",
        "        for images, labels in self.val_dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # One hot encoding of new task labels \n",
        "            one_hot_labels = self.to_onehot(labels)\n",
        "\n",
        "            # New net forward pass\n",
        "            outputs = self.net(images)  \n",
        "            loss = self.criterion(outputs, one_hot_labels) # BCE Loss with sigmoids over outputs\n",
        "\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update the number of correctly classified validation samples\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            batch_idx += 1\n",
        "\n",
        "        # Calculate scores\n",
        "        val_loss = running_val_loss / batch_idx\n",
        "        val_accuracy = running_corrects / float(total)\n",
        "\n",
        "        print(f\"Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n",
        "\n",
        "        return val_loss, val_accuracy\n",
        "\n",
        "    def test(self, test_dataset, train_dataset=None):\n",
        "        \"\"\"Test the model.\n",
        "\n",
        "        Args:\n",
        "            test_dataset: dataset on which to test the network\n",
        "            train_dataset: training set used to train the last split, if\n",
        "                available\n",
        "        Returns:\n",
        "            accuracy (float): accuracy of the model on the test set\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)  # Set Network to evaluation mode\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "\n",
        "        self.test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "\n",
        "        # To store all predictions\n",
        "        all_preds = torch.tensor([])\n",
        "        all_preds = all_preds.type(torch.LongTensor)\n",
        "\n",
        "        # Clear mean of exemplars cache\n",
        "        self.cached_means = None\n",
        "        \n",
        "        # Disable transformations for train_dataset, if available, as we will\n",
        "        # need original PIL images from which to extract features.\n",
        "        if train_dataset is not None: train_dataset.dataset.disable_transform()\n",
        "\n",
        "        for images, labels in self.test_dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                preds = self.classify(images, train_dataset)\n",
        "\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            all_preds = torch.cat(\n",
        "                (all_preds.to(self.device), preds.to(self.device)), dim=0\n",
        "            )\n",
        "\n",
        "        if train_dataset is not None: train_dataset.dataset.enable_transform()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = running_corrects / float(total)  \n",
        "\n",
        "        print(f\"Test accuracy (iCaRL): {accuracy} \", end=\"\")\n",
        "\n",
        "        if train_dataset is None:\n",
        "            print(\"(only exemplars)\")\n",
        "        else:\n",
        "            print(\"(exemplars and training data)\")\n",
        "\n",
        "        return accuracy, all_preds\n",
        "\n",
        "    # @DEBUG\n",
        "    def test_rand(self, test_dataset, train_dataset=None):\n",
        "        \"\"\"Test the model.\n",
        "\n",
        "        Args:\n",
        "            test_dataset: dataset on which to test the network\n",
        "            train_dataset: training set used to train the last split, if\n",
        "                available\n",
        "        Returns:\n",
        "            accuracy (float): accuracy of the model on the test set\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)  # Set Network to evaluation mode\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "\n",
        "        self.test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "\n",
        "        # To store all predictions\n",
        "        all_preds = torch.tensor([])\n",
        "        all_preds = all_preds.type(torch.LongTensor)\n",
        "\n",
        "        # Clear mean of exemplars cache\n",
        "        self.cached_means = None\n",
        "        \n",
        "        # Disable transformations for train_dataset, if available, as we will\n",
        "        # need original PIL images from which to extract features.\n",
        "        if train_dataset is not None: train_dataset.dataset.disable_transform()\n",
        "\n",
        "        for images, labels in self.test_dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                preds = self.classify_rand(images, train_dataset)\n",
        "\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            all_preds = torch.cat(\n",
        "                (all_preds.to(self.device), preds.to(self.device)), dim=0\n",
        "            )\n",
        "\n",
        "        if train_dataset is not None: train_dataset.dataset.enable_transform()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = running_corrects / float(total)  \n",
        "\n",
        "        print(f\"Test accuracy (iCaRL): {accuracy} \", end=\"\")\n",
        "\n",
        "        if train_dataset is None:\n",
        "            print(\"(only exemplars)\")\n",
        "        else:\n",
        "            print(\"(exemplars and training data)\")\n",
        "\n",
        "        return accuracy, all_preds\n",
        "\n",
        "    def test_without_classifier(self, test_dataset):\n",
        "        \"\"\"Test the model without classifier, using the outputs of the\n",
        "        network instead.\n",
        "\n",
        "        Args:\n",
        "            test_dataset: dataset on which to test the network\n",
        "        Returns:\n",
        "            accuracy (float): accuracy of the model on the test set\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False) # Set Network to evaluation mode\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "\n",
        "        self.test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "\n",
        "        all_preds = torch.tensor([]) # to store all predictions\n",
        "        all_preds = all_preds.type(torch.LongTensor)\n",
        "        \n",
        "        for images, labels in self.test_dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Forward Pass\n",
        "            with torch.no_grad():\n",
        "                if self.VALIDATE:\n",
        "                    outputs = self.best_net(images)\n",
        "                else:\n",
        "                    outputs = self.net(images)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update Corrects\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Append batch predictions\n",
        "            all_preds = torch.cat(\n",
        "                (all_preds.to(self.device), preds.to(self.device)), dim=0\n",
        "            )\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = running_corrects / float(total)  \n",
        "\n",
        "        print(f\"Test accuracy (hybrid1): {accuracy}\")\n",
        "\n",
        "        return accuracy, all_preds\n",
        "    \n",
        "    def increment_classes(self, n=10):\n",
        "        \"\"\"Add n classes in the final fully connected layer.\"\"\"\n",
        "\n",
        "        in_features = self.net.fc.in_features  # size of each input sample\n",
        "        out_features = self.net.fc.out_features  # size of each output sample\n",
        "        weight = self.net.fc.weight.data\n",
        "\n",
        "        self.net.fc = nn.Linear(in_features, out_features+n)\n",
        "        self.net.fc.weight.data[:out_features] = weight\n",
        "    \n",
        "    def output_neurons_count(self):\n",
        "        \"\"\"Return the number of output neurons of the current network.\"\"\"\n",
        "\n",
        "        return self.net.fc.out_features\n",
        "    \n",
        "    def feature_neurons_count(self):\n",
        "        \"\"\"Return the number of neurons of the last layer of the feature extractor.\"\"\"\n",
        "\n",
        "        return self.net.fc.in_features\n",
        "    \n",
        "    def to_onehot(self, targets):\n",
        "        \"\"\"Convert targets to one-hot encoding (for BCE loss).\n",
        "\n",
        "        Args:\n",
        "            targets: dataloader.dataset.targets of the new task images\n",
        "        \"\"\"\n",
        "        num_classes = self.net.fc.out_features\n",
        "        one_hot_targets = torch.eye(num_classes)[targets]\n",
        "\n",
        "        return one_hot_targets.to(self.device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmCpRMBKgvDB",
        "colab_type": "text"
      },
      "source": [
        "### Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nRU--zYjmZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# iCaRL hyperparameters\n",
        "LR = 2\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.00001\n",
        "MILESTONES = [49, 63]\n",
        "GAMMA = 0.2\n",
        "NUM_EPOCHS = 70\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mcTQUN7VLPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2aa7d144-84df-4917-99a5-bc91f88b346f"
      },
      "source": [
        "\n",
        "\n",
        "# Define what tests to run\n",
        "TEST_ICARL = True # Run test with iCaRL (exemplars + train dataset)\n",
        "TEST_HYBRID1 = False # Run test with hybrid1\n",
        "\n",
        "# Initialize logs\n",
        "logs_icarl = [[] for _ in range(NUM_RUNS)]\n",
        "logs_icarl_rand = [[] for _ in range(NUM_RUNS)] # @DEBUG\n",
        "logs_hybrid1 = [[] for _ in range(NUM_RUNS)]\n",
        "logs_analisys = [[] for _ in range(NUM_RUNS)]\n",
        "logs_analisys_split = [dict() for _ in range(NUM_RUNS)]\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "    net = resnet32()\n",
        "    icarl = iCaRL(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS, BATCH_SIZE, train_transform, test_transform)\n",
        "\n",
        "    for split_i in range(10):\n",
        "        print(f\"## Split {split_i} of run {run_i} ##\")\n",
        "        \n",
        "        train_logs = icarl.incremental_train(split_i, train_subsets[run_i][split_i], val_subsets[run_i][split_i])\n",
        "\n",
        "        all_targets = torch.stack([label[0] for _, label in DataLoader(test_subsets[run_i][split_i])])\n",
        "\n",
        "        if TEST_ICARL:\n",
        "            logs_icarl[run_i].append({})\n",
        "            logs_icarl_rand[run_i].append({}) # @DEBUG\n",
        "              #  print(\"Herding:\")\n",
        "              #  acc, all_preds = icarl.test(test_subsets[run_i][split_i], train_subsets[run_i][split_i])\n",
        "           \n",
        "\n",
        "            print(\"Random:\") # @DEBUG\n",
        "            acc_rand, _ = icarl.test_rand(test_subsets[run_i][split_i], train_subsets[run_i][split_i]) # @DEBUG\n",
        "\n",
        "            logs_icarl_rand[run_i][split_i]['accuracy'] = acc_rand # @DEBUG\n",
        "\n",
        "            # if split_i > 0:\n",
        "            #   logs_analisys_split[run_i][split_i] = icarl.examined_exemplars_features[split_i]\n",
        "\n",
        "            # logs_icarl[run_i][split_i]['accuracy'] = acc\n",
        "            # logs_icarl[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n",
        "\n",
        "            logs_icarl[run_i][split_i]['train_loss'] = train_logs[0]\n",
        "            logs_icarl[run_i][split_i]['train_accuracy'] = train_logs[1]\n",
        "            logs_icarl[run_i][split_i]['val_loss'] = train_logs[2]\n",
        "            logs_icarl[run_i][split_i]['val_accuracy'] = train_logs[3]\n",
        "\n",
        "        if TEST_HYBRID1:\n",
        "            logs_hybrid1[run_i].append({})\n",
        "\n",
        "            acc, all_preds = icarl.test_without_classifier(test_subsets[run_i][split_i])\n",
        "\n",
        "            logs_hybrid1[run_i][split_i]['accuracy'] = acc\n",
        "\n",
        "    # logs_analisys[run_i] = icarl.examined_exemplars_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "## Split 0 of run 0 ##\n",
            "Length of exemplars set: 0\n",
            "Epoch: 1, LR: [2]\n",
            "Train loss: 0.37516499970640454, Train accuracy: 0.11584821428571429\n",
            "Validation loss: 0.3228461061205183, Validation accuracy: 0.09598214285714286\n",
            "Epoch: 2, LR: [2]\n",
            "Train loss: 0.32389820643833706, Train accuracy: 0.11830357142857142\n",
            "Validation loss: 0.3224814363888332, Validation accuracy: 0.14285714285714285\n",
            "Epoch: 3, LR: [2]\n",
            "Train loss: 0.323296018583434, Train accuracy: 0.11629464285714286\n",
            "Validation loss: 0.3191903957298824, Validation accuracy: 0.15178571428571427\n",
            "Epoch: 4, LR: [2]\n",
            "Train loss: 0.31907734530312676, Train accuracy: 0.13392857142857142\n",
            "Validation loss: 0.3145141048090799, Validation accuracy: 0.19196428571428573\n",
            "Epoch: 5, LR: [2]\n",
            "Train loss: 0.3176531323364803, Train accuracy: 0.14910714285714285\n",
            "Validation loss: 0.3095556029251644, Validation accuracy: 0.15401785714285715\n",
            "Epoch: 6, LR: [2]\n",
            "Train loss: 0.3162459407533918, Train accuracy: 0.15200892857142856\n",
            "Validation loss: 0.3134803218500955, Validation accuracy: 0.15848214285714285\n",
            "Epoch: 7, LR: [2]\n",
            "Train loss: 0.313584218280656, Train accuracy: 0.1747767857142857\n",
            "Validation loss: 0.31011284249169485, Validation accuracy: 0.22767857142857142\n",
            "Epoch: 8, LR: [2]\n",
            "Train loss: 0.3116292425564357, Train accuracy: 0.1859375\n",
            "Validation loss: 0.3126887721674783, Validation accuracy: 0.20758928571428573\n",
            "Epoch: 9, LR: [2]\n",
            "Train loss: 0.3029868768794196, Train accuracy: 0.2328125\n",
            "Validation loss: 0.30056269254003254, Validation accuracy: 0.265625\n",
            "Epoch: 10, LR: [2]\n",
            "Train loss: 0.28995222577026913, Train accuracy: 0.2825892857142857\n",
            "Validation loss: 0.32134588275636944, Validation accuracy: 0.2611607142857143\n",
            "Epoch: 11, LR: [2]\n",
            "Train loss: 0.28321346172264644, Train accuracy: 0.31138392857142855\n",
            "Validation loss: 0.2922535070351192, Validation accuracy: 0.2857142857142857\n",
            "Epoch: 12, LR: [2]\n",
            "Train loss: 0.27160652961049764, Train accuracy: 0.3600446428571429\n",
            "Validation loss: 0.2785776810986655, Validation accuracy: 0.3080357142857143\n",
            "Epoch: 13, LR: [2]\n",
            "Train loss: 0.2597183012536594, Train accuracy: 0.4095982142857143\n",
            "Validation loss: 0.24036705493927002, Validation accuracy: 0.453125\n",
            "Epoch: 14, LR: [2]\n",
            "Train loss: 0.2476030198591096, Train accuracy: 0.45669642857142856\n",
            "Validation loss: 0.3314186632633209, Validation accuracy: 0.26339285714285715\n",
            "Epoch: 15, LR: [2]\n",
            "Train loss: 0.2282696076801845, Train accuracy: 0.4941964285714286\n",
            "Validation loss: 0.23064861127308436, Validation accuracy: 0.4799107142857143\n",
            "Epoch: 16, LR: [2]\n",
            "Train loss: 0.21223024470465524, Train accuracy: 0.5484375\n",
            "Validation loss: 0.21156384263719832, Validation accuracy: 0.5290178571428571\n",
            "Epoch: 17, LR: [2]\n",
            "Train loss: 0.20317624977656773, Train accuracy: 0.5709821428571429\n",
            "Validation loss: 0.20662072513784682, Validation accuracy: 0.546875\n",
            "Epoch: 18, LR: [2]\n",
            "Train loss: 0.19050679526158742, Train accuracy: 0.6015625\n",
            "Validation loss: 0.18490036683423178, Validation accuracy: 0.6026785714285714\n",
            "Epoch: 19, LR: [2]\n",
            "Train loss: 0.18247002086469105, Train accuracy: 0.61875\n",
            "Validation loss: 0.18213325526033128, Validation accuracy: 0.6316964285714286\n",
            "Epoch: 20, LR: [2]\n",
            "Train loss: 0.17274917417338917, Train accuracy: 0.6410714285714286\n",
            "Validation loss: 0.18055772994245803, Validation accuracy: 0.6316964285714286\n",
            "Epoch: 21, LR: [2]\n",
            "Train loss: 0.16285710952111654, Train accuracy: 0.6642857142857143\n",
            "Validation loss: 0.15943838868822371, Validation accuracy: 0.6651785714285714\n",
            "Epoch: 22, LR: [2]\n",
            "Train loss: 0.1571683102420398, Train accuracy: 0.6852678571428571\n",
            "Validation loss: 0.18663201800414495, Validation accuracy: 0.609375\n",
            "Epoch: 23, LR: [2]\n",
            "Train loss: 0.14911574774554798, Train accuracy: 0.7033482142857143\n",
            "Validation loss: 0.1620400164808546, Validation accuracy: 0.6852678571428571\n",
            "Epoch: 24, LR: [2]\n",
            "Train loss: 0.14354488253593445, Train accuracy: 0.7131696428571429\n",
            "Validation loss: 0.16871483836855208, Validation accuracy: 0.6919642857142857\n",
            "Epoch: 25, LR: [2]\n",
            "Train loss: 0.13821180696998323, Train accuracy: 0.7292410714285714\n",
            "Validation loss: 0.15674257384879248, Validation accuracy: 0.7120535714285714\n",
            "Epoch: 26, LR: [2]\n",
            "Train loss: 0.13497799092105456, Train accuracy: 0.7388392857142857\n",
            "Validation loss: 0.13631085732153483, Validation accuracy: 0.7455357142857143\n",
            "Epoch: 27, LR: [2]\n",
            "Train loss: 0.1258296761661768, Train accuracy: 0.7627232142857143\n",
            "Validation loss: 0.1317538619041443, Validation accuracy: 0.7142857142857143\n",
            "Epoch: 28, LR: [2]\n",
            "Train loss: 0.1217840878026826, Train accuracy: 0.7625\n",
            "Validation loss: 0.15423021678413665, Validation accuracy: 0.6964285714285714\n",
            "Epoch: 29, LR: [2]\n",
            "Train loss: 0.12062264372195516, Train accuracy: 0.7674107142857143\n",
            "Validation loss: 0.19418229162693024, Validation accuracy: 0.625\n",
            "Epoch: 30, LR: [2]\n",
            "Train loss: 0.1125980899802276, Train accuracy: 0.78125\n",
            "Validation loss: 0.11999442215476717, Validation accuracy: 0.78125\n",
            "Epoch: 31, LR: [2]\n",
            "Train loss: 0.10780539427484785, Train accuracy: 0.790625\n",
            "Validation loss: 0.12026592876229968, Validation accuracy: 0.765625\n",
            "Epoch: 32, LR: [2]\n",
            "Train loss: 0.10251104874270303, Train accuracy: 0.8058035714285714\n",
            "Validation loss: 0.1571712749344962, Validation accuracy: 0.703125\n",
            "Epoch: 33, LR: [2]\n",
            "Train loss: 0.10587359251720564, Train accuracy: 0.7977678571428571\n",
            "Validation loss: 0.1104563804609435, Validation accuracy: 0.78125\n",
            "Epoch: 34, LR: [2]\n",
            "Train loss: 0.09736574238964489, Train accuracy: 0.8180803571428571\n",
            "Validation loss: 0.1028616332582065, Validation accuracy: 0.8013392857142857\n",
            "Epoch: 35, LR: [2]\n",
            "Train loss: 0.09528173921363695, Train accuracy: 0.8191964285714286\n",
            "Validation loss: 0.13122173505170004, Validation accuracy: 0.7544642857142857\n",
            "Epoch: 36, LR: [2]\n",
            "Train loss: 0.09454113819769451, Train accuracy: 0.8229910714285714\n",
            "Validation loss: 0.12756884204489843, Validation accuracy: 0.7700892857142857\n",
            "Epoch: 37, LR: [2]\n",
            "Train loss: 0.08714977582650525, Train accuracy: 0.8345982142857142\n",
            "Validation loss: 0.0937754288315773, Validation accuracy: 0.8236607142857143\n",
            "Epoch: 38, LR: [2]\n",
            "Train loss: 0.08142701467233045, Train accuracy: 0.8482142857142857\n",
            "Validation loss: 0.1408607949103628, Validation accuracy: 0.7611607142857143\n",
            "Epoch: 39, LR: [2]\n",
            "Train loss: 0.0861359076840537, Train accuracy: 0.8419642857142857\n",
            "Validation loss: 0.11648213118314743, Validation accuracy: 0.796875\n",
            "Epoch: 40, LR: [2]\n",
            "Train loss: 0.07667789906263352, Train accuracy: 0.8573660714285715\n",
            "Validation loss: 0.11086850932666234, Validation accuracy: 0.7879464285714286\n",
            "Epoch: 41, LR: [2]\n",
            "Train loss: 0.08162181036812918, Train accuracy: 0.8462053571428572\n",
            "Validation loss: 0.10612726105110985, Validation accuracy: 0.8125\n",
            "Epoch: 42, LR: [2]\n",
            "Train loss: 0.07807157156722887, Train accuracy: 0.8526785714285714\n",
            "Validation loss: 0.08842432871460915, Validation accuracy: 0.8325892857142857\n",
            "Epoch: 43, LR: [2]\n",
            "Train loss: 0.07085749214248997, Train accuracy: 0.8738839285714286\n",
            "Validation loss: 0.10082811117172241, Validation accuracy: 0.8125\n",
            "Epoch: 44, LR: [2]\n",
            "Train loss: 0.06807994672230312, Train accuracy: 0.8745535714285714\n",
            "Validation loss: 0.09005192773682731, Validation accuracy: 0.828125\n",
            "Epoch: 45, LR: [2]\n",
            "Train loss: 0.06604740396142006, Train accuracy: 0.8792410714285714\n",
            "Validation loss: 0.08540597770895277, Validation accuracy: 0.8571428571428571\n",
            "Epoch: 46, LR: [2]\n",
            "Train loss: 0.06635711906211716, Train accuracy: 0.8772321428571429\n",
            "Validation loss: 0.09601228258439473, Validation accuracy: 0.828125\n",
            "Epoch: 47, LR: [2]\n",
            "Train loss: 0.06763813091175896, Train accuracy: 0.8770089285714285\n",
            "Validation loss: 0.09470457849758011, Validation accuracy: 0.8415178571428571\n",
            "Epoch: 48, LR: [2]\n",
            "Train loss: 0.0681546999407666, Train accuracy: 0.8785714285714286\n",
            "Validation loss: 0.08979474327393941, Validation accuracy: 0.8705357142857143\n",
            "Epoch: 49, LR: [2]\n",
            "Train loss: 0.05938103161752224, Train accuracy: 0.8948660714285714\n",
            "Validation loss: 0.0907491317817143, Validation accuracy: 0.8370535714285714\n",
            "Epoch: 50, LR: [0.4]\n",
            "Train loss: 0.04254690081413303, Train accuracy: 0.9294642857142857\n",
            "Validation loss: 0.05268422381154129, Validation accuracy: 0.90625\n",
            "Epoch: 51, LR: [0.4]\n",
            "Train loss: 0.03443207935030971, Train accuracy: 0.9473214285714285\n",
            "Validation loss: 0.05819733680358955, Validation accuracy: 0.8973214285714286\n",
            "Epoch: 52, LR: [0.4]\n",
            "Train loss: 0.03341224611337696, Train accuracy: 0.9470982142857143\n",
            "Validation loss: 0.06200176743524415, Validation accuracy: 0.8973214285714286\n",
            "Epoch: 53, LR: [0.4]\n",
            "Train loss: 0.028970751339303595, Train accuracy: 0.9560267857142857\n",
            "Validation loss: 0.050857545009681156, Validation accuracy: 0.9196428571428571\n",
            "Epoch: 54, LR: [0.4]\n",
            "Train loss: 0.025209430365690164, Train accuracy: 0.9602678571428571\n",
            "Validation loss: 0.060812106622116904, Validation accuracy: 0.8928571428571429\n",
            "Epoch: 55, LR: [0.4]\n",
            "Train loss: 0.024708196953205124, Train accuracy: 0.9602678571428571\n",
            "Validation loss: 0.049146052449941635, Validation accuracy: 0.9151785714285714\n",
            "Epoch: 56, LR: [0.4]\n",
            "Train loss: 0.024745934390063798, Train accuracy: 0.9622767857142858\n",
            "Validation loss: 0.0486060016389404, Validation accuracy: 0.921875\n",
            "Epoch: 57, LR: [0.4]\n",
            "Train loss: 0.023474258317479065, Train accuracy: 0.9609375\n",
            "Validation loss: 0.06463130457060677, Validation accuracy: 0.8973214285714286\n",
            "Epoch: 58, LR: [0.4]\n",
            "Train loss: 0.022846295232219354, Train accuracy: 0.9649553571428572\n",
            "Validation loss: 0.06420994097633022, Validation accuracy: 0.9017857142857143\n",
            "Epoch: 59, LR: [0.4]\n",
            "Train loss: 0.021723106849406447, Train accuracy: 0.965625\n",
            "Validation loss: 0.06407323160341807, Validation accuracy: 0.8883928571428571\n",
            "Epoch: 60, LR: [0.4]\n",
            "Train loss: 0.0204091794123607, Train accuracy: 0.9696428571428571\n",
            "Validation loss: 0.05049727378146989, Validation accuracy: 0.9129464285714286\n",
            "Epoch: 61, LR: [0.4]\n",
            "Train loss: 0.020002509992835777, Train accuracy: 0.9700892857142858\n",
            "Validation loss: 0.06169678909437997, Validation accuracy: 0.9040178571428571\n",
            "Epoch: 62, LR: [0.4]\n",
            "Train loss: 0.01973086345408644, Train accuracy: 0.9700892857142858\n",
            "Validation loss: 0.05886651442519256, Validation accuracy: 0.9040178571428571\n",
            "Epoch: 63, LR: [0.4]\n",
            "Train loss: 0.018902593955863266, Train accuracy: 0.9703125\n",
            "Validation loss: 0.05632222284163747, Validation accuracy: 0.9040178571428571\n",
            "Epoch: 64, LR: [0.08000000000000002]\n",
            "Train loss: 0.015581177880189248, Train accuracy: 0.9763392857142857\n",
            "Validation loss: 0.049719538273555894, Validation accuracy: 0.9107142857142857\n",
            "Epoch: 65, LR: [0.08000000000000002]\n",
            "Train loss: 0.013558386153142367, Train accuracy: 0.9823660714285715\n",
            "Validation loss: 0.049155945756605694, Validation accuracy: 0.9151785714285714\n",
            "Epoch: 66, LR: [0.08000000000000002]\n",
            "Train loss: 0.013945870909706823, Train accuracy: 0.9794642857142857\n",
            "Validation loss: 0.05507692348744188, Validation accuracy: 0.9084821428571429\n",
            "Epoch: 67, LR: [0.08000000000000002]\n",
            "Train loss: 0.01460886711387762, Train accuracy: 0.98125\n",
            "Validation loss: 0.0502674574298518, Validation accuracy: 0.9129464285714286\n",
            "Epoch: 68, LR: [0.08000000000000002]\n",
            "Train loss: 0.012952418399176427, Train accuracy: 0.9834821428571429\n",
            "Validation loss: 0.05172166680651052, Validation accuracy: 0.9151785714285714\n",
            "Epoch: 69, LR: [0.08000000000000002]\n",
            "Train loss: 0.011875194018440588, Train accuracy: 0.9841517857142857\n",
            "Validation loss: 0.05837683592523847, Validation accuracy: 0.9151785714285714\n",
            "Epoch: 70, LR: [0.08000000000000002]\n",
            "Train loss: 0.01233941333900605, Train accuracy: 0.9837053571428571\n",
            "Validation loss: 0.058807886338659694, Validation accuracy: 0.9084821428571429\n",
            "Target number of exemplars per class: 200\n",
            "Target total number of exemplars: 2000\n",
            "Extracting exemplars from class 0 of current split... Extracted 200 exemplars.\n",
            "Saved 10 exemplars for features analysis\n",
            "Extracting exemplars from class 1 of current split... Extracted 200 exemplars.\n",
            "Extracting exemplars from class 2 of current split... Extracted 200 exemplars.\n",
            "Extracting exemplars from class 3 of current split... Extracted 200 exemplars.\n",
            "Extracting exemplars from class 4 of current split... Extracted 200 exemplars.\n",
            "Extracting exemplars from class 5 of current split... Extracted 200 exemplars.\n",
            "Extracting exemplars from class 6 of current split... Extracted 200 exemplars.\n",
            "Extracting exemplars from class 7 of current split... Extracted 200 exemplars.\n",
            "Extracting exemplars from class 8 of current split... Extracted 200 exemplars.\n",
            "Extracting exemplars from class 9 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 0 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 1 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 2 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 3 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 4 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 5 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 6 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 7 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 8 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 9 of current split... Extracted 200 exemplars.\n",
            "Random:\n",
            "Computing mean of exemplars... done\n",
            "Test accuracy (iCaRL): 0.888 (exemplars and training data)\n",
            "## Split 1 of run 0 ##\n",
            "Length of exemplars set: 2000\n",
            "Epoch: 1, LR: [2]\n",
            "Train loss: 0.15700153755669546, Train accuracy: 0.3310643564356436\n",
            "Validation loss: 0.22817089089325496, Validation accuracy: 0.11383928571428571\n",
            "Epoch: 2, LR: [2]\n",
            "Train loss: 0.12453023145104399, Train accuracy: 0.4368811881188119\n",
            "Validation loss: 0.18830807081290654, Validation accuracy: 0.26339285714285715\n",
            "Epoch: 3, LR: [2]\n",
            "Train loss: 0.11439779954086436, Train accuracy: 0.510519801980198\n",
            "Validation loss: 0.1878391525575093, Validation accuracy: 0.2857142857142857\n",
            "Epoch: 4, LR: [2]\n",
            "Train loss: 0.10678474332141404, Train accuracy: 0.562809405940594\n",
            "Validation loss: 0.17731202287333353, Validation accuracy: 0.36160714285714285\n",
            "Epoch: 5, LR: [2]\n",
            "Train loss: 0.10279858554943953, Train accuracy: 0.5909653465346535\n",
            "Validation loss: 0.18302969208785466, Validation accuracy: 0.4017857142857143\n",
            "Epoch: 6, LR: [2]\n",
            "Train loss: 0.10014100896544975, Train accuracy: 0.6183477722772277\n",
            "Validation loss: 0.17518837111336844, Validation accuracy: 0.4263392857142857\n",
            "Epoch: 7, LR: [2]\n",
            "Train loss: 0.09639363492479419, Train accuracy: 0.6448019801980198\n",
            "Validation loss: 0.15770259925297328, Validation accuracy: 0.47544642857142855\n",
            "Epoch: 8, LR: [2]\n",
            "Train loss: 0.09243087182835777, Train accuracy: 0.6649133663366337\n",
            "Validation loss: 0.16234107315540314, Validation accuracy: 0.5044642857142857\n",
            "Epoch: 9, LR: [2]\n",
            "Train loss: 0.09068399977565993, Train accuracy: 0.6811571782178217\n",
            "Validation loss: 0.16371831936495645, Validation accuracy: 0.5\n",
            "Epoch: 10, LR: [2]\n",
            "Train loss: 0.09138273890360747, Train accuracy: 0.6806930693069307\n",
            "Validation loss: 0.1676337080342429, Validation accuracy: 0.5223214285714286\n",
            "Epoch: 11, LR: [2]\n",
            "Train loss: 0.08583834302602428, Train accuracy: 0.7046720297029703\n",
            "Validation loss: 0.17501544952392578, Validation accuracy: 0.47544642857142855\n",
            "Epoch: 12, LR: [2]\n",
            "Train loss: 0.08594068062334957, Train accuracy: 0.708230198019802\n",
            "Validation loss: 0.16221262088843755, Validation accuracy: 0.5223214285714286\n",
            "Epoch: 13, LR: [2]\n",
            "Train loss: 0.08652621640427278, Train accuracy: 0.7161200495049505\n",
            "Validation loss: 0.17946480001722062, Validation accuracy: 0.4799107142857143\n",
            "Epoch: 14, LR: [2]\n",
            "Train loss: 0.0846152519530589, Train accuracy: 0.7249381188118812\n",
            "Validation loss: 0.16521317405360086, Validation accuracy: 0.5915178571428571\n",
            "Epoch: 15, LR: [2]\n",
            "Train loss: 0.08275072180693692, Train accuracy: 0.7315903465346535\n",
            "Validation loss: 0.15126768818923406, Validation accuracy: 0.5669642857142857\n",
            "Epoch: 16, LR: [2]\n",
            "Train loss: 0.08119134450017816, Train accuracy: 0.7425742574257426\n",
            "Validation loss: 0.1630502130304064, Validation accuracy: 0.5625\n",
            "Epoch: 17, LR: [2]\n",
            "Train loss: 0.08201523737447096, Train accuracy: 0.7467512376237624\n",
            "Validation loss: 0.15742541210991995, Validation accuracy: 0.5558035714285714\n",
            "Epoch: 18, LR: [2]\n",
            "Train loss: 0.08037671461553857, Train accuracy: 0.7523205445544554\n",
            "Validation loss: 0.16238523168223246, Validation accuracy: 0.546875\n",
            "Epoch: 19, LR: [2]\n",
            "Train loss: 0.07887052040141408, Train accuracy: 0.7585086633663366\n",
            "Validation loss: 0.1615117085831506, Validation accuracy: 0.5870535714285714\n",
            "Epoch: 20, LR: [2]\n",
            "Train loss: 0.07986532653322315, Train accuracy: 0.7602103960396039\n",
            "Validation loss: 0.1550618771995817, Validation accuracy: 0.578125\n",
            "Epoch: 21, LR: [2]\n",
            "Train loss: 0.07741495848882317, Train accuracy: 0.7634591584158416\n",
            "Validation loss: 0.17380192875862122, Validation accuracy: 0.5424107142857143\n",
            "Epoch: 22, LR: [2]\n",
            "Train loss: 0.07721429141146122, Train accuracy: 0.7721225247524752\n",
            "Validation loss: 0.14626815489360265, Validation accuracy: 0.6183035714285714\n",
            "Epoch: 23, LR: [2]\n",
            "Train loss: 0.07610784353006005, Train accuracy: 0.7797029702970297\n",
            "Validation loss: 0.1610531998532159, Validation accuracy: 0.5803571428571429\n",
            "Epoch: 24, LR: [2]\n",
            "Train loss: 0.07607365827454199, Train accuracy: 0.7800123762376238\n",
            "Validation loss: 0.1564639380999974, Validation accuracy: 0.5982142857142857\n",
            "Epoch: 25, LR: [2]\n",
            "Train loss: 0.07419293584062321, Train accuracy: 0.7928527227722773\n",
            "Validation loss: 0.17486385362488882, Validation accuracy: 0.5111607142857143\n",
            "Epoch: 26, LR: [2]\n",
            "Train loss: 0.07368013063574781, Train accuracy: 0.7962561881188119\n",
            "Validation loss: 0.16189623730523245, Validation accuracy: 0.6026785714285714\n",
            "Epoch: 27, LR: [2]\n",
            "Train loss: 0.07449565134426155, Train accuracy: 0.7930074257425742\n",
            "Validation loss: 0.16638818170343125, Validation accuracy: 0.578125\n",
            "Epoch: 28, LR: [2]\n",
            "Train loss: 0.07204594156972252, Train accuracy: 0.8044554455445545\n",
            "Validation loss: 0.15813427524907248, Validation accuracy: 0.5959821428571429\n",
            "Epoch: 29, LR: [2]\n",
            "Train loss: 0.07403248530065659, Train accuracy: 0.7937809405940595\n",
            "Validation loss: 0.13607555414949143, Validation accuracy: 0.6160714285714286\n",
            "Epoch: 30, LR: [2]\n",
            "Train loss: 0.07247363252214867, Train accuracy: 0.8030631188118812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-3e0ef86b6d55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"## Split {split_i} of run {run_i} ##\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0micarl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincremental_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_subsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrun_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_subsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrun_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mall_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_subsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrun_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-a6d269b7c03c>\u001b[0m in \u001b[0;36mincremental_train\u001b[0;34m(self, split, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;31m# Improve network parameters upon receiving new classes. Effectively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;31m# train a new network starting from the current network parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mtrain_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_representation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;31m# Compute the number of exemplars per class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-a6d269b7c03c>\u001b[0m in \u001b[0;36mupdate_representation\u001b[0;34m(self, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Train the network on combined dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtrain_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_with_exemplars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# @todo: include exemplars in validation set?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;31m# Keep a copy of the current network in order to compute its outputs for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-a6d269b7c03c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;31m# Validate after each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;31m# Validation criterion: best net is the one that minimizes the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-a6d269b7c03c>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0mbatch_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m                 \u001b[0;31m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m                     \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                     \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel_join_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuDM6Ydokv05",
        "colab_type": "text"
      },
      "source": [
        "### Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEn9P6usbwYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = [[logs_icarl[run_i][i]['train_loss'] for i in range(10)] for run_i in range(NUM_RUNS)]\n",
        "train_accuracy = [[logs_icarl[run_i][i]['train_accuracy'] for i in range(10)] for run_i in range(NUM_RUNS)]\n",
        "val_loss = [[logs_icarl[run_i][i]['val_loss'] for i in range(10)] for run_i in range(NUM_RUNS)]\n",
        "val_accuracy = [[logs_icarl[run_i][i]['val_accuracy'] for i in range(10)] for run_i in range(NUM_RUNS)]\n",
        "test_accuracy = [[logs_icarl[run_i][i]['accuracy'] for i in range(10)] for run_i in range(NUM_RUNS)]\n",
        "\n",
        "train_loss = np.array(train_loss)\n",
        "train_accuracy = np.array(train_accuracy)\n",
        "val_loss = np.array(val_loss)\n",
        "val_accuracy = np.array(val_accuracy)\n",
        "test_accuracy = np.array(test_accuracy)\n",
        "\n",
        "train_loss_stats = np.array([train_loss.mean(0), train_loss.std(0)]).transpose()\n",
        "train_accuracy_stats = np.array([train_accuracy.mean(0), train_accuracy.std(0)]).transpose()\n",
        "val_loss_stats = np.array([val_loss.mean(0), val_loss.std(0)]).transpose()\n",
        "val_accuracy_stats = np.array([val_accuracy.mean(0), val_accuracy.std(0)]).transpose()\n",
        "test_accuracy_stats = np.array([test_accuracy.mean(0), test_accuracy.std(0)]).transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GmiU4BfhVht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot.train_val_scores(train_loss_stats, train_accuracy_stats, val_loss_stats, val_accuracy_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAWPfif3j7pK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot.test_scores(test_accuracy_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kiw-q4AChB6",
        "colab_type": "text"
      },
      "source": [
        "### Analisys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvD4EPGJLNZn",
        "colab_type": "text"
      },
      "source": [
        "#### Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07e3f7xqCnGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logs_analysis = obj_load('logs_analysis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jICRWSARLUuD",
        "colab_type": "text"
      },
      "source": [
        "#### Analisys"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg747VOAHFqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Last net scores over 10 selected exemplars\n",
        "# Scres over all old nodes\n",
        "# need to remove last 10 old nodes since they are produced by the same net in the 2 cases\n",
        "\n",
        "ln_split_nine_first = logs_analisys[0][9][0] # first = 1st random state\n",
        "ln_split_nine_second = logs_analisys[1][9][0] # # second = 2nd random state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g17fu2Q_HWZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert into numpy array\n",
        "ln_split_nine_first = np.array(ln_split_nine_first)\n",
        "ln_split_nine_second = np.array(ln_split_nine_second)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2YJo1PtIJFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove last ten nodes. Not meaningful since produced by the same net\n",
        "ln_split_nine_first = ln_split_nine_first[:, 0:-10]\n",
        "ln_split_nine_second = ln_split_nine_second[:, :-10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIGHwwslIgfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ad hoc old nets scores over 10 selected exemplars\n",
        "# Scores over all old nodes from 0 to 80\n",
        "on_split_nine_first = logs_analisys[0][9][1]\n",
        "on_split_nine_second = logs_analisys[1][9][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gue4hJAsJxbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ad hoc old nets scores over 10 selected exemplars\n",
        "# Scores over all old nodes from 0 to 80\n",
        "on_split_nine_first = np.array(on_split_nine_first)\n",
        "on_split_nine_second = np.array(on_split_nine_first)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9zI9MYNKGMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "on_random_seeds_avg = np.mean((on_split_nine_first, on_split_nine_second), axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eK0eMQtKyPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ln_random_seeds_avg = np.mean((ln_split_nine_first, ln_split_nine_second), axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mhjDj2GQHSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "on_images_mean = on_random_seeds_avg.mean(0)\n",
        "ln_images_mean = ln_random_seeds_avg.mean(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5vbpFwKYTo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "on_images_mean_std = np.array((on_random_seeds_avg.mean(0), on_random_seeds_avg.std(0))).transpose()\n",
        "ln_images_mean_std = np.array((ln_random_seeds_avg.mean(0), ln_random_seeds_avg.std(0))).transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p2lUro2OUPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def outputs_delta(last_net_outputs, old_net_outputs):\n",
        "  '''\n",
        "  Args:\n",
        "  last_net_outputs (numpy array): average over all random seeds and over all images of outputs on old classes obtained\n",
        "                                  with last net logic. Outputs are related to a single split\n",
        "  old_net_outputs (numpy array): average over all random seeds and over all images of outputs on old classes obtained \n",
        "                                 with old nets logic. Outputs are related to a single split\n",
        "  '''\n",
        "  # Posso fare due cose: creare tanti plot quanto è il numero di split\n",
        "  # Comparando gli scores dei nodi presi 10 a 10\n",
        "  # Oppure comparare tutti gli scores tra tutti i nodi. Per ora\n",
        "  # implemento questa seconda opzione\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # Set y scale to logarithmic\n",
        "  plt.yscale('log')\n",
        "\n",
        "  x = np.array(range(0, last_net_outputs.shape[0]))\n",
        "\n",
        "  # Posso fare un plot unidimensionale o usare l'indice come \n",
        "  ax.scatter(x, last_net_outputs, color = '#450c8d', label = 'Last net', s = 10)\n",
        "  ax.scatter(x, old_net_outputs, color = '#e1c30c' , label = 'Old nets', s = 10)\n",
        "\n",
        "\n",
        "  ax.legend()\n",
        "\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16GztQLzRyhG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "5c004101-6e5c-416d-a4eb-783c69a4b965"
      },
      "source": [
        "outputs_delta(ln_images_mean, on_images_mean)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pW81-20BebQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "T = 2\n",
        "\n",
        "sigmoid = nn.Sigmoid()\n",
        "\n",
        "soften_targets_first = sigmoid(torch.tensor(np.log(on_split_nine_first/(1-on_split_nine_first)))/T).numpy()\n",
        "soften_targets_second = sigmoid(torch.tensor(np.log(on_split_nine_second/(1-on_split_nine_second)))/T).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHdZLL9QEgNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "soften_targets_seeds_avg = np.mean((soften_targets_first, soften_targets_second), axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFoHbWmfE8zY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "on_soften_targets = soften_targets_seeds_avg.mean(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy8_ys2NCzCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compare_targets(not_soften, soften, color1, color2, T):\n",
        "  '''\n",
        "  Args:\n",
        "  last_net_outputs (numpy array): average over all random seeds and over all images of outputs on old classes obtained\n",
        "                                  with last net logic. Outputs are related to a single split\n",
        "  old_net_outputs (numpy array): average over all random seeds and over all images of outputs on old classes obtained \n",
        "                                 with old nets logic. Outputs are related to a single split\n",
        "  '''\n",
        "  # Posso fare due cose: creare tanti plot quanto è il numero di split\n",
        "  # Comparando gli scores dei nodi presi 10 a 10\n",
        "  # Oppure comparare tutti gli scores tra tutti i nodi. Per ora\n",
        "  # implemento questa seconda opzione\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # Set y scale to logarithmic\n",
        "  plt.yscale('log')\n",
        "\n",
        "  x = np.array(range(0, soften.shape[0]))\n",
        "\n",
        "  # Posso fare un plot unidimensionale o usare l'indice come \n",
        "  ax.scatter(x, soften, color = color2, label = 'Old Nets - T: {}'.format(T), s = 10)\n",
        "  ax.scatter(x, not_soften, color = color1, label = f'Old Nets - T: {1}', s = 10)\n",
        "\n",
        "  ax.legend()\n",
        "\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPepptayDkzL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "a2aad140-c6f3-41c0-fb3c-e547f4b3273c"
      },
      "source": [
        "# Compare old nets policy targets soften by T = 2 and not soften (T = 1)\n",
        "\n",
        "compare_targets(on_images_mean, on_soften_targets, '#e1c30c', '#008080', 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YHbSmnyGxKq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "8e8c03e4-d9c4-4934-f71e-fab7e1a18533"
      },
      "source": [
        "# Compare old nets policy targets soften by T = 2 last nets policy targets\n",
        "\n",
        "compare_targets(ln_images_mean, on_soften_targets, '#450c8d', '#008080', 2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vsb3dRhYGg_F",
        "colab_type": "text"
      },
      "source": [
        "## iCaRL - Distillation Soften Targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz2sEjvxY5ur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from math import floor\n",
        "from copy import deepcopy\n",
        "import random\n",
        "\n",
        "sigmoid = nn.Sigmoid() # Sigmoid function\n",
        "\n",
        "class Exemplars(torch.utils.data.Dataset):\n",
        "    def __init__(self, exemplars, transform=None):\n",
        "        # exemplars = [\n",
        "        #     [ex0_class0, ex1_class0, ex2_class0, ...],\n",
        "        #     [ex0_class1, ex1_class1, ex2_class1, ...],\n",
        "        #     ...\n",
        "        #     [ex0_classN, ex1_classN, ex2_classN, ...]\n",
        "        # ]\n",
        "\n",
        "        self.dataset = []\n",
        "        self.targets = []\n",
        "\n",
        "        for y, exemplar_y in enumerate(exemplars):\n",
        "            self.dataset.extend(exemplar_y)\n",
        "            self.targets.extend([y] * len(exemplar_y))\n",
        "\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image = self.dataset[index]\n",
        "        target = self.targets[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "class iCaRLDist:\n",
        "    \"\"\"Implement iCaRL, a strategy for simultaneously learning classifiers and a\n",
        "    feature representation in the class-incremental setting.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device, net, lr, momentum, weight_decay, milestones, gamma, num_epochs, batch_size, train_transform, test_transform, T):\n",
        "        self.device = device\n",
        "        self.net = net\n",
        "\n",
        "        # Set hyper-parameters\n",
        "        self.LR = lr\n",
        "        self.MOMENTUM = momentum\n",
        "        self.WEIGHT_DECAY = weight_decay\n",
        "        self.MILESTONES = milestones\n",
        "        self.GAMMA = gamma\n",
        "        self.NUM_EPOCHS = num_epochs\n",
        "        self.BATCH_SIZE = batch_size\n",
        "        \n",
        "        # Set transformations\n",
        "        self.train_transform = train_transform\n",
        "        self.test_transform = test_transform\n",
        "\n",
        "        # List of exemplar sets. Each set contains memory_size/num_classes exemplars\n",
        "        # with num_classes the number of classes seen until now by the network.\n",
        "        self.exemplars = []\n",
        "\n",
        "        # Initialize the copy of the old network, used to compute outputs of the\n",
        "        # previous network for the distillation loss, to None. This is useful to\n",
        "        # correctly apply the first function when training the network for the\n",
        "        # first time.\n",
        "        self.old_net = None\n",
        "        self.old_nets = []\n",
        "        self.T = T\n",
        "\n",
        "        # Maximum number of exemplars\n",
        "        self.memory_size = 2000\n",
        "    \n",
        "        # Loss function\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        # If True, test on the best model found (e.g., minimize loss). If False,\n",
        "        # test on the last model build (of the last epoch).\n",
        "        self.VALIDATE = False\n",
        "\n",
        "    def classify(self, batch, train_dataset=None):\n",
        "        \"\"\"Mean-of-exemplars classifier used to classify images into the set of\n",
        "        classes observed so far.\n",
        "\n",
        "        Args:\n",
        "            batch (torch.tensor): batch to classify\n",
        "        Returns:\n",
        "            label (int): class label assigned to the image\n",
        "        \"\"\"\n",
        "\n",
        "        batch_features = self.extract_features(batch) # (batch size, 64)\n",
        "        for i in range(batch_features.size(0)):\n",
        "            batch_features[i] = batch_features[i]/batch_features[i].norm() # Normalize sample feature representation\n",
        "        batch_features = batch_features.to(self.device)\n",
        "\n",
        "        if self.cached_means is None:\n",
        "            print(\"Computing mean of exemplars... \", end=\"\")\n",
        "\n",
        "            self.cached_means = []\n",
        "\n",
        "            # Number of known classes\n",
        "            num_classes = len(self.exemplars)\n",
        "\n",
        "            # Compute the means of classes with all the data available,\n",
        "            # including training data which contains samples belonging to\n",
        "            # the latest 10 classes. This will remove noise from the mean\n",
        "            # estimate, improving the results.\n",
        "            if train_dataset is not None:\n",
        "                train_features_list = [[] for _ in range(10)]\n",
        "\n",
        "                for train_sample, label in train_dataset:\n",
        "                    features = self.extract_features(train_sample, batch=False, transform=self.test_transform)\n",
        "                    features = features/features.norm()\n",
        "                    train_features_list[label % 10].append(features)\n",
        "\n",
        "            # Compute means of exemplars for all known classes\n",
        "            for y in range(num_classes):\n",
        "                if (train_dataset is not None) and (y in range(num_classes-10, num_classes)):\n",
        "                    features_list = train_features_list[y % 10]\n",
        "                else:\n",
        "                    features_list = []\n",
        "\n",
        "                for exemplar in self.exemplars[y]:\n",
        "                    features = self.extract_features(exemplar, batch=False, transform=self.test_transform)\n",
        "                    features = features/features.norm() # Normalize the feature representation of the exemplar\n",
        "                    features_list.append(features)\n",
        "                \n",
        "                features_list = torch.stack(features_list)\n",
        "                class_means = features_list.mean(dim=0)\n",
        "                class_means = class_means/class_means.norm() # Normalize the class means\n",
        "\n",
        "                self.cached_means.append(class_means)\n",
        "            \n",
        "            self.cached_means = torch.stack(self.cached_means).to(self.device)\n",
        "            print(\"done\")\n",
        "\n",
        "        preds = []\n",
        "        for i in range(batch_features.size(0)):\n",
        "            f_arg = torch.norm(batch_features[i] - self.cached_means, dim=1)\n",
        "            preds.append(torch.argmin(f_arg))\n",
        "        \n",
        "        return torch.stack(preds)\n",
        "    \n",
        "    def extract_features(self, sample, batch=True, transform=None):\n",
        "        \"\"\"Extract features from single sample or from batch.\n",
        "        \n",
        "        Args:\n",
        "            sample (PIL image or torch.tensor): sample(s) from which to\n",
        "                extract features\n",
        "            batch (bool): if True, sample is a torch.tensor containing a batch\n",
        "                of images with dimensions (batch_size, 3, 32, 32)\n",
        "            transform: transformations to apply to the PIL image before\n",
        "                processing\n",
        "        Returns:\n",
        "            features: torch.tensor, 1-D of dimension 64 for single samples or\n",
        "                2-D of dimension (batch_size, 64) for batch\n",
        "        \"\"\"\n",
        "\n",
        "        assert not (batch is False and transform is None), \"if a PIL image is passed to extract_features, a transform must be defined\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)\n",
        "        if self.old_net is not None: \n",
        "          self.old_net.train(False)\n",
        "          for i in range(len(self.old_nets)):\n",
        "            self.old_nets[i] = self.old_nets[i].train(False)\n",
        "\n",
        "        if batch is False: # Treat sample as single PIL image\n",
        "            sample = transform(sample)\n",
        "            sample = sample.unsqueeze(0) # https://stackoverflow.com/a/59566009/6486336\n",
        "\n",
        "        sample = sample.to(self.device)\n",
        "\n",
        "        if self.VALIDATE:\n",
        "            features = self.best_net.features(sample)\n",
        "        else:\n",
        "            features = self.net.features(sample)\n",
        "\n",
        "        if batch is False:\n",
        "            features = features[0]\n",
        "\n",
        "        return features\n",
        "\n",
        "    def incremental_train(self, split, train_dataset, val_dataset):\n",
        "        \"\"\"Adjust internal knowledge based on the additional information\n",
        "        available in the new observations.\n",
        "\n",
        "        Args:\n",
        "            split (int): current split number, counting from zero\n",
        "            train_dataset: dataset for training the model\n",
        "            val_dataset: dataset for validating the model\n",
        "        Returns:\n",
        "            train_logs: tuple of four metrics (train_loss, train_accuracy,\n",
        "            val_loss, val_accuracy) obtained during network training\n",
        "        \"\"\"\n",
        "\n",
        "        if split is not 0:\n",
        "            # Increment the number of output nodes for the new network by 10\n",
        "            self.increment_classes(10)\n",
        "\n",
        "        self.split = split\n",
        "\n",
        "        # Improve network parameters upon receiving new classes. Effectively\n",
        "        # train a new network starting from the current network parameters.\n",
        "        train_logs = self.update_representation(train_dataset, val_dataset)\n",
        "\n",
        "        # Compute the number of exemplars per class\n",
        "        num_classes = self.output_neurons_count()\n",
        "        m = floor(self.memory_size / num_classes)\n",
        "\n",
        "        print(f\"Target number of exemplars per class: {m}\")\n",
        "        print(f\"Target total number of exemplars: {m*num_classes}\")\n",
        "\n",
        "        # Reduce pre-existing exemplar sets in order to fit new exemplars\n",
        "        for y in range(len(self.exemplars)):\n",
        "            self.exemplars[y] = self.reduce_exemplar_set(self.exemplars[y], m)\n",
        "\n",
        "        # Construct exemplar set for new classes\n",
        "        new_exemplars = self.construct_exemplar_set_rand(train_dataset, m)\n",
        "        self.exemplars.extend(new_exemplars)\n",
        "\n",
        "        return train_logs\n",
        "\n",
        "    def update_representation(self, train_dataset, val_dataset):\n",
        "        \"\"\"Update the parameters of the network.\n",
        "\n",
        "        Args:\n",
        "            train_dataset: dataset for training the model\n",
        "            val_dataset: dataset for validating the model\n",
        "        Returns:\n",
        "            train_logs: tuple of four metrics (train_loss, train_accuracy,\n",
        "            val_loss, val_accuracy) obtained during network training\n",
        "        \"\"\"\n",
        "\n",
        "        # Combine the new training data with existing exemplars.\n",
        "        print(f\"Length of exemplars set: {sum([len(self.exemplars[y]) for y in range(len(self.exemplars))])}\")\n",
        "        exemplars_dataset = Exemplars(self.exemplars, self.train_transform)\n",
        "        train_dataset_with_exemplars = ConcatDataset([exemplars_dataset, train_dataset])\n",
        "\n",
        "        # Train the network on combined dataset\n",
        "        train_logs = self.train(train_dataset_with_exemplars, val_dataset) # @todo: include exemplars in validation set?\n",
        "\n",
        "        # Keep a copy of the current network in order to compute its outputs for\n",
        "        # the distillation loss while the new network is being trained.\n",
        "        self.old_net = deepcopy(self.net)\n",
        "        self.old_nets.append(self.old_net)\n",
        "\n",
        "        return train_logs\n",
        "\n",
        "    def construct_exemplar_set_rand(self, dataset, m):\n",
        "        \"\"\"Randomly sample m elements from a dataset without replacement.\n",
        "\n",
        "        Args:\n",
        "            dataset: dataset containing a split (samples from 10 classes) from\n",
        "                which to take exemplars\n",
        "            m (int): target number of exemplars per class\n",
        "        Returns:\n",
        "            exemplars: list of samples extracted from the dataset\n",
        "        \"\"\"\n",
        "\n",
        "        dataset.dataset.disable_transform()\n",
        "\n",
        "        samples = [[] for _ in range(10)]\n",
        "        for image, label in dataset:\n",
        "            label = label % 10 # Map labels to 0-9 range\n",
        "            samples[label].append(image)\n",
        "\n",
        "        dataset.dataset.enable_transform()\n",
        "\n",
        "        exemplars = [[] for _ in range(10)]\n",
        "\n",
        "        for y in range(10):\n",
        "            print(f\"Randomly extracting exemplars from class {y} of current split... \", end=\"\")\n",
        "\n",
        "            # Randomly choose m samples from samples[y] without replacement\n",
        "            exemplars[y] = random.sample(samples[y], m)\n",
        "\n",
        "            print(f\"Extracted {len(exemplars[y])} exemplars.\")\n",
        "\n",
        "        return exemplars\n",
        "\n",
        "    def reduce_exemplar_set(self, exemplar_set, m):\n",
        "        \"\"\"Procedure for removing exemplars from a given set.\n",
        "\n",
        "        Args:\n",
        "            exemplar_set (set): set of exemplars belonging to a certain class\n",
        "            m (int): target number of exemplars\n",
        "        Returns:\n",
        "            exemplar_set: reduced exemplar set\n",
        "        \"\"\"\n",
        "\n",
        "        return exemplar_set[:m]\n",
        "\n",
        "    def train(self, train_dataset, val_dataset):\n",
        "        \"\"\"Train the network for a specified number of epochs, and save\n",
        "        the best performing model on the validation set.\n",
        "        \n",
        "        Args:\n",
        "            train_dataset: dataset for training the model\n",
        "            val_dataset: dataset for validating the model\n",
        "        Returns: train_logs: tuple of four metrics (train_loss, train_accuracy,\n",
        "            val_loss, val_accuracy) obtained during network training. If\n",
        "            validation is enabled, return scores of the best epoch, otherwise\n",
        "            return scores of the last epoch.\n",
        "        \"\"\"\n",
        "        if self.split == 0:\n",
        "          parameters_to_optimize = self.net.parameters()\n",
        "          self.optimizer = optim.SGD(parameters_to_optimize, \n",
        "                                  lr=2,\n",
        "                                  momentum=self.MOMENTUM,\n",
        "                                  weight_decay=self.WEIGHT_DECAY)\n",
        "\n",
        "        else: \n",
        "          parameters_to_optimize = self.net.parameters()\n",
        "          self.optimizer = optim.SGD(parameters_to_optimize, \n",
        "                                    lr=self.LR,\n",
        "                                    momentum=self.MOMENTUM,\n",
        "                                    weight_decay=self.WEIGHT_DECAY)\n",
        "        \n",
        "        # Define the learning rate decaying policy\n",
        "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer,\n",
        "                                                        milestones=self.MILESTONES,\n",
        "                                                        gamma=self.GAMMA)\n",
        "\n",
        "        # Create DataLoaders for training and validation\n",
        "        self.train_dataloader = DataLoader(train_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "        self.val_dataloader = DataLoader(val_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "        # Send networks to chosen device\n",
        "        self.net = self.net.to(self.device)\n",
        "        if self.old_net is not None: \n",
        "          self.old_net.train(False)\n",
        "          for i in range(len(self.old_nets)):\n",
        "            self.old_nets[i] = self.old_nets[i].train(False)\n",
        "\n",
        "        cudnn.benchmark  # Calling this optimizes runtime\n",
        "\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.best_val_accuracy = 0\n",
        "        self.best_train_loss = float('inf')\n",
        "        self.best_train_accuracy = 0\n",
        "        \n",
        "        self.best_net = None\n",
        "        self.best_epoch = -1\n",
        "\n",
        "        for epoch in range(self.NUM_EPOCHS):\n",
        "            # Run an epoch (start counting form 1)\n",
        "            train_loss, train_accuracy = self.do_epoch(epoch+1)\n",
        "        \n",
        "            # Validate after each epoch \n",
        "            val_loss, val_accuracy = self.validate()    \n",
        "\n",
        "            # Validation criterion: best net is the one that minimizes the loss\n",
        "            # on the validation set.\n",
        "            if self.VALIDATE and val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.best_val_accuracy = val_accuracy\n",
        "                self.best_train_loss = train_loss\n",
        "                self.best_train_accuracy = train_accuracy\n",
        "\n",
        "                self.best_net = deepcopy(self.net)\n",
        "                self.best_epoch = epoch\n",
        "                print(\"Best model updated\")\n",
        "\n",
        "        if self.VALIDATE:\n",
        "            val_loss = self.best_val_loss\n",
        "            val_accuracy = self.best_val_accuracy\n",
        "            train_loss = self.best_train_loss\n",
        "            train_accuracy = self.best_train_accuracy\n",
        "\n",
        "            print(f\"Best model found at epoch {self.best_epoch+1}\")\n",
        "\n",
        "        return train_loss, train_accuracy, val_loss, val_accuracy\n",
        "    \n",
        "    def do_epoch(self, current_epoch):\n",
        "        \"\"\"Trains model for one epoch.\n",
        "        \n",
        "        Args:\n",
        "            current_epoch (int): current epoch number (begins from 1)\n",
        "        Returns:\n",
        "            train_loss: average training loss over all batches of the\n",
        "                current epoch.\n",
        "            train_accuracy: training accuracy of the current epoch over\n",
        "                all samples.\n",
        "        \"\"\"\n",
        "\n",
        "        # Set the current network in training mode\n",
        "        self.net.train()\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "        if self.old_net is not None: \n",
        "          self.old_net.train(False)\n",
        "          for i in range(len(self.old_nets)):\n",
        "            self.old_nets[i] = self.old_nets[i].train(False)\n",
        "\n",
        "        running_train_loss = 0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        batch_idx = 0\n",
        "\n",
        "        print(f\"Epoch: {current_epoch}, LR: {self.scheduler.get_last_lr()}\")\n",
        "\n",
        "        for images, labels in self.train_dataloader:\n",
        "            loss, corrects = self.do_batch(images, labels)\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "            running_corrects += corrects\n",
        "            total += labels.size(0)\n",
        "            batch_idx += 1\n",
        "\n",
        "        self.scheduler.step()\n",
        "\n",
        "        # Calculate average scores\n",
        "        train_loss = running_train_loss / batch_idx # Average over all batches\n",
        "        train_accuracy = running_corrects / float(total) # Average over all samples\n",
        "\n",
        "        print(f\"Train loss: {train_loss}, Train accuracy: {train_accuracy}\")\n",
        "\n",
        "        return train_loss, train_accuracy\n",
        "\n",
        "    def do_batch(self, batch, labels):\n",
        "        \"\"\"Train network for a batch. Loss is applied here.\n",
        "\n",
        "        Args:\n",
        "            batch: batch of data used for training the network\n",
        "            labels: targets of the batch\n",
        "        Returns:\n",
        "            loss: output of the criterion applied\n",
        "            running_corrects: number of correctly classified elements\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        # Zero-ing the gradients\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        # One-hot encoding of labels of the new training data (new classes)\n",
        "        # Size: batch size (rows) by number of classes seen until now (columns)\n",
        "        #\n",
        "        # e.g., suppose we have four images in a batch, and each incremental\n",
        "        #   step adds three new classes. At the second step, the one-hot\n",
        "        #   encoding may return the following tensor:\n",
        "        #\n",
        "        #       tensor([[0., 0., 0., 1., 0., 0.],   # image 0 (label 3)\n",
        "        #               [0., 0., 0., 0., 1., 0.],   # image 1 (label 4)\n",
        "        #               [0., 0., 0., 0., 0., 1.],   # image 2 (label 5)\n",
        "        #               [0., 0., 0., 0., 1., 0.]])  # image 3 (label 4)\n",
        "        #\n",
        "        #   The first three elements of each vector will always be 0, as the\n",
        "        #   new training batch does not contain images belonging to classes\n",
        "        #   already seen in previous steps.\n",
        "        #\n",
        "        #   The last three elements of each vector will contain the actual\n",
        "        #   information about the class of each image (one-hot encoding of the\n",
        "        #   label). Therefore, we slice the tensor and remove the columns \n",
        "        #   related to old classes (all zeros).\n",
        "        num_classes = self.output_neurons_count() # Number of classes seen until now, including new classes\n",
        "        one_hot_labels = self.to_onehot(labels)[:, num_classes-10:num_classes]\n",
        "\n",
        "        if self.old_net is None:\n",
        "            # Network is training for the first time, so we only apply the\n",
        "            # classification loss.\n",
        "            targets = one_hot_labels\n",
        "            # Forward pass\n",
        "            outputs = self.net(batch)\n",
        "\n",
        "        else:\n",
        "            # Old net forward pass. We compute the outputs of the old network\n",
        "            # and apply a sigmoid function. These are used in the distillation\n",
        "            # loss. We discard the output of the new neurons, as they are not\n",
        "            # considered in the distillation loss.\n",
        "            # if self.split == 1:\n",
        "            #    old_net_outputs = sigmoid(self.old_nets[0](batch))\n",
        "            # else:\n",
        "            old_net_outputs = sigmoid(self.old_nets[0](batch)/self.T)\n",
        "            for i, old_net in enumerate(self.old_nets[1:]):\n",
        "              old_net_outputs = torch.cat((old_net_outputs, sigmoid(old_net(batch)[:, (i+1)*10:]/self.T)), dim=1)\n",
        "\n",
        "            # Concatenate the outputs of the old network and the one-hot encoded\n",
        "            # labels along dimension 1 (columns).\n",
        "            # \n",
        "            # Each row refers to an image in the training set, and contains:\n",
        "            # - the output of the old network for that image, used by the\n",
        "            #   distillation loss\n",
        "            # - the one-hot label of the image, used by the classification loss\n",
        "            targets = torch.cat((old_net_outputs, one_hot_labels), dim=1)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.net(batch)\n",
        "            outputs = torch.cat((outputs[:, :num_classes-10]/self.T, outputs[:, num_classes-10:]), dim=1)\n",
        "\n",
        "        \n",
        "        loss = self.criterion(outputs, targets)\n",
        "\n",
        "        # Get predictions\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Accuracy over NEW IMAGES, not over all images\n",
        "        running_corrects = torch.sum(preds == labels.data).data.item() \n",
        "\n",
        "        # Backward pass: computes gradients\n",
        "        loss.backward()\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss, running_corrects\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\"Validate the model.\n",
        "        \n",
        "        Returns:\n",
        "            val_loss: average loss function computed on the network outputs\n",
        "                of the validation set (val_dataloader).\n",
        "            val_accuracy: accuracy computed on the validation set.\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "        if self.old_net is not None: \n",
        "          self.old_net.train(False)\n",
        "          for i in range(len(self.old_nets)):\n",
        "            self.old_nets[i] = self.old_nets[i].train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)\n",
        "\n",
        "        running_val_loss = 0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        batch_idx = 0\n",
        "\n",
        "        for images, labels in self.val_dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # One hot encoding of new task labels \n",
        "            one_hot_labels = self.to_onehot(labels)\n",
        "\n",
        "            # New net forward pass\n",
        "            outputs = self.net(images)  \n",
        "            loss = self.criterion(outputs, one_hot_labels) # BCE Loss with sigmoids over outputs\n",
        "\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update the number of correctly classified validation samples\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            batch_idx += 1\n",
        "\n",
        "        # Calculate scores\n",
        "        val_loss = running_val_loss / batch_idx\n",
        "        val_accuracy = running_corrects / float(total)\n",
        "\n",
        "        print(f\"Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n",
        "\n",
        "        return val_loss, val_accuracy\n",
        "\n",
        "    def test(self, test_dataset, train_dataset=None):\n",
        "        \"\"\"Test the model.\n",
        "\n",
        "        Args:\n",
        "            test_dataset: dataset on which to test the network\n",
        "            train_dataset: training set used to train the last split, if\n",
        "                available\n",
        "        Returns:\n",
        "            accuracy (float): accuracy of the model on the test set\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)  # Set Network to evaluation mode\n",
        "        if self.old_net is not None: \n",
        "          self.old_net.train(False)\n",
        "          for i in range(len(self.old_nets)):\n",
        "            self.old_nets[i] = self.old_nets[i].train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)\n",
        "\n",
        "        self.test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "\n",
        "        # To store all predictions\n",
        "        all_preds = torch.tensor([])\n",
        "        all_preds = all_preds.type(torch.LongTensor)\n",
        "        all_targets = torch.tensor([])\n",
        "        all_targets = all_targets.type(torch.LongTensor)\n",
        "\n",
        "        # Clear mean of exemplars cache\n",
        "        self.cached_means = None\n",
        "        \n",
        "        # Disable transformations for train_dataset, if available, as we will\n",
        "        # need original PIL images from which to extract features.\n",
        "        if train_dataset is not None: train_dataset.dataset.disable_transform()\n",
        "\n",
        "        for images, labels in self.test_dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                preds = self.classify(images, train_dataset)\n",
        "\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            all_targets = torch.cat(\n",
        "                (all_targets.to(self.device), labels.to(self.device)), dim=0\n",
        "            )\n",
        "\n",
        "            all_preds = torch.cat(\n",
        "                (all_preds.to(self.device), preds.to(self.device)), dim=0\n",
        "            )\n",
        "\n",
        "        if train_dataset is not None: train_dataset.dataset.enable_transform()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = running_corrects / float(total)  \n",
        "\n",
        "        print(f\"Test accuracy (iCaRL): {accuracy} \", end=\"\")\n",
        "\n",
        "        if train_dataset is None:\n",
        "            print(\"(only exemplars)\")\n",
        "        else:\n",
        "            print(\"(exemplars and training data)\")\n",
        "\n",
        "        return accuracy, all_targets, all_preds\n",
        "\n",
        "    def test_without_classifier(self, test_dataset):\n",
        "        \"\"\"Test the model without classifier, using the outputs of the\n",
        "        network instead.\n",
        "\n",
        "        Args:\n",
        "            test_dataset: dataset on which to test the network\n",
        "        Returns:\n",
        "            accuracy (float): accuracy of the model on the test set\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False) # Set Network to evaluation mode\n",
        "        if self.old_net is not None: \n",
        "          self.old_net.train(False)\n",
        "          for i in range(len(self.old_nets)):\n",
        "            self.old_nets[i] = self.old_nets[i].train(False)\n",
        "\n",
        "        self.test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "\n",
        "        all_preds = torch.tensor([]) # to store all predictions\n",
        "        all_preds = all_preds.type(torch.LongTensor)\n",
        "        all_targets = torch.tensor([])\n",
        "        all_targets = all_targets.type(torch.LongTensor)\n",
        "        \n",
        "        for images, labels in self.test_dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Forward Pass\n",
        "            with torch.no_grad():\n",
        "                if self.VALIDATE:\n",
        "                    outputs = self.best_net(images)\n",
        "                else:\n",
        "                    outputs = self.net(images)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update Corrects\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            all_targets = torch.cat(\n",
        "                (all_targets.to(self.device), labels.to(self.device)), dim=0\n",
        "            )\n",
        "\n",
        "            # Append batch predictions\n",
        "            all_preds = torch.cat(\n",
        "                (all_preds.to(self.device), preds.to(self.device)), dim=0\n",
        "            )\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = running_corrects / float(total)  \n",
        "\n",
        "        print(f\"Test accuracy (hybrid1): {accuracy}\")\n",
        "\n",
        "        return accuracy, all_targets, all_preds\n",
        "    \n",
        "    def increment_classes(self, n=10):\n",
        "        \"\"\"Add n classes in the final fully connected layer.\"\"\"\n",
        "\n",
        "        in_features = self.net.fc.in_features  # size of each input sample\n",
        "        out_features = self.net.fc.out_features  # size of each output sample\n",
        "        weight = self.net.fc.weight.data\n",
        "        bias = self.net.fc.bias.data\n",
        "\n",
        "        self.net.fc = nn.Linear(in_features, out_features+n)\n",
        "        self.net.fc.weight.data[:out_features] = weight\n",
        "        self.net.fc.bias.data[:out_features] = bias\n",
        "    \n",
        "    def output_neurons_count(self):\n",
        "        \"\"\"Return the number of output neurons of the current network.\"\"\"\n",
        "\n",
        "        return self.net.fc.out_features\n",
        "    \n",
        "    def feature_neurons_count(self):\n",
        "        \"\"\"Return the number of neurons of the last layer of the feature extractor.\"\"\"\n",
        "\n",
        "        return self.net.fc.in_features\n",
        "    \n",
        "    def to_onehot(self, targets):\n",
        "        \"\"\"Convert targets to one-hot encoding (for BCE loss).\n",
        "\n",
        "        Args:\n",
        "            targets: dataloader.dataset.targets of the new task images\n",
        "        \"\"\"\n",
        "        num_classes = self.net.fc.out_features\n",
        "        one_hot_targets = torch.eye(num_classes)[targets]\n",
        "\n",
        "        return one_hot_targets.to(self.device)\n",
        "\n",
        "    def network_params(self):\n",
        "        weight = self.net.fc.weight.data\n",
        "        bias = self.net.fc.bias.data\n",
        "\n",
        "        return weight, bias\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udUGrKrkNfBU",
        "colab_type": "text"
      },
      "source": [
        "## Pre saved distillation targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7cJ8KoRaYdN",
        "colab_type": "text"
      },
      "source": [
        "### Deterministic Sampler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUzIhV6eDggM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Sampler\n",
        "import math\n",
        "\n",
        "class MyDeterministicSampler(Sampler):\n",
        "    \"\"\"Samples elements randomly. If without replacement, then sample from a shuffled dataset.\n",
        "    If with replacement, then user can specify ``num_samples`` to draw.\n",
        "\n",
        "    Arguments:\n",
        "        data_source (Dataset): dataset to sample from\n",
        "        replacement (bool): samples are drawn with replacement if ``True``, default=``False``\n",
        "        num_samples (int): number of samples to draw, default=`len(dataset)`. This argument\n",
        "            is supposed to be specified only when `replacement` is ``True``.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_source, batch_size=None, drop_last = True, random_state = 653):\n",
        "        self.data_source = data_source\n",
        "        self.batch_size = batch_size\n",
        "        self.drop_last = drop_last\n",
        "\n",
        "        self.num_samples = math.floor(len(self.data_source)/self.batch_size)\n",
        "\n",
        "        indices = np.arange(0, len(self.data_source))\n",
        "\n",
        "        # Make deterministic\n",
        "        np.random.seed(random_state) # seed the generator\n",
        "        np.random.shuffle(indices) # shuffle indices to get balanced distribution in training and validation set      \n",
        "\n",
        "        indices = indices.tolist()\n",
        "        # Define batch indices\n",
        "        self.make_batch(indices)\n",
        "\n",
        "\n",
        "    def make_batch(self, indices):\n",
        "      \n",
        "      self.batch_indices = []\n",
        "      if self.drop_last:\n",
        "        for el in range(self.num_samples):\n",
        "          if el < self.num_samples-1:\n",
        "            self.batch_indices.append([indices[self.batch_size*el : self.batch_size*(el+1)]])\n",
        "      else:\n",
        "        for el in range(self.num_samples):\n",
        "          if el != self.num_samples-1:\n",
        "            self.batch_indices.append([indices[self.batch_size*el : self.batch_size*(el+1)]])\n",
        "          else:\n",
        "            self.batch_indices.append([indices[self.batch_size*el:]])\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "      \n",
        "      for idx in range(self.num_samples-1):\n",
        "        \n",
        "        iter_indices = iter(self.batch_indices[idx][0])\n",
        "        \n",
        "        yield iter_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLyW8sm2adDI",
        "colab_type": "text"
      },
      "source": [
        "### Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qYtfGFRNAcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from math import floor\n",
        "from copy import deepcopy\n",
        "import random\n",
        "\n",
        "sigmoid = nn.Sigmoid() # Sigmoid function\n",
        "\n",
        "class Exemplars(torch.utils.data.Dataset):\n",
        "    def __init__(self, exemplars, transform=None):\n",
        "        # exemplars = [\n",
        "        #     [ex0_class0, ex1_class0, ex2_class0, ...],\n",
        "        #     [ex0_class1, ex1_class1, ex2_class1, ...],\n",
        "        #     ...\n",
        "        #     [ex0_classN, ex1_classN, ex2_classN, ...]\n",
        "        # ]\n",
        "\n",
        "        self.dataset = []\n",
        "        self.targets = []\n",
        "\n",
        "        for y, exemplar_y in enumerate(exemplars):\n",
        "            self.dataset.extend(exemplar_y)\n",
        "            self.targets.extend([y] * len(exemplar_y))\n",
        "\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image = self.dataset[index]\n",
        "        target = self.targets[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "class iCaRLDist:\n",
        "    \"\"\"Implement iCaRL, a strategy for simultaneously learning classifiers and a\n",
        "    feature representation in the class-incremental setting.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device, net, lr, momentum, weight_decay, milestones, gamma, num_epochs, batch_size, train_transform, test_transform, T):\n",
        "        self.device = device\n",
        "        self.net = net\n",
        "\n",
        "        # Set hyper-parameters\n",
        "        self.LR = lr\n",
        "        self.MOMENTUM = momentum\n",
        "        self.WEIGHT_DECAY = weight_decay\n",
        "        self.MILESTONES = milestones\n",
        "        self.GAMMA = gamma\n",
        "        self.NUM_EPOCHS = num_epochs\n",
        "        self.BATCH_SIZE = batch_size\n",
        "        \n",
        "        # Set transformations\n",
        "        self.train_transform = train_transform\n",
        "        self.test_transform = test_transform\n",
        "\n",
        "        # List of exemplar sets. Each set contains memory_size/num_classes exemplars\n",
        "        # with num_classes the number of classes seen until now by the network.\n",
        "        self.exemplars = []\n",
        "\n",
        "        # Initialize the copy of the old network, used to compute outputs of the\n",
        "        # previous network for the distillation loss, to None. This is useful to\n",
        "        # correctly apply the first function when training the network for the\n",
        "        # first time.\n",
        "        self.old_net = None\n",
        "        self.old_nets = []\n",
        "        self.old_targets = []\n",
        "        self.T = T\n",
        "\n",
        "        # Maximum number of exemplars\n",
        "        self.memory_size = 2000\n",
        "    \n",
        "        # Loss function\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        # If True, test on the best model found (e.g., minimize loss). If False,\n",
        "        # test on the last model build (of the last epoch).\n",
        "        self.VALIDATE = False\n",
        "\n",
        "    def classify(self, batch, train_dataset=None):\n",
        "        \"\"\"Mean-of-exemplars classifier used to classify images into the set of\n",
        "        classes observed so far.\n",
        "\n",
        "        Args:\n",
        "            batch (torch.tensor): batch to classify\n",
        "        Returns:\n",
        "            label (int): class label assigned to the image\n",
        "        \"\"\"\n",
        "\n",
        "        batch_features = self.extract_features(batch) # (batch size, 64)\n",
        "        for i in range(batch_features.size(0)):\n",
        "            batch_features[i] = batch_features[i]/batch_features[i].norm() # Normalize sample feature representation\n",
        "        batch_features = batch_features.to(self.device)\n",
        "\n",
        "        if self.cached_means is None:\n",
        "            print(\"Computing mean of exemplars... \", end=\"\")\n",
        "\n",
        "            self.cached_means = []\n",
        "\n",
        "            # Number of known classes\n",
        "            num_classes = len(self.exemplars)\n",
        "\n",
        "            # Compute the means of classes with all the data available,\n",
        "            # including training data which contains samples belonging to\n",
        "            # the latest 10 classes. This will remove noise from the mean\n",
        "            # estimate, improving the results.\n",
        "            if train_dataset is not None:\n",
        "                train_features_list = [[] for _ in range(10)]\n",
        "\n",
        "                for train_sample, label in train_dataset:\n",
        "                    features = self.extract_features(train_sample, batch=False, transform=self.test_transform)\n",
        "                    features = features/features.norm()\n",
        "                    train_features_list[label % 10].append(features)\n",
        "\n",
        "            # Compute means of exemplars for all known classes\n",
        "            for y in range(num_classes):\n",
        "                if (train_dataset is not None) and (y in range(num_classes-10, num_classes)):\n",
        "                    features_list = train_features_list[y % 10]\n",
        "                else:\n",
        "                    features_list = []\n",
        "\n",
        "                for exemplar in self.exemplars[y]:\n",
        "                    features = self.extract_features(exemplar, batch=False, transform=self.test_transform)\n",
        "                    features = features/features.norm() # Normalize the feature representation of the exemplar\n",
        "                    features_list.append(features)\n",
        "                \n",
        "                features_list = torch.stack(features_list)\n",
        "                class_means = features_list.mean(dim=0)\n",
        "                class_means = class_means/class_means.norm() # Normalize the class means\n",
        "\n",
        "                self.cached_means.append(class_means)\n",
        "            \n",
        "            self.cached_means = torch.stack(self.cached_means).to(self.device)\n",
        "            print(\"done\")\n",
        "\n",
        "        preds = []\n",
        "        for i in range(batch_features.size(0)):\n",
        "            f_arg = torch.norm(batch_features[i] - self.cached_means, dim=1)\n",
        "            preds.append(torch.argmin(f_arg))\n",
        "        \n",
        "        return torch.stack(preds)\n",
        "    \n",
        "    def extract_features(self, sample, batch=True, transform=None):\n",
        "        \"\"\"Extract features from single sample or from batch.\n",
        "        \n",
        "        Args:\n",
        "            sample (PIL image or torch.tensor): sample(s) from which to\n",
        "                extract features\n",
        "            batch (bool): if True, sample is a torch.tensor containing a batch\n",
        "                of images with dimensions (batch_size, 3, 32, 32)\n",
        "            transform: transformations to apply to the PIL image before\n",
        "                processing\n",
        "        Returns:\n",
        "            features: torch.tensor, 1-D of dimension 64 for single samples or\n",
        "                2-D of dimension (batch_size, 64) for batch\n",
        "        \"\"\"\n",
        "\n",
        "        assert not (batch is False and transform is None), \"if a PIL image is passed to extract_features, a transform must be defined\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)\n",
        "        if self.old_net is not None: \n",
        "          self.old_net.train(False)\n",
        "          for i in range(len(self.old_nets)):\n",
        "            self.old_nets[i] = self.old_nets[i].train(False)\n",
        "\n",
        "        if batch is False: # Treat sample as single PIL image\n",
        "            sample = transform(sample)\n",
        "            sample = sample.unsqueeze(0) # https://stackoverflow.com/a/59566009/6486336\n",
        "\n",
        "        sample = sample.to(self.device)\n",
        "\n",
        "        if self.VALIDATE:\n",
        "            features = self.best_net.features(sample)\n",
        "        else:\n",
        "            features = self.net.features(sample)\n",
        "\n",
        "        if batch is False:\n",
        "            features = features[0]\n",
        "\n",
        "        return features\n",
        "\n",
        "    def incremental_train(self, split, train_dataset, val_dataset):\n",
        "        \"\"\"Adjust internal knowledge based on the additional information\n",
        "        available in the new observations.\n",
        "\n",
        "        Args:\n",
        "            split (int): current split number, counting from zero\n",
        "            train_dataset: dataset for training the model\n",
        "            val_dataset: dataset for validating the model\n",
        "        Returns:\n",
        "            train_logs: tuple of four metrics (train_loss, train_accuracy,\n",
        "            val_loss, val_accuracy) obtained during network training\n",
        "        \"\"\"\n",
        "\n",
        "        self.split = split\n",
        "\n",
        "        if self.split is not 0:\n",
        "            # Increment the number of output nodes for the new network by 10\n",
        "            self.increment_classes(10)\n",
        "\n",
        "        # Improve network parameters upon receiving new classes. Effectively\n",
        "        # train a new network starting from the current network parameters.\n",
        "        train_logs = self.update_representation(train_dataset, val_dataset)\n",
        "\n",
        "        # Compute the number of exemplars per class\n",
        "        num_classes = self.output_neurons_count()\n",
        "        m = floor(self.memory_size / num_classes)\n",
        "\n",
        "        print(f\"Target number of exemplars per class: {m}\")\n",
        "        print(f\"Target total number of exemplars: {m*num_classes}\")\n",
        "\n",
        "        # Reduce pre-existing exemplar sets in order to fit new exemplars\n",
        "        for y in range(len(self.exemplars)):\n",
        "            self.exemplars[y] = self.reduce_exemplar_set(self.exemplars[y], m)\n",
        "\n",
        "        # Construct exemplar set for new classes\n",
        "        new_exemplars = self.construct_exemplar_set_rand(train_dataset, m)\n",
        "        self.exemplars.extend(new_exemplars)\n",
        "\n",
        "        return train_logs\n",
        "\n",
        "    def update_representation(self, train_dataset, val_dataset):\n",
        "        \"\"\"Update the parameters of the network.\n",
        "\n",
        "        Args:\n",
        "            train_dataset: dataset for training the model\n",
        "            val_dataset: dataset for validating the model\n",
        "        Returns:\n",
        "            train_logs: tuple of four metrics (train_loss, train_accuracy,\n",
        "            val_loss, val_accuracy) obtained during network training\n",
        "        \"\"\"\n",
        "\n",
        "        # Combine the new training data with existing exemplars.\n",
        "        print(f\"Length of exemplars set: {sum([len(self.exemplars[y]) for y in range(len(self.exemplars))])}\")\n",
        "        exemplars_dataset = Exemplars(self.exemplars, self.train_transform)\n",
        "        train_dataset_with_exemplars = ConcatDataset([exemplars_dataset, train_dataset])\n",
        "\n",
        "        # Train the network on combined dataset\n",
        "        train_logs = self.train(train_dataset_with_exemplars, val_dataset) # @todo: include exemplars in validation set?\n",
        "\n",
        "        # Keep a copy of the current network in order to compute its outputs for\n",
        "        # the distillation loss while the new network is being trained.\n",
        "        self.old_net = deepcopy(self.net)\n",
        "        self.old_nets.append(self.old_net)\n",
        "\n",
        "        return train_logs\n",
        "\n",
        "    def construct_exemplar_set_rand(self, dataset, m):\n",
        "        \"\"\"Randomly sample m elements from a dataset without replacement.\n",
        "\n",
        "        Args:\n",
        "            dataset: dataset containing a split (samples from 10 classes) from\n",
        "                which to take exemplars\n",
        "            m (int): target number of exemplars per class\n",
        "        Returns:\n",
        "            exemplars: list of samples extracted from the dataset\n",
        "        \"\"\"\n",
        "\n",
        "        dataset.dataset.disable_transform()\n",
        "\n",
        "        samples = [[] for _ in range(10)]\n",
        "        for image, label in dataset:\n",
        "            label = label % 10 # Map labels to 0-9 range\n",
        "            samples[label].append(image)\n",
        "\n",
        "        dataset.dataset.enable_transform()\n",
        "\n",
        "        exemplars = [[] for _ in range(10)]\n",
        "\n",
        "        for y in range(10):\n",
        "            print(f\"Randomly extracting exemplars from class {y} of current split... \", end=\"\")\n",
        "\n",
        "            # Randomly choose m samples from samples[y] without replacement\n",
        "            exemplars[y] = random.sample(samples[y], m)\n",
        "\n",
        "            print(f\"Extracted {len(exemplars[y])} exemplars.\")\n",
        "\n",
        "        return exemplars\n",
        "\n",
        "    def reduce_exemplar_set(self, exemplar_set, m):\n",
        "        \"\"\"Procedure for removing exemplars from a given set.\n",
        "\n",
        "        Args:\n",
        "            exemplar_set (set): set of exemplars belonging to a certain class\n",
        "            m (int): target number of exemplars\n",
        "        Returns:\n",
        "            exemplar_set: reduced exemplar set\n",
        "        \"\"\"\n",
        "\n",
        "        return exemplar_set[:m]\n",
        "\n",
        "    def train(self, train_dataset, val_dataset):\n",
        "        \"\"\"Train the network for a specified number of epochs, and save\n",
        "        the best performing model on the validation set.\n",
        "        \n",
        "        Args:\n",
        "            train_dataset: dataset for training the model\n",
        "            val_dataset: dataset for validating the model\n",
        "        Returns: train_logs: tuple of four metrics (train_loss, train_accuracy,\n",
        "            val_loss, val_accuracy) obtained during network training. If\n",
        "            validation is enabled, return scores of the best epoch, otherwise\n",
        "            return scores of the last epoch.\n",
        "        \"\"\"\n",
        "        if self.split == 0:\n",
        "          parameters_to_optimize = self.net.parameters()\n",
        "          self.optimizer = optim.SGD(parameters_to_optimize, \n",
        "                                  lr=2,\n",
        "                                  momentum=self.MOMENTUM,\n",
        "                                  weight_decay=self.WEIGHT_DECAY)\n",
        "\n",
        "        else: \n",
        "          parameters_to_optimize = self.net.parameters()\n",
        "          self.optimizer = optim.SGD(parameters_to_optimize, \n",
        "                                    lr=self.LR,\n",
        "                                    momentum=self.MOMENTUM,\n",
        "                                    weight_decay=self.WEIGHT_DECAY)\n",
        "        \n",
        "        # Define the learning rate decaying policy\n",
        "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer,\n",
        "                                                        milestones=self.MILESTONES,\n",
        "                                                        gamma=self.GAMMA)\n",
        "        \n",
        "        my_sampler = MyDeterministicSampler(train_dataset, batch_size=self.BATCH_SIZE, drop_last= True)\n",
        "\n",
        "        # Create DataLoaders for training and validation\n",
        "        self.train_dataloader = DataLoader(train_dataset,  num_workers=4, batch_sampler = my_sampler)\n",
        "        # self.train_dataloader = DataLoader(train_dataset,batch_size=self.BATCH_SIZE, shuffle=False, num_workers=4, drop_last=True, sampler =)\n",
        "        self.val_dataloader = DataLoader(val_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "        # Send networks to chosen device\n",
        "        self.net = self.net.to(self.device)\n",
        "        if self.old_net is not None: \n",
        "          self.old_net.train(False)\n",
        "          for i in range(len(self.old_nets)):\n",
        "            self.old_nets[i] = self.old_nets[i].train(False)\n",
        "\n",
        "        cudnn.benchmark  # Calling this optimizes runtime\n",
        "\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.best_val_accuracy = 0\n",
        "        self.best_train_loss = float('inf')\n",
        "        self.best_train_accuracy = 0\n",
        "        \n",
        "        self.best_net = None\n",
        "        self.best_epoch = -1\n",
        "\n",
        "        for epoch in range(self.NUM_EPOCHS):\n",
        "            # Run an epoch (start counting form 1)\n",
        "            train_loss, train_accuracy = self.do_epoch(epoch+1)\n",
        "        \n",
        "            # Validate after each epoch \n",
        "            val_loss, val_accuracy = self.validate()    \n",
        "\n",
        "            # Validation criterion: best net is the one that minimizes the loss\n",
        "            # on the validation set.\n",
        "            if self.VALIDATE and val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.best_val_accuracy = val_accuracy\n",
        "                self.best_train_loss = train_loss\n",
        "                self.best_train_accuracy = train_accuracy\n",
        "\n",
        "                self.best_net = deepcopy(self.net)\n",
        "                self.best_epoch = epoch\n",
        "                print(\"Best model updated\")\n",
        "\n",
        "        if self.VALIDATE:\n",
        "            val_loss = self.best_val_loss\n",
        "            val_accuracy = self.best_val_accuracy\n",
        "            train_loss = self.best_train_loss\n",
        "            train_accuracy = self.best_train_accuracy\n",
        "\n",
        "            print(f\"Best model found at epoch {self.best_epoch+1}\")\n",
        "\n",
        "        return train_loss, train_accuracy, val_loss, val_accuracy\n",
        "    \n",
        "    def do_epoch(self, current_epoch):\n",
        "        \"\"\"Trains model for one epoch.\n",
        "        \n",
        "        Args:\n",
        "            current_epoch (int): current epoch number (begins from 1)\n",
        "        Returns:\n",
        "            train_loss: average training loss over all batches of the\n",
        "                current epoch.\n",
        "            train_accuracy: training accuracy of the current epoch over\n",
        "                all samples.\n",
        "        \"\"\"\n",
        "\n",
        "        # Set the current network in training mode\n",
        "        self.net.train()\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "        if self.old_net is not None: \n",
        "          self.old_net.train(False)\n",
        "          for i in range(len(self.old_nets)):\n",
        "            self.old_nets[i] = self.old_nets[i].train(False)\n",
        "\n",
        "        running_train_loss = 0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        batch_idx = 0\n",
        "\n",
        "        print(f\"Epoch: {current_epoch}, LR: {self.scheduler.get_last_lr()}\")\n",
        "\n",
        "        for images, labels in self.train_dataloader:\n",
        "            # Clear old targets at  the  beginning of the training of a new split\n",
        "            if current_epoch == 1 and batch_idx == 0:\n",
        "              self.old_targets = []\n",
        "\n",
        "            loss, corrects = self.do_batch(images, labels, current_epoch, batch_idx)\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "            running_corrects += corrects\n",
        "            total += labels.size(0)\n",
        "            batch_idx += 1\n",
        "\n",
        "        self.scheduler.step()\n",
        "\n",
        "        # Calculate average scores\n",
        "        train_loss = running_train_loss / batch_idx # Average over all batches\n",
        "        train_accuracy = running_corrects / float(total) # Average over all samples\n",
        "\n",
        "        print(f\"Train loss: {train_loss}, Train accuracy: {train_accuracy}\")\n",
        "\n",
        "        return train_loss, train_accuracy\n",
        "\n",
        "    def do_batch(self, batch, labels, current_epoch, batch_idx):\n",
        "        \"\"\"Train network for a batch. Loss is applied here.\n",
        "\n",
        "        Args:\n",
        "            batch: batch of data used for training the network\n",
        "            labels: targets of the batch\n",
        "        Returns:\n",
        "            loss: output of the criterion applied\n",
        "            running_corrects: number of correctly classified elements\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        # Zero-ing the gradients\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        # One-hot encoding of labels of the new training data (new classes)\n",
        "        # Size: batch size (rows) by number of classes seen until now (columns)\n",
        "        #\n",
        "        # e.g., suppose we have four images in a batch, and each incremental\n",
        "        #   step adds three new classes. At the second step, the one-hot\n",
        "        #   encoding may return the following tensor:\n",
        "        #\n",
        "        #       tensor([[0., 0., 0., 1., 0., 0.],   # image 0 (label 3)\n",
        "        #               [0., 0., 0., 0., 1., 0.],   # image 1 (label 4)\n",
        "        #               [0., 0., 0., 0., 0., 1.],   # image 2 (label 5)\n",
        "        #               [0., 0., 0., 0., 1., 0.]])  # image 3 (label 4)\n",
        "        #\n",
        "        #   The first three elements of each vector will always be 0, as the\n",
        "        #   new training batch does not contain images belonging to classes\n",
        "        #   already seen in previous steps.\n",
        "        #\n",
        "        #   The last three elements of each vector will contain the actual\n",
        "        #   information about the class of each image (one-hot encoding of the\n",
        "        #   label). Therefore, we slice the tensor and remove the columns \n",
        "        #   related to old classes (all zeros).\n",
        "        num_classes = self.output_neurons_count() # Number of classes seen until now, including new classes\n",
        "        one_hot_labels = self.to_onehot(labels)[:, num_classes-10:num_classes]\n",
        "\n",
        "        if self.old_net is None:\n",
        "            # Network is training for the first time, so we only apply the\n",
        "            # classification loss.\n",
        "            targets = one_hot_labels\n",
        "            # Forward pass\n",
        "            outputs = self.net(batch)\n",
        "\n",
        "        else:\n",
        "          # Pre save targets una tantum during all epochs\n",
        "          # This makes the process faster, avoiding to repeat old net predicitons at each epoch\n",
        "\n",
        "          if current_epoch == 1:\n",
        "            # Old net forward pass. We compute the outputs of the old network\n",
        "            # and apply a sigmoid function. These are used in the distillation\n",
        "            # loss. We discard the output of the new neurons, as they are not\n",
        "            # considered in the distillation loss.\n",
        "            \n",
        "            old_net_outputs = sigmoid(self.old_nets[0](batch)/self.T)\n",
        "            for i, old_net in enumerate(self.old_nets[1:]):\n",
        "              old_net_outputs = torch.cat((old_net_outputs, sigmoid(old_net(batch)[:, (i+1)*10:]/self.T)), dim=1)\n",
        "\n",
        "            # Concatenate the outputs of the old network and the one-hot encoded\n",
        "            # labels along dimension 1 (columns).\n",
        "            # \n",
        "            # Each row refers to an image in the training set, and contains:\n",
        "            # - the output of the old network for that image, used by the\n",
        "            #   distillation loss\n",
        "            # - the one-hot label of the image, used by the classification loss\n",
        "\n",
        "            targets = torch.cat((old_net_outputs, one_hot_labels), dim=1)\n",
        "\n",
        "            # append old nets targets batch by batch.\n",
        "            # Non itero mai su self.old net targets, potendo solo fare backward una volta\n",
        "            # se uso retain graph in backward, mi da memory error\n",
        "            self.old_targets.append(old_net_outputs)\n",
        "\n",
        "          else:\n",
        "            # deep copy since we do not want to iterate over self.old_targets\n",
        "            targets = torch.cat((deepcopy(self.old_targets[batch_idx].clone().detach().requires_grad_(True)), one_hot_labels), dim=1)\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = self.net(batch)\n",
        "          outputs = torch.cat((outputs[:, :num_classes-10]/self.T, outputs[:, num_classes-10:]), dim=1)\n",
        "\n",
        "        \n",
        "        loss = self.criterion(outputs, targets)\n",
        "\n",
        "        # Get predictions\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Accuracy over NEW IMAGES, not over all images\n",
        "        running_corrects = torch.sum(preds == labels.data).data.item() \n",
        "\n",
        "        # Backward pass: computes gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        \n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss, running_corrects\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\"Validate the model.\n",
        "        \n",
        "        Returns:\n",
        "            val_loss: average loss function computed on the network outputs\n",
        "                of the validation set (val_dataloader).\n",
        "            val_accuracy: accuracy computed on the validation set.\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "        if self.old_net is not None: \n",
        "          self.old_net.train(False)\n",
        "          for i in range(len(self.old_nets)):\n",
        "            self.old_nets[i] = self.old_nets[i].train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)\n",
        "\n",
        "        running_val_loss = 0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        batch_idx = 0\n",
        "\n",
        "        for images, labels in self.val_dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # One hot encoding of new task labels \n",
        "            one_hot_labels = self.to_onehot(labels)\n",
        "\n",
        "            # New net forward pass\n",
        "            outputs = self.net(images)  \n",
        "            loss = self.criterion(outputs, one_hot_labels) # BCE Loss with sigmoids over outputs\n",
        "\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update the number of correctly classified validation samples\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            batch_idx += 1\n",
        "\n",
        "        # Calculate scores\n",
        "        val_loss = running_val_loss / batch_idx\n",
        "        val_accuracy = running_corrects / float(total)\n",
        "\n",
        "        print(f\"Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n",
        "\n",
        "        return val_loss, val_accuracy\n",
        "\n",
        "    def test(self, test_dataset, train_dataset=None):\n",
        "        \"\"\"Test the model.\n",
        "\n",
        "        Args:\n",
        "            test_dataset: dataset on which to test the network\n",
        "            train_dataset: training set used to train the last split, if\n",
        "                available\n",
        "        Returns:\n",
        "            accuracy (float): accuracy of the model on the test set\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)  # Set Network to evaluation mode\n",
        "        if self.old_net is not None: \n",
        "          self.old_net.train(False)\n",
        "          for i in range(len(self.old_nets)):\n",
        "            self.old_nets[i] = self.old_nets[i].train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)\n",
        "\n",
        "        self.test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "\n",
        "        # To store all predictions\n",
        "        all_preds = torch.tensor([])\n",
        "        all_preds = all_preds.type(torch.LongTensor)\n",
        "        all_targets = torch.tensor([])\n",
        "        all_targets = all_targets.type(torch.LongTensor)\n",
        "\n",
        "        # Clear mean of exemplars cache\n",
        "        self.cached_means = None\n",
        "        \n",
        "        # Disable transformations for train_dataset, if available, as we will\n",
        "        # need original PIL images from which to extract features.\n",
        "        if train_dataset is not None: train_dataset.dataset.disable_transform()\n",
        "\n",
        "        for images, labels in self.test_dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                preds = self.classify(images, train_dataset)\n",
        "\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            all_targets = torch.cat(\n",
        "                (all_targets.to(self.device), labels.to(self.device)), dim=0\n",
        "            )\n",
        "\n",
        "            all_preds = torch.cat(\n",
        "                (all_preds.to(self.device), preds.to(self.device)), dim=0\n",
        "            )\n",
        "\n",
        "        if train_dataset is not None: train_dataset.dataset.enable_transform()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = running_corrects / float(total)  \n",
        "\n",
        "        print(f\"Test accuracy (iCaRL): {accuracy} \", end=\"\")\n",
        "\n",
        "        if train_dataset is None:\n",
        "            print(\"(only exemplars)\")\n",
        "        else:\n",
        "            print(\"(exemplars and training data)\")\n",
        "\n",
        "        return accuracy, all_targets, all_preds\n",
        "\n",
        "    def test_without_classifier(self, test_dataset):\n",
        "        \"\"\"Test the model without classifier, using the outputs of the\n",
        "        network instead.\n",
        "\n",
        "        Args:\n",
        "            test_dataset: dataset on which to test the network\n",
        "        Returns:\n",
        "            accuracy (float): accuracy of the model on the test set\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False) # Set Network to evaluation mode\n",
        "        if self.old_net is not None: \n",
        "          self.old_net.train(False)\n",
        "          for i in range(len(self.old_nets)):\n",
        "            self.old_nets[i] = self.old_nets[i].train(False)\n",
        "\n",
        "        self.test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "\n",
        "        all_preds = torch.tensor([]) # to store all predictions\n",
        "        all_preds = all_preds.type(torch.LongTensor)\n",
        "        all_targets = torch.tensor([])\n",
        "        all_targets = all_targets.type(torch.LongTensor)\n",
        "        \n",
        "        for images, labels in self.test_dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Forward Pass\n",
        "            with torch.no_grad():\n",
        "                if self.VALIDATE:\n",
        "                    outputs = self.best_net(images)\n",
        "                else:\n",
        "                    outputs = self.net(images)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update Corrects\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            all_targets = torch.cat(\n",
        "                (all_targets.to(self.device), labels.to(self.device)), dim=0\n",
        "            )\n",
        "\n",
        "            # Append batch predictions\n",
        "            all_preds = torch.cat(\n",
        "                (all_preds.to(self.device), preds.to(self.device)), dim=0\n",
        "            )\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = running_corrects / float(total)  \n",
        "\n",
        "        print(f\"Test accuracy (hybrid1): {accuracy}\")\n",
        "\n",
        "        return accuracy, all_targets, all_preds\n",
        "    \n",
        "    def increment_classes(self, n=10):\n",
        "        \"\"\"Add n classes in the final fully connected layer.\"\"\"\n",
        "\n",
        "        in_features = self.net.fc.in_features  # size of each input sample\n",
        "        out_features = self.net.fc.out_features  # size of each output sample\n",
        "        weight = self.net.fc.weight.data\n",
        "        bias = self.net.fc.bias.data\n",
        "\n",
        "        self.net.fc = nn.Linear(in_features, out_features+n)\n",
        "        self.net.fc.weight.data[:out_features] = weight\n",
        "        self.net.fc.bias.data[:out_features] = bias\n",
        "    \n",
        "    def output_neurons_count(self):\n",
        "        \"\"\"Return the number of output neurons of the current network.\"\"\"\n",
        "\n",
        "        return self.net.fc.out_features\n",
        "    \n",
        "    def feature_neurons_count(self):\n",
        "        \"\"\"Return the number of neurons of the last layer of the feature extractor.\"\"\"\n",
        "\n",
        "        return self.net.fc.in_features\n",
        "    \n",
        "    def to_onehot(self, targets):\n",
        "        \"\"\"Convert targets to one-hot encoding (for BCE loss).\n",
        "\n",
        "        Args:\n",
        "            targets: dataloader.dataset.targets of the new task images\n",
        "        \"\"\"\n",
        "        num_classes = self.net.fc.out_features\n",
        "        one_hot_targets = torch.eye(num_classes)[targets]\n",
        "\n",
        "        return one_hot_targets.to(self.device)\n",
        "\n",
        "    def network_params(self):\n",
        "        weight = self.net.fc.weight.data\n",
        "        bias = self.net.fc.bias.data\n",
        "\n",
        "        return weight, bias\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3dtSmVxJKUi",
        "colab_type": "text"
      },
      "source": [
        "### Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aKLPSKn_JWPW",
        "colab": {}
      },
      "source": [
        "# iCaRL hyperparameters\n",
        "LR = 3\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.00001\n",
        "MILESTONES = [44, 63]\n",
        "GAMMA = 0.2\n",
        "NUM_EPOCHS = 70\n",
        "BATCH_SIZE = 64\n",
        "T = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgWHZvsPWuGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define what tests to run\n",
        "TEST_ICARL = True # Run test with iCaRL (exemplars + train dataset)\n",
        "TEST_HYBRID1 = False # Run test with hybrid1\n",
        "\n",
        "# Initialize logs\n",
        "logs_icarl = [[] for _ in range(NUM_RUNS)]\n",
        "logs_hybrid1 = [[] for _ in range(NUM_RUNS)]\n",
        "\n",
        "for run_i in range(NUM_RUNS-2):\n",
        "    net = resnet32()\n",
        "    icarl = iCaRLDist(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS, BATCH_SIZE, train_transform, test_transform, T)\n",
        "\n",
        "    for split_i in range(3):\n",
        "        print(f\"## Split {split_i} of run {run_i} ##\")\n",
        "        \n",
        "        train_logs = icarl.incremental_train(split_i, train_subsets[run_i][split_i], val_subsets[run_i][split_i])\n",
        "\n",
        "        all_targets = torch.stack([label[0] for _, label in DataLoader(test_subsets[run_i][split_i])])\n",
        "\n",
        "        if TEST_ICARL:\n",
        "            logs_icarl[run_i].append({})\n",
        "\n",
        "            acc, all_targets, all_preds = icarl.test(test_subsets[run_i][split_i], train_subsets[run_i][split_i])\n",
        "\n",
        "            logs_icarl[run_i][split_i]['accuracy'] = acc\n",
        "            logs_icarl[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n",
        "\n",
        "            logs_icarl[run_i][split_i]['train_loss'] = train_logs[0]\n",
        "            logs_icarl[run_i][split_i]['train_accuracy'] = train_logs[1]\n",
        "            logs_icarl[run_i][split_i]['val_loss'] = train_logs[2]\n",
        "            logs_icarl[run_i][split_i]['val_accuracy'] = train_logs[3]\n",
        "\n",
        "        if TEST_HYBRID1:\n",
        "            logs_hybrid1[run_i].append({})\n",
        "\n",
        "            acc, all_preds = icarl.test_without_classifier(test_subsets[run_i][split_i])\n",
        "\n",
        "            logs_hybrid1[run_i][split_i]['accuracy'] = acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vr_tjdSeJWPl"
      },
      "source": [
        "### Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HL8BgBfOZIyl",
        "colab": {}
      },
      "source": [
        "train_loss = [[logs_icarl[run_i][i]['train_loss'] for i in range(10)] for run_i in range(NUM_RUNS)]\n",
        "train_accuracy = [[logs_icarl[run_i][i]['train_accuracy'] for i in range(10)] for run_i in range(NUM_RUNS)]\n",
        "val_loss = [[logs_icarl[run_i][i]['val_loss'] for i in range(10)] for run_i in range(NUM_RUNS)]\n",
        "val_accuracy = [[logs_icarl[run_i][i]['val_accuracy'] for i in range(10)] for run_i in range(NUM_RUNS)]\n",
        "test_accuracy = [[logs_icarl[run_i][i]['accuracy'] for i in range(10)] for run_i in range(NUM_RUNS)]\n",
        "\n",
        "train_loss = np.array(train_loss)\n",
        "train_accuracy = np.array(train_accuracy)\n",
        "val_loss = np.array(val_loss)\n",
        "val_accuracy = np.array(val_accuracy)\n",
        "test_accuracy = np.array(test_accuracy)\n",
        "\n",
        "train_loss_stats = np.array([train_loss.mean(0), train_loss.std(0)]).transpose()\n",
        "train_accuracy_stats = np.array([train_accuracy.mean(0), train_accuracy.std(0)]).transpose()\n",
        "val_loss_stats = np.array([val_loss.mean(0), val_loss.std(0)]).transpose()\n",
        "val_accuracy_stats = np.array([val_accuracy.mean(0), val_accuracy.std(0)]).transpose()\n",
        "test_accuracy_stats = np.array([test_accuracy.mean(0), test_accuracy.std(0)]).transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HLmON0tpZIy7",
        "colab": {}
      },
      "source": [
        "plot.train_val_scores(train_loss_stats, train_accuracy_stats, val_loss_stats, val_accuracy_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "58l3FXd1ZIzG",
        "colab": {}
      },
      "source": [
        "plot.test_scores(test_accuracy_stats)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}