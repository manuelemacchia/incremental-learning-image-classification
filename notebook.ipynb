{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bf66f34d57fb4f9692d8284119449282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5641eba324714766b122c68b155fa610",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0c98a9b5df4a4334a5b6a87e0bcfa71e",
              "IPY_MODEL_556589a23f254e2992141f8b584a2867"
            ]
          }
        },
        "5641eba324714766b122c68b155fa610": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0c98a9b5df4a4334a5b6a87e0bcfa71e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_328fa6ce7f3b4c0fad6048b60405f468",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f1df558b4ad24fcf9ad921c1074b55b0"
          }
        },
        "556589a23f254e2992141f8b584a2867": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_47d82c1498e74e3a98826f155f20c7ce",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:20&lt;00:00, 52019562.55it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0d188a0d1fdb40769ac5b98ed2356721"
          }
        },
        "328fa6ce7f3b4c0fad6048b60405f468": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f1df558b4ad24fcf9ad921c1074b55b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "47d82c1498e74e3a98826f155f20c7ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0d188a0d1fdb40769ac5b98ed2356721": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a7992569bcf043908512003e1288b919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_57b29fe6f03445ad8dfca9df4c8e0dca",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4ba3a4c50b0b4282bc25265169c04f10",
              "IPY_MODEL_1cfab0cab132415a9d26bcc312d2db48"
            ]
          }
        },
        "57b29fe6f03445ad8dfca9df4c8e0dca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4ba3a4c50b0b4282bc25265169c04f10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_de0e468df55f4a1497beb3331457ec8c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fbc309f26beb48c5a1412ad75f2c1234"
          }
        },
        "1cfab0cab132415a9d26bcc312d2db48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2b9373ee94674c77979a2e4eea50b566",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:20&lt;00:00, 50921441.94it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5344bb9d3a3549ae89d948480c5cf695"
          }
        },
        "de0e468df55f4a1497beb3331457ec8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fbc309f26beb48c5a1412ad75f2c1234": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2b9373ee94674c77979a2e4eea50b566": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5344bb9d3a3549ae89d948480c5cf695": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NzqxHIh4OCdW"
      },
      "source": [
        "# Incremental learning on image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wBHSznCZxpNB"
      },
      "source": [
        "## Libraries and packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4eQ6O12jxMFf",
        "colab": {}
      },
      "source": [
        "!pip3 install 'torch==1.4.0'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xAYXtIdpx0Yy",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import urllib\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, Subset, DataLoader, ConcatDataset\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet34\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from copy import deepcopy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "09iWc_oCotu2",
        "outputId": "02d51e5b-e205-4eeb-c78e-4d0ea0926759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "# GitHub credentials for cloning private repository\n",
        "username = ''\n",
        "password = ''\n",
        "\n",
        "# Download packages from repository\n",
        "password = urllib.parse.quote(password)\n",
        "!git clone https://$username:$password@github.com/manuelemacchia/incremental-learning-image-classification.git\n",
        "password = ''\n",
        "\n",
        "!mv -v incremental-learning-image-classification/* .\n",
        "!rm -rf incremental-learning-image-classification README.md"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'incremental-learning-image-classification'...\n",
            "remote: Enumerating objects: 144, done.\u001b[K\n",
            "remote: Counting objects: 100% (144/144), done.\u001b[K\n",
            "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
            "remote: Total 375 (delta 70), reused 49 (delta 17), pack-reused 231\u001b[K\n",
            "Receiving objects: 100% (375/375), 2.30 MiB | 8.14 MiB/s, done.\n",
            "Resolving deltas: 100% (183/183), done.\n",
            "renamed 'incremental-learning-image-classification/data' -> './data'\n",
            "renamed 'incremental-learning-image-classification/model' -> './model'\n",
            "renamed 'incremental-learning-image-classification/notebook.ipynb' -> './notebook.ipynb'\n",
            "renamed 'incremental-learning-image-classification/README.md' -> './README.md'\n",
            "renamed 'incremental-learning-image-classification/report' -> './report'\n",
            "renamed 'incremental-learning-image-classification/utils' -> './utils'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QPLViftqtC3I",
        "outputId": "0b8b91c8-da66-418d-920b-6ce063351c29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from data.cifar100 import Cifar100\n",
        "from model.resnet_cifar import resnet32\n",
        "from model.manager import Manager\n",
        "from utils import plot"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j12pgffMR6Qv"
      },
      "source": [
        "## Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JwE0x8gkSisn",
        "colab": {}
      },
      "source": [
        "# Directories\n",
        "DATA_DIR = 'data'       # Directory where the dataset will be downloaded\n",
        "\n",
        "# Settings\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# Dataset\n",
        "RANDOM_STATE = 420      # For reproducibility of results                        \n",
        "                        # Note: different random states give very different\n",
        "                        # splits and therefore very different results.\n",
        "\n",
        "NUM_CLASSES = 100       # Total number of classes\n",
        "NUM_BATCHES = 10\n",
        "CLASS_BATCH_SIZE = 10   # Size of batch of classes for incremental learning\n",
        "\n",
        "VAL_SIZE = 0.1          # Proportion of validation set with respect to training set (between 0 and 1)\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE = 64         # Batch size (iCaRL sets this to 128)\n",
        "LR = 2                  # Initial learning rate\n",
        "                        # iCaRL sets LR = 2. Since they use BinaryCrossEntropy loss it is feasible,\n",
        "                        # in our case it would diverge as we use CrossEntropy loss.\n",
        "MOMENTUM = 0.9          # Momentum for stochastic gradient descent (SGD)\n",
        "WEIGHT_DECAY = 1e-5     # Weight decay from iCaRL\n",
        "\n",
        "NUM_RUNS = 3            # Number of runs of every method\n",
        "                        # Note: this should be at least 3 to have a fair benchmark\n",
        "\n",
        "NUM_EPOCHS = 70         # Total number of training epochs\n",
        "MILESTONES = [49, 63]   # Step down policy from iCaRL (MultiStepLR)\n",
        "                        # Decrease the learning rate by gamma at each milestone\n",
        "GAMMA = 0.2             # Gamma factor from iCaRL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HDDdumxRwbdQ"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "skknIP5Jwspm",
        "colab": {}
      },
      "source": [
        "# Define transformations for training\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Define transformations for evaluation\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5wcci5vi5PIG",
        "colab": {}
      },
      "source": [
        "train_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "val_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "test_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "\n",
        "    test_subsets = []\n",
        "\n",
        "    for split_i in range(CLASS_BATCH_SIZE):\n",
        "\n",
        "        # Download dataset only at first instantiation\n",
        "        if(run_i+split_i == 0):\n",
        "            download = True\n",
        "        else:\n",
        "            download = False\n",
        "\n",
        "        # Create CIFAR100 dataset\n",
        "        train_dataset = Cifar100(DATA_DIR, train=True, download=download,\n",
        "                                 random_state=RANDOM_STATE+run_i, transform=train_transform)\n",
        "        test_dataset = Cifar100(DATA_DIR, train=False, download=False,\n",
        "                                random_state=RANDOM_STATE+run_i, transform=test_transform)\n",
        "\n",
        "        # Subspace of CIFAR100 of 10 classes\n",
        "        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i])\n",
        "        test_dataset.set_classes_batch(\n",
        "            [test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n",
        "\n",
        "        # Define train and validation indices\n",
        "        train_indices, val_indices = train_dataset.train_val_split(\n",
        "            VAL_SIZE, RANDOM_STATE)\n",
        "\n",
        "        train_dataloaders[run_i].append(DataLoader(Subset(train_dataset, train_indices),\n",
        "                                                   batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True))\n",
        "\n",
        "        val_dataloaders[run_i].append(DataLoader(Subset(train_dataset, val_indices),\n",
        "                                                 batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True))\n",
        "\n",
        "        # Dataset with all seen class\n",
        "        test_dataloaders[run_i].append(DataLoader(test_dataset,\n",
        "                                                  batch_size=BATCH_SIZE, shuffle=True, num_workers=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mNwcf1fpsvm_",
        "colab": {}
      },
      "source": [
        "# Sanity check: visualize a batch of images\n",
        "dataiter = iter(test_dataloaders[0][5])\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "plot.image_grid(images, one_channel=False)\n",
        "unique_labels = np.unique(labels, return_counts=True)\n",
        "unique_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QJqnljCV5gJ5"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yya_CxODY4sd",
        "colab": {}
      },
      "source": [
        "# @todo try xavier initialization "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JqA6VD_VxTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss_history = []\n",
        "train_accuracy_history = []\n",
        "val_loss_history = []\n",
        "val_accuracy_history = []\n",
        "test_accuracy_history = []\n",
        "\n",
        "# Iterate over runs\n",
        "for train_dataloader, val_dataloader, test_dataloader in zip(train_dataloaders,\n",
        "                                                             val_dataloaders, test_dataloaders):\n",
        "  \n",
        "    \n",
        "    train_loss_history.append({})\n",
        "    train_accuracy_history.append({})\n",
        "    val_loss_history.append({})\n",
        "    val_accuracy_history.append({})\n",
        "    test_accuracy_history.append({})\n",
        "\n",
        "    net = resnet32()  # Define the net\n",
        "    \n",
        "    criterion = nn.BCEWithLogitsLoss()  # Define the loss      \n",
        "    \n",
        "    i = 0\n",
        "    for train_split, val_split, test_split in zip(train_dataloader,\n",
        "                                                  val_dataloader, test_dataloader):\n",
        "      \n",
        "        \n",
        "        current_split = \"Split %i\"%(i)\n",
        "        print(current_split)\n",
        "\n",
        "        parameters_to_optimize = net.parameters()\n",
        "        optimizer = optim.SGD(parameters_to_optimize, lr=LR,\n",
        "                          momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, \n",
        "                                               milestones=MILESTONES, gamma=GAMMA)\n",
        "\n",
        "        # Define Manager Object\n",
        "        manager = Manager(DEVICE, net, criterion, optimizer, scheduler,\n",
        "                          train_split, val_split, test_split)\n",
        "\n",
        "        scores = manager.train(NUM_EPOCHS)  # train the model\n",
        "\n",
        "        # score[i] = dictionary with key:epoch, value: score\n",
        "        train_loss_history[-1][current_split] = scores[0]\n",
        "        train_accuracy_history[-1][current_split] = scores[1]\n",
        "        val_loss_history[-1][current_split] = scores[2]\n",
        "        val_accuracy_history[-1][current_split] = scores[3]\n",
        "\n",
        "        # Test the model on classes seen until now\n",
        "        test_accuracy, all_preds = manager.test()\n",
        "\n",
        "        test_accuracy_history[-1][current_split] = test_accuracy\n",
        "\n",
        "        # Uncomment if default resnet has 10 node at last FC layer\n",
        "        manager.increment_classes(n=10)  # add 10 nodes to last FC layer\n",
        "\n",
        "        i+=1\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JWdj0wvu996S",
        "colab": {}
      },
      "source": [
        "# Confusion matrix over last run test predictions\n",
        "targets = test_dataset.targets\n",
        "preds = all_preds.to('cpu').numpy()\n",
        "\n",
        "plot.heatmap_cm(targets, preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5ESzk5gF2c_c",
        "colab": {}
      },
      "source": [
        "def mean_std_scores(train_loss_history, train_accuracy_history,\n",
        "                   val_loss_history, val_accuracy_history, test_accuracy_history):\n",
        "  '''\n",
        "      Average the scores of runs different splits\n",
        "  '''\n",
        "  # keys = 'Split i-esim'\n",
        "  keys = train_loss_history[0].keys()\n",
        "\n",
        "  # Containers for average scores\n",
        "  avg_train_loss = {k:[] for k in keys}\n",
        "  avg_train_accuracy = {k:[] for k in keys}\n",
        "  avg_val_loss = {k:[] for k in keys}\n",
        "  avg_val_accuracy = {k:[] for k in keys}\n",
        "  avg_test_accuracy = {k:[] for k in keys}\n",
        "  \n",
        "  train_loss = []\n",
        "  train_accuracy = []\n",
        "  val_loss = []\n",
        "  val_accuracy = []\n",
        "  test_accuracy = []\n",
        "\n",
        "  for key in keys:\n",
        "    for run in range(NUM_RUNS):\n",
        "\n",
        "      # Append all i-th scores (split i-esim) for the different runs\n",
        "      avg_train_loss[key].append(train_loss_history[run][key])\n",
        "      avg_train_accuracy[key].append(train_accuracy_history[run][key])\n",
        "      avg_val_loss[key].append(val_loss_history[run][key])\n",
        "      avg_val_accuracy[key].append(val_accuracy_history[run][key])\n",
        "      avg_test_accuracy[key].append(test_accuracy_history[run][key])\n",
        "\n",
        "    # Define (mean, std) of the i-th score for each split\n",
        "    train_loss.append([np.array(avg_train_loss[key]).mean(), np.array(avg_train_loss[key]).std()])\n",
        "    train_accuracy.append([np.array(avg_train_accuracy[key]).mean(), np.array(avg_train_accuracy[key]).std()])\n",
        "    val_loss.append([np.array(avg_val_loss[key]).mean(), np.array(avg_val_loss[key]).std()])\n",
        "    val_accuracy.append([np.array(avg_val_accuracy[key]).mean(), np.array(avg_val_accuracy[key]).std()])\n",
        "    test_accuracy.append([np.array(avg_test_accuracy[key]).mean(), np.array(avg_test_accuracy[key]).std()])\n",
        "\n",
        "  train_loss = np.array(train_loss)\n",
        "  train_accuracy = np.array(train_accuracy)\n",
        "  val_loss = np.array(val_loss)\n",
        "  val_accuracy = np.array(val_accuracy)\n",
        "  test_accuracy = np.array(test_accuracy)\n",
        "\n",
        "  # Return averaged scores\n",
        "  return(train_loss, train_accuracy, val_loss, val_accuracy, test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dKCztHSw7lHB",
        "colab": {}
      },
      "source": [
        "# Get the average scores\n",
        "train_loss, train_accuracy, val_loss, val_accuracy,\\\n",
        "test_accuracy = mean_std_scores(train_loss_history, train_accuracy_history,\n",
        "                                   val_loss_history, val_accuracy_history, test_accuracy_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7TAdCtXDEa5d",
        "colab": {}
      },
      "source": [
        "plot.train_val_scores(train_loss, train_accuracy, val_loss, val_accuracy, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E1APlRtTpmkK",
        "colab": {}
      },
      "source": [
        "plot.test_scores(test_accuracy, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dlbZp6TJzuLZ",
        "colab": {}
      },
      "source": [
        "# @todo: create utils package for functions\n",
        "\n",
        "import ast\n",
        "\n",
        "def load_json_scores(root):\n",
        "\n",
        "  with open(os.path.join(root, 'train_accuracy_history.json')) as f:\n",
        "      train_accuracy_history = ast.literal_eval(f.read())\n",
        "\n",
        "  with open(os.path.join(root, 'train_loss_history.json')) as f:\n",
        "      train_loss_history = ast.literal_eval(f.read())\n",
        "\n",
        "  with open(os.path.join(root, 'val_accuracy_history.json')) as f:\n",
        "      val_accuracy_history = ast.literal_eval(f.read())\n",
        "\n",
        "  with open(os.path.join(root, 'val_loss_history.json')) as f:\n",
        "      val_loss_history = ast.literal_eval(f.read())\n",
        "\n",
        "  with open(os.path.join(root, 'test_accuracy_history.json')) as f:\n",
        "      test_accuracy_history = ast.literal_eval(f.read())\n",
        "\n",
        "  return(train_loss_history, train_accuracy_history, val_loss_history,\n",
        "         val_accuracy_history, test_accuracy_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tdI9oGAN3heI",
        "colab": {}
      },
      "source": [
        "# @todo: create utils package for functions\n",
        "import json\n",
        "\n",
        "def save_json_scores(root, train_loss_history, train_accuracy_history,\n",
        "                   val_loss_history, val_accuracy_history, test_accuracy_history):\n",
        "\n",
        "    with open(os.path.join(root, 'train_loss_history.json'), 'w') as fout:\n",
        "        json.dump(train_loss_history, fout)\n",
        "\n",
        "    with open(os.path.join(root, 'train_accuracy_history.json'), 'w') as fout:\n",
        "        json.dump(train_accuracy_history, fout)\n",
        "\n",
        "    with open(os.path.join(root, 'val_loss_history.json'), 'w') as fout:\n",
        "        json.dump(val_loss_history, fout)\n",
        "\n",
        "    with open(os.path.join(root, 'val_accuracy_history.json'), 'w') as fout:\n",
        "        json.dump(val_accuracy_history, fout)\n",
        "\n",
        "    with open(os.path.join(root, 'test_accuracy_history.json'), 'w') as fout:\n",
        "        json.dump(test_accuracy_history, fout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S_ZTJSSRd_Ea",
        "colab": {}
      },
      "source": [
        "save_json_scores('scores', train_loss_history, train_accuracy_history,\n",
        "                   val_loss_history, val_accuracy_history, test_accuracy_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QnBlpXBme3L-",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "!zip -r scores.zip scores\n",
        "files.download(\"scores.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJ_Z48QmQ2C",
        "colab_type": "text"
      },
      "source": [
        "## Learning Without Forgetting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERL_PF-cm1N_",
        "colab_type": "text"
      },
      "source": [
        "### Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JHBfXPTXm16d",
        "colab": {}
      },
      "source": [
        "# Training settings for Learning Without Forgetting\n",
        "RANDOM_STATE = 420\n",
        "BATCH_SIZE = 128\n",
        "LR = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EHqtSdwzm16h"
      },
      "source": [
        "### Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "373M_sOAm16i",
        "colab": {}
      },
      "source": [
        "# Transformations for Learning Without Forgetting\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "97Gi3Sp8m16k",
        "outputId": "6d87798a-78c7-4411-e99f-401cce48a388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "bf66f34d57fb4f9692d8284119449282",
            "5641eba324714766b122c68b155fa610",
            "0c98a9b5df4a4334a5b6a87e0bcfa71e",
            "556589a23f254e2992141f8b584a2867",
            "328fa6ce7f3b4c0fad6048b60405f468",
            "f1df558b4ad24fcf9ad921c1074b55b0",
            "47d82c1498e74e3a98826f155f20c7ce",
            "0d188a0d1fdb40769ac5b98ed2356721"
          ]
        }
      },
      "source": [
        "train_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "val_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "test_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "\n",
        "  test_subsets = []\n",
        "\n",
        "  for split_i in range(CLASS_BATCH_SIZE):\n",
        "\n",
        "    # Download dataset only at first instantiation\n",
        "    if(run_i+split_i == 0):\n",
        "      download = True\n",
        "    else:\n",
        "      download = False\n",
        "\n",
        "    # Create CIFAR100 dataset\n",
        "    train_dataset = Cifar100(DATA_DIR, train = True, download = download, random_state = RANDOM_STATE+run_i, transform=train_transform)\n",
        "    test_dataset = Cifar100(DATA_DIR, train = False, download = False, random_state = RANDOM_STATE+run_i, transform=test_transform)\n",
        "   \n",
        "    # Subspace of CIFAR100 of 10 classes\n",
        "    train_dataset.set_classes_batch(train_dataset.batch_splits[split_i]) \n",
        "    test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n",
        "\n",
        "    # Define train and validation indices\n",
        "    train_indices, val_indices = train_dataset.train_val_split(VAL_SIZE, RANDOM_STATE)\n",
        "    \n",
        "    train_dataloaders[run_i].append(DataLoader(Subset(train_dataset, train_indices), \n",
        "                               batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True))\n",
        "    \n",
        "    val_dataloaders[run_i].append(DataLoader(Subset(train_dataset, val_indices), \n",
        "                                batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True))\n",
        "    \n",
        "    # Dataset with all seen class\n",
        "    test_dataloaders[run_i].append(DataLoader(test_dataset, \n",
        "                               batch_size=BATCH_SIZE, shuffle=True, num_workers=4))           "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf66f34d57fb4f9692d8284119449282",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ksaz2qZ5m16n",
        "colab": {}
      },
      "source": [
        "# Sanity check: visualize a batch of images\n",
        "dataiter = iter(test_dataloaders[0][5])\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "plot.image_grid(images, one_channel=False)\n",
        "unique_labels = np.unique(labels, return_counts=True)\n",
        "unique_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw6a_xAumXQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn import BCEWithLogitsLoss\n",
        "from copy import deepcopy\n",
        "\n",
        "'''BCE formulation:\n",
        " let x = logits, z = labels. The logistic loss is\n",
        "\n",
        "  z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n",
        "'''\n",
        "\n",
        "   \n",
        "CLASS_BATCH_SIZE = 10\n",
        "\n",
        "\n",
        "class LWF():\n",
        "  def __init__(self, device, net, old_net, criterion, optimizer, scheduler,\n",
        "               train_dataloader, val_dataloader, test_dataloader, num_classes=10):\n",
        "    \n",
        "    self.device = device\n",
        "\n",
        "    self.net = net\n",
        "    self.best_net = self.net\n",
        "    self.old_net = old_net # None for first ten classes\n",
        "\n",
        "    self.criterion = BCEWithLogitsLoss() # Classifier criterion \n",
        "    self.optimizer = optimizer\n",
        "    self.scheduler = scheduler\n",
        "\n",
        "    self.train_dataloader = train_dataloader\n",
        "    self.val_dataloader = val_dataloader\n",
        "    self.test_dataloader = test_dataloader\n",
        "\n",
        "    self.num_classes = num_classes # can be incremented ouitside methods in the main, or inside methods\n",
        "    self.order = np.arange(100)\n",
        "\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "  def warm_up():\n",
        "    pass\n",
        "\n",
        "  def increment_classes(self, n=10):\n",
        "    \"\"\"Add n classes in the final fully connected layer.\"\"\"\n",
        "\n",
        "    in_features = self.net.fc.in_features  # size of each input sample\n",
        "    out_features = self.net.fc.out_features  # size of each output sample\n",
        "    weight = self.net.fc.weight.data\n",
        "\n",
        "    self.net.fc = nn.Linear(in_features, out_features+n)\n",
        "    self.net.fc.weight.data[:out_features] = weight\n",
        "\n",
        "  def to_onehot(self, targets): \n",
        "    '''\n",
        "    Args:\n",
        "    targets : dataloader.dataset.targets of the new task images\n",
        "    '''\n",
        "    one_hot_targets = torch.eye(self.num_classes)[targets]\n",
        "\n",
        "    return one_hot_targets.to(self.device)\n",
        "\n",
        "  def do_first_batch(self, batch, labels):\n",
        "\n",
        "    batch = batch.to(self.device)\n",
        "    labels = labels.to(self.device) # new classes labels\n",
        "\n",
        "    # Zero-ing the gradients\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "    # One hot encoding of new task labels \n",
        "    one_hot_labels = self.to_onehot(labels) # Size = [128, 10]\n",
        "\n",
        "    # New net forward pass\n",
        "    outputs = self.net(batch)  \n",
        "    \n",
        "    loss = self.criterion(outputs, one_hot_labels) # BCE Loss with sigmoids over outputs\n",
        "\n",
        "    # Get predictions\n",
        "    _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "    # Accuracy over NEW IMAGES, not over all images\n",
        "    running_corrects = \\\n",
        "        torch.sum(preds == labels.data).data.item() # Può essere che debba usare targets e non labels\n",
        "\n",
        "    # Backward pass: computes gradients\n",
        "    loss.backward()\n",
        "\n",
        "    self.optimizer.step()\n",
        "\n",
        "    return loss, running_corrects\n",
        "\n",
        "\n",
        "  def do_batch(self, batch, labels):\n",
        "\n",
        "    batch = batch.to(self.device)\n",
        "    labels = labels.to(self.device) # new classes labels\n",
        "\n",
        "    # Zero-ing the gradients\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "    # One hot encoding of new task labels \n",
        "    one_hot_labels = self.to_onehot(labels) # Size = [128, n_classes] will be sliced as [:, :self.num_classes-10]\n",
        "    new_classes = (self.order[range(self.num_classes-10, self.num_classes)]).astype(np.int32)\n",
        "    one_hot_labels = torch.stack([one_hot_labels[:, i] for i in new_classes], axis=1)\n",
        "\n",
        "    # Old net forward pass\n",
        "    old_outputs = self.sigmoid(self.old_net(batch)) # Size = [128, 100]\n",
        "    old_classes = (self.order[range(self.num_classes-10)]).astype(np.int32)\n",
        "    old_outputs = torch.stack([old_outputs[:, i] for i in old_classes], axis =1)\n",
        "    \n",
        "    # Combine new and old class targets\n",
        "    targets = torch.cat((old_outputs, one_hot_labels), 1)\n",
        "\n",
        "    # New net forward pass\n",
        "    outputs = self.net(batch) # Size = [128, 100] comparable with the define targets\n",
        "    out_classes = (self.order[range(self.num_classes)]).astype(np.int32)\n",
        "    outputs = torch.stack([outputs[:, i] for i in out_classes], axis=1)\n",
        "  \n",
        "    \n",
        "    loss = self.criterion(outputs, targets) # BCE Loss with sigmoids over outputs (over targets must be done manually)\n",
        "\n",
        "    # Get predictions\n",
        "    _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "    # Accuracy over NEW IMAGES, not over all images\n",
        "    running_corrects = \\\n",
        "        torch.sum(preds == labels.data).data.item() \n",
        "\n",
        "    # Backward pass: computes gradients\n",
        "    loss.backward()\n",
        "\n",
        "    self.optimizer.step()\n",
        "\n",
        "    return loss, running_corrects\n",
        "\n",
        "\n",
        "  def do_epoch(self, current_epoch):\n",
        "\n",
        "    self.net.train()\n",
        "\n",
        "    running_train_loss = 0\n",
        "    running_corrects = 0\n",
        "    total = 0\n",
        "    batch_idx = 0\n",
        "\n",
        "    print(f\"Epoch: {current_epoch}, LR: {self.scheduler.get_last_lr()}\")\n",
        "\n",
        "    for images, labels in self.train_dataloader:\n",
        "\n",
        "      if self.num_classes == CLASS_BATCH_SIZE:\n",
        "        loss, corrects = self.do_first_batch(images, labels)\n",
        "      else:\n",
        "        loss, corrects = self.do_batch(images, labels)\n",
        "\n",
        "      running_train_loss += loss.item()\n",
        "      running_corrects += corrects\n",
        "      total += labels.size(0)\n",
        "      batch_idx += 1\n",
        "\n",
        "    self.scheduler.step()\n",
        "\n",
        "    # Calculate average scores\n",
        "    train_loss = running_train_loss / batch_idx # Average over all batches\n",
        "    train_accuracy = running_corrects / float(total) # Average over all samples\n",
        "\n",
        "    print(f\"Train loss: {train_loss}, Train accuracy: {train_accuracy}\")\n",
        "\n",
        "    return (train_loss, train_accuracy)\n",
        "\n",
        "\n",
        "  def train(self, num_epochs):\n",
        "    \"\"\"Train the network for a specified number of epochs, and save\n",
        "    the best performing model on the validation set.\n",
        "    \n",
        "    Args:\n",
        "        num_epochs (int): number of epochs for training the network.\n",
        "    Returns:\n",
        "        train_loss: loss computed on the last epoch\n",
        "        train_accuracy: accuracy computed on the last epoch\n",
        "        val_loss: average loss on the validation set of the last epoch\n",
        "        val_accuracy: accuracy on the validation set of the last epoch\n",
        "    \"\"\"\n",
        "\n",
        "    # @todo: is the return behaviour intended? (scores of the last epoch)\n",
        "\n",
        "    self.net = self.net.to(self.device)\n",
        "    if self.old_net != None:\n",
        "      self.old_net = self.old_net.to(self.device)\n",
        "      self.old_net.train(False)\n",
        "\n",
        "    cudnn.benchmark  # Calling this optimizes runtime\n",
        "\n",
        "    self.best_loss = float(\"inf\")\n",
        "    self.best_epoch = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Run an epoch (start counting form 1)\n",
        "        train_loss, train_accuracy = self.do_epoch(epoch+1)\n",
        "    \n",
        "        # Validate after each epoch \n",
        "        val_loss, val_accuracy = self.validate()    \n",
        "\n",
        "        # Best validation model\n",
        "        if val_loss < self.best_loss:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_net = deepcopy(self.net)\n",
        "            self.best_epoch = epoch\n",
        "            print(\"Best model updated\")\n",
        "\n",
        "        print(\"\")\n",
        "\n",
        "    return (train_loss, train_accuracy,\n",
        "            val_loss, val_accuracy)\n",
        "\n",
        "\n",
        "  def validate(self):\n",
        "    \"\"\"Validate the model.\n",
        "    \n",
        "    Returns:\n",
        "        val_loss: average loss function computed on the network outputs\n",
        "            of the validation set (val_dataloader).\n",
        "        val_accuracy: accuracy computed on the validation set.\n",
        "    \"\"\"\n",
        "\n",
        "    self.net.train(False)\n",
        "\n",
        "    running_val_loss = 0\n",
        "    running_corrects = 0\n",
        "    total = 0\n",
        "    batch_idx = 0\n",
        "\n",
        "\n",
        "    for batch, labels in self.val_dataloader:\n",
        "      batch = batch.to(self.device)\n",
        "      labels = labels.to(self.device)\n",
        "      total += labels.size(0)\n",
        "\n",
        "      # One hot encoding of new task labels \n",
        "      one_hot_labels = self.to_onehot(labels) # Size = [128, 100] will be sliced as [:, :self.num_classes-10]\n",
        "      new_classes = (self.order[range(self.num_classes-10, self.num_classes)]).astype(np.int32)\n",
        "      one_hot_labels = torch.stack([one_hot_labels[:, i] for i in new_classes], axis=1)\n",
        "\n",
        "      if self.num_classes > 10:\n",
        "        # Old net forward pass\n",
        "        old_outputs = self.sigmoid(self.old_net(batch)) # Size = [128, 100]\n",
        "        old_classes = (self.order[range(self.num_classes-10)]).astype(np.int32)\n",
        "        old_outputs = torch.stack([old_outputs[:, i] for i in old_classes], axis =1)\n",
        "\n",
        "        # Combine new and old class targets\n",
        "        targets = torch.cat((old_outputs, one_hot_labels), 1)\n",
        "\n",
        "      else:\n",
        "        targets = one_hot_labels\n",
        "\n",
        "      # New net forward pass\n",
        "      outputs = self.net(batch) # Size = [128, 100] comparable with the define targets\n",
        "      out_classes = (self.order[range(self.num_classes)]).astype(np.int32)\n",
        "      outputs = torch.stack([outputs[:, i] for i in out_classes], axis=1)\n",
        "\n",
        "      \n",
        "      loss = self.criterion(outputs, targets) # BCE Loss with sigmoids over outputs (over targets must be done manually)\n",
        "\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "      # Update the number of correctly classified validation samples\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "      running_val_loss += loss.item()\n",
        "\n",
        "      batch_idx += 1\n",
        "\n",
        "    # Calcuate scores\n",
        "    val_loss = running_val_loss / batch_idx\n",
        "    val_accuracy = running_corrects / float(total)\n",
        "\n",
        "    print(f\"Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n",
        "\n",
        "    return (val_loss, val_accuracy)\n",
        "\n",
        "\n",
        "  def test(self):\n",
        "    \"\"\"Test the model.\n",
        "    Returns:\n",
        "        accuracy (float): accuracy of the model on the test set\n",
        "    \"\"\"\n",
        "\n",
        "    self.best_net.train(False)  # Set Network to evaluation mode\n",
        "\n",
        "    running_corrects = 0\n",
        "    total = 0\n",
        "\n",
        "    all_preds = torch.tensor([]) # to store all predictions\n",
        "    all_preds = all_preds.type(torch.LongTensor)\n",
        "    \n",
        "    for images, labels in self.test_dataloader:\n",
        "      images = images.to(self.device)\n",
        "      labels = labels.to(self.device)\n",
        "      total += labels.size(0)\n",
        "\n",
        "      # Forward Pass\n",
        "      outputs = self.best_net(images)\n",
        "\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "      # Update Corrects\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "      # Append batch predictions\n",
        "      all_preds = torch.cat(\n",
        "          (all_preds.to(self.device), preds.to(self.device)), dim=0\n",
        "      )\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = running_corrects / float(total)  \n",
        "\n",
        "    print(f\"Test accuracy: {accuracy}\")\n",
        "\n",
        "    return (accuracy, all_preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlThDLCvXJwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss_history = []\n",
        "train_accuracy_history = []\n",
        "val_loss_history = []\n",
        "val_accuracy_history = []\n",
        "test_accuracy_history = []\n",
        "\n",
        "\n",
        "\n",
        "# Iterate over runs\n",
        "for train_dataloader, val_dataloader, test_dataloader in zip(train_dataloaders,\n",
        "                                                             val_dataloaders, test_dataloaders):\n",
        "  \n",
        "    \n",
        "    train_loss_history.append({})\n",
        "    train_accuracy_history.append({})\n",
        "    val_loss_history.append({})\n",
        "    val_accuracy_history.append({})\n",
        "    test_accuracy_history.append({})\n",
        "\n",
        "    net = resnet32()  # Define the net\n",
        "    \n",
        "    criterion = nn.BCEWithLogitsLoss()  # Define the loss\n",
        "        \n",
        "    \n",
        "    i = 0\n",
        "    for train_split, val_split, test_split in zip(train_dataloader,\n",
        "                                                  val_dataloader, test_dataloader):\n",
        "      \n",
        "      # Redefine optimizer at each split (pass by reference issue)\n",
        "      parameters_to_optimize = net.parameters()\n",
        "      optimizer = optim.SGD(parameters_to_optimize, lr=LR,\n",
        "                            momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "      scheduler = optim.lr_scheduler.MultiStepLR(optimizer, \n",
        "                                                milestones=MILESTONES, gamma=GAMMA)\n",
        "        \n",
        "      current_split = \"Split %i\"%(i)\n",
        "      print(current_split)\n",
        "\n",
        "      num_classes = CLASS_BATCH_SIZE*(i+1)\n",
        "\n",
        "      if num_classes == CLASS_BATCH_SIZE:\n",
        "        # Old Network = None\n",
        "        lwf = LWF(DEVICE, net, None, criterion, optimizer, scheduler,\n",
        "                          train_split, val_split, test_split, num_classes)\n",
        "      else:\n",
        "        lwf = LWF(DEVICE, net, old_net, criterion, optimizer, scheduler,\n",
        "                        train_split, val_split, test_split, num_classes)\n",
        "        \n",
        "\n",
        "      scores = lwf.train(NUM_EPOCHS)  # train the model\n",
        "\n",
        "      # score[i] = dictionary with key:epoch, value: score\n",
        "      train_loss_history[-1][current_split] = scores[0]\n",
        "      train_accuracy_history[-1][current_split] = scores[1]\n",
        "      val_loss_history[-1][current_split] = scores[2]\n",
        "      val_accuracy_history[-1][current_split] = scores[3]\n",
        "\n",
        "      # Test the model on classes seen until now\n",
        "      test_accuracy, all_preds = lwf.test()\n",
        "\n",
        "      test_accuracy_history[-1][current_split] = test_accuracy\n",
        "\n",
        "      # Uncomment if default resnet has 10 node at last FC layer\n",
        "      old_net = deepcopy(lwf.net)\n",
        "      lwf.increment_classes()\n",
        "\n",
        "      i =i+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B6CcDcDlMf_",
        "colab_type": "text"
      },
      "source": [
        "## iCaRL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4ZuxPA0lPPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @todo: move to package once finished\n",
        "# @todo: provide an uncommented .py for easier editing of class for ablation studies\n",
        "\n",
        "from math import floor\n",
        "from copy import deepcopy\n",
        "import random\n",
        "\n",
        "sigmoid = nn.Sigmoid() # Sigmoid function\n",
        "\n",
        "class Exemplars(torch.utils.data.Dataset):\n",
        "    dataset = []\n",
        "    targets = []\n",
        "\n",
        "    def __init__(self, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "    def add_exemplar(self, sample, target):\n",
        "        self.dataset.append(sample)\n",
        "        self.targets.append(target)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image = self.dataset[index]\n",
        "        target = self.targets[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "class iCaRL:\n",
        "    \"\"\"Implement iCaRL, a strategy for simultaneously learning classifiers and a\n",
        "    feature representation in the class-incremental setting.\n",
        "\n",
        "    Args:\n",
        "        ...\n",
        "    \"\"\"\n",
        "\n",
        "    # Maximum number of exemplars\n",
        "    memory_size = 2000\n",
        "\n",
        "    # List of exemplar sets. Each set contains memory_size/num_classes exemplars\n",
        "    # with num_classes the number of classes seen until now by the network.\n",
        "    exemplars = []\n",
        "\n",
        "    # Initialize the copy of the old network, used to compute outputs of the\n",
        "    # previous network for the distillation loss, to None. This is useful to\n",
        "    # correctly apply the first function when training the network for the first\n",
        "    # time.\n",
        "    old_net = None\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.BCEWithLogitsLoss() # @todo: should this be reduction='sum'?\n",
        "\n",
        "    def __init__(self, device, net, lr, momentum, weight_decay, milestones, gamma, num_epochs, batch_size, train_transform, test_transform):\n",
        "        self.device = device\n",
        "        self.net = net\n",
        "\n",
        "        # Set hyper-parameters\n",
        "        self.LR = lr\n",
        "        self.MOMENTUM = momentum\n",
        "        self.WEIGHT_DECAY = weight_decay\n",
        "        self.MILESTONES = milestones\n",
        "        self.GAMMA = gamma\n",
        "        self.NUM_EPOCHS = num_epochs\n",
        "        self.BATCH_SIZE = batch_size\n",
        "        \n",
        "        self.train_transform = train_transform\n",
        "        self.test_transform = test_transform\n",
        "\n",
        "    def classify(self, image):\n",
        "        \"\"\"Mean-of-exemplars classifier used to classify images into the set of\n",
        "        classes observed so far.\n",
        "\n",
        "        Args:\n",
        "            image (torch.tensor): sample to classify\n",
        "        Returns:\n",
        "            label (int): class label assigned to the image\n",
        "        \"\"\"\n",
        "\n",
        "        features = self.extract_features(image, pil=False).to(self.device)\n",
        "\n",
        "        # @todo (?): post di Cermelli su forum:\n",
        "        # 3) Compute the means for the testing with all the data available.\n",
        "        # That means: don't compute the means on the new exemplars only,\n",
        "        # but on all the images you have available for that step (e.g in\n",
        "        # the last step you have 2000/90=22 images for each previous class and\n",
        "        # all the images for the 10 novel ones.  Don't compute the means on\n",
        "        # the novel exemplars which are only 20 per class, but on all the\n",
        "        # data you have - this will remove noise from the mean estimate,\n",
        "        # improving the results).\n",
        "\n",
        "        if self.cached_means is None:\n",
        "            means = torch.stack([self.mean_of_exemplars(y) for y in range(len(self.exemplars))])\n",
        "        else:\n",
        "            means = self.cached_means\n",
        "\n",
        "        means = means.to(self.device)\n",
        "        \n",
        "        f_arg = torch.norm(features - means, dim=1)\n",
        "\n",
        "        label = torch.argmin(f_arg)\n",
        "        \n",
        "        return label\n",
        "    \n",
        "    def mean_of_exemplars(self, class_id):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            ...\n",
        "        \"\"\"\n",
        "\n",
        "        # See iCaRL algorithm 1\n",
        "        exemplar_features = torch.stack(\n",
        "            [self.extract_features(self.exemplars[class_id][i]) for i in range(len(self.exemplars[class_id]))]\n",
        "        ).to(self.device)\n",
        "\n",
        "        return exemplar_features.mean(dim=0)\n",
        "\n",
        "    def cache_mean_of_exemplars(self):\n",
        "        \"\"\"Compute the mean of the current exemplar sets and cache it to speed\n",
        "        up testing.\"\"\"\n",
        "\n",
        "        # Compute the mean of exemplars for all available classes.\n",
        "        # means is a 2-dimensional tensor, the i-th row containing the mean of \n",
        "        # exemplars in the i-th class.\n",
        "        self.cached_means = torch.stack([self.mean_of_exemplars(y) for y in range(len(self.exemplars))])\n",
        "\n",
        "    def clear_cache_mean_of_exemplars(self):\n",
        "        self.cached_means = None\n",
        "    \n",
        "    def extract_features(self, sample, pil=True):\n",
        "        \"\"\"Extract features from sample.\n",
        "\n",
        "        Args:\n",
        "            sample: sample from which to extract features.\n",
        "            pil (bool): if True, sample is treated as a PIL image and\n",
        "                train_transform is applied. Otherwise, sample is treated\n",
        "                as a tensor and no transforms are applied.\n",
        "        Returns:\n",
        "            tensor: features extracted from the sample.\n",
        "        \"\"\"\n",
        "\n",
        "        self.best_net.train(False)\n",
        "\n",
        "        # Transform sample if it is a PIL image\n",
        "        if pil is True:\n",
        "            sample = self.test_transform(sample) # @todo: should this just be ToTensor() + Normalize?\n",
        "            sample = sample.unsqueeze(0) # https://stackoverflow.com/a/59566009/6486336\n",
        "        \n",
        "        sample = sample.to(self.device)\n",
        "\n",
        "        return self.best_net.features(sample)[0]\n",
        "\n",
        "    def incremental_train(self, split, train_dataset, val_dataset):\n",
        "        \"\"\"Adjust internal knowledge based on the additional information\n",
        "        available in the new observations.\n",
        "\n",
        "        Args:\n",
        "            ...\n",
        "        Returns:\n",
        "            ...\n",
        "        \"\"\"\n",
        "\n",
        "        if split is not 0:\n",
        "            # Increment the number of output nodes for the new network by 10\n",
        "            self.increment_classes(10)\n",
        "\n",
        "        # Improve network parameters upon receiving new classes. Effectively\n",
        "        # train a new network starting from the current network parameters.\n",
        "        self.update_representation(train_dataset, val_dataset)\n",
        "\n",
        "        # Compute the number of exemplars per class\n",
        "        num_classes = self.output_neurons_count()\n",
        "        m = floor(self.memory_size / num_classes)\n",
        "\n",
        "        print(f\"Target number of exemplars per class: {m}\")\n",
        "        print(f\"Total number of exemplars: {m*num_classes}\")\n",
        "\n",
        "        # Reduce pre-existing exemplar sets in order to fit new exemplars\n",
        "        for y in range(len(self.exemplars)):\n",
        "            self.exemplars[y] = self.reduce_exemplar_set(self.exemplars[y], m)\n",
        "\n",
        "        # Construct exemplar set for new classes\n",
        "        new_exemplars = self.construct_exemplar_set_rand(train_dataset, m)\n",
        "        self.exemplars.extend(new_exemplars)\n",
        "\n",
        "    def update_representation(self, train_dataset, val_dataset):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            ...\n",
        "        \"\"\"\n",
        "\n",
        "        # Define the optimization algorithm\n",
        "        parameters_to_optimize = self.net.parameters()\n",
        "        self.optimizer = optim.SGD(parameters_to_optimize, \n",
        "                                   lr=self.LR,\n",
        "                                   momentum=self.MOMENTUM,\n",
        "                                   weight_decay=self.WEIGHT_DECAY)\n",
        "        \n",
        "        # Define the learning rate decaying policy\n",
        "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer,\n",
        "                                                        milestones=self.MILESTONES,\n",
        "                                                        gamma=self.GAMMA)\n",
        "\n",
        "        # Combine the new training data with existing exemplars.\n",
        "        # create exemplars dataset\n",
        "        exemplars_dataset = self.create_exemplars_dataset()\n",
        "        train_dataset_with_exemplars = ConcatDataset([train_dataset, exemplars_dataset])\n",
        "\n",
        "        # Train the network on combined dataset\n",
        "        self.train(train_dataset_with_exemplars, val_dataset) # @todo: include exemplars in validation set?\n",
        "\n",
        "        # Keep a copy of the current network in order to compute its outputs for\n",
        "        # the distillation loss while the new network is being trained.\n",
        "        self.old_net = deepcopy(self.net)\n",
        "\n",
        "    def construct_exemplar_set_rand(self, dataset, m):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            dataset: dataset containing a split (samples from 10 classes) from\n",
        "                which to take exemplars.\n",
        "            m (int): target number of exemplars per class.\n",
        "        \"\"\"\n",
        "\n",
        "        dataset.dataset.disable_transform()\n",
        "\n",
        "        samples = [[] for _ in range(10)]\n",
        "        for image, label in dataset:\n",
        "            label = label % 10 # Map labels to 0-9 range\n",
        "            samples[label].append(image)\n",
        "\n",
        "        dataset.dataset.enable_transform()\n",
        "\n",
        "        exemplars = [[] for _ in range(10)]\n",
        "\n",
        "        for y in range(10):\n",
        "            print(f\"Randomly extracting exemplars from class {y} of current split... \", end=\"\")\n",
        "\n",
        "            # Randomly choose m samples from samples[y]\n",
        "            exemplars[y] = random.sample(samples[y], m)\n",
        "\n",
        "            print(f\"Extracted {len(exemplars[y])} exemplars.\")\n",
        "\n",
        "        return exemplars\n",
        "    \n",
        "    # @todo: herding requires too much time. implement and use construct_exemplar_set_rand instead\n",
        "    def construct_exemplar_set(self, dataset, m):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            dataset: dataset containing a split (samples from 10 classes) from\n",
        "                which to take exemplars.\n",
        "            m (int): target number of exemplars per class.\n",
        "        \"\"\"\n",
        "\n",
        "        # Disable transformations when getting items. We will need original PIL\n",
        "        # images to store in our exemplar set.\n",
        "        dataset.dataset.disable_transform()\n",
        "\n",
        "        # Create a list that stores all images and labels in the dataset by\n",
        "        # their class index in an ordered fashion, structured as follows:\n",
        "        #\n",
        "        # samples = [\n",
        "        #     [image0, image1, ...] # Class 0\n",
        "        #     [image0, image1, ...] # Class 1\n",
        "        #     ...\n",
        "        #     [image0, image1, ...] # Class 9\n",
        "        # ]\n",
        "        samples = [[] for _ in range(10)]\n",
        "        for image, label in dataset:\n",
        "            label = label % 10 # Map labels to 0-9 range\n",
        "            samples[label].append(image)\n",
        "\n",
        "        # Re-enable transformations on the dataset\n",
        "        dataset.dataset.enable_transform()\n",
        "\n",
        "        # Initialize exemplar set\n",
        "        exemplars = [[] for _ in range(10)]\n",
        "        \n",
        "        # Iterate over classes\n",
        "        for y in range(10):\n",
        "            print(f\"Extracting exemplars from class {y} of current split... \", end=\"\")\n",
        "\n",
        "            # Extract features from samples of the current class\n",
        "            samples_features = [self.extract_features(samples[y][i]).to(self.device) for i in range(len(samples[y]))]\n",
        "\n",
        "            # Compute the feature mean of the current class\n",
        "            features_mean = torch.stack(samples_features).mean(dim=0)\n",
        "\n",
        "            # See iCaRL algorithm 4\n",
        "            for k in range(1, m+1): # k = 1, ..., m -- Choose m exemplars\n",
        "                if k == 1: # No exemplars chosen yet, sum to 0 vector\n",
        "                    f_sum = torch.tensor([0 for _ in range(samples_features[0].size(0))])\n",
        "                else: # Sum of features of all exemplars chosen until now (j = 1, ..., k-1)\n",
        "                    exemplars_features = torch.stack([self.extract_features(exemplars[y][j]) for j in range(len(exemplars[y]))]).to(self.device)\n",
        "                    f_sum = exemplars_features.sum(dim=0)\n",
        "                \n",
        "                f_sum = f_sum.to(self.device)\n",
        "\n",
        "                f_arg = torch.norm(features_mean - 1/k * (torch.stack(samples_features) + f_sum), dim=1)\n",
        "\n",
        "                exemplar_idx = torch.argmin(f_arg)\n",
        "\n",
        "                exemplars[y].append(samples[y][exemplar_idx])\n",
        "\n",
        "                # Remove sample from list after adding it to the exemplar set.\n",
        "                # This is done to avoid creating duplicate exemplars.\n",
        "                del samples[y][exemplar_idx]\n",
        "                del samples_features[exemplar_idx]\n",
        "\n",
        "            print(f\"Extracted {len(exemplars[y])} exemplars.\")\n",
        "        \n",
        "        return exemplars\n",
        "\n",
        "    def reduce_exemplar_set(self, exemplar_set, m):\n",
        "        \"\"\"Procedure for removing exemplars from a given set.\n",
        "\n",
        "        Args:\n",
        "            exemplar_set (set): set of exemplars belonging to a certain class.\n",
        "            m (int): target number of exemplars.\n",
        "        \"\"\"\n",
        "\n",
        "        return exemplar_set[:m]\n",
        "\n",
        "    def train(self, train_dataset, val_dataset):\n",
        "        \"\"\"Train the network for a specified number of epochs, and save\n",
        "        the best performing model on the validation set.\n",
        "        \n",
        "        Args:\n",
        "            ...\n",
        "        Returns:\n",
        "            train_loss: loss computed on the last epoch\n",
        "            train_accuracy: accuracy computed on the last epoch\n",
        "            val_loss: average loss on the validation set of the last epoch\n",
        "            val_accuracy: accuracy on the validation set of the last epoch\n",
        "        \"\"\"\n",
        "\n",
        "        # Define the optimization algorithm\n",
        "        parameters_to_optimize = self.net.parameters()\n",
        "        self.optimizer = optim.SGD(parameters_to_optimize, \n",
        "                                   lr=self.LR,\n",
        "                                   momentum=self.MOMENTUM,\n",
        "                                   weight_decay=self.WEIGHT_DECAY)\n",
        "        \n",
        "        # Define the learning rate decaying policy\n",
        "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer,\n",
        "                                                        milestones=self.MILESTONES,\n",
        "                                                        gamma=self.GAMMA)\n",
        "\n",
        "        # Create DataLoaders for training and validation\n",
        "        self.train_dataloader = DataLoader(train_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "        self.val_dataloader = DataLoader(val_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "        # @todo: is the return behaviour intended? (scores of the last epoch)\n",
        "\n",
        "        # Send the old network to the chosen device and set it to evaluation mode\n",
        "        if self.old_net is not None:\n",
        "            self.old_net = self.old_net.to(self.device)\n",
        "            self.old_net.train(False)\n",
        "\n",
        "        # Send the current network to the chosen device\n",
        "        self.net.to(self.device)\n",
        "        cudnn.benchmark  # Calling this optimizes runtime\n",
        "\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.best_epoch = 0\n",
        "\n",
        "        for epoch in range(self.NUM_EPOCHS):\n",
        "            # Run an epoch (start counting form 1)\n",
        "            train_loss, train_accuracy = self.do_epoch(epoch+1)\n",
        "        \n",
        "            # Validate after each epoch \n",
        "            val_loss, val_accuracy = self.validate()    \n",
        "\n",
        "            # Best validation model\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.best_net = deepcopy(self.net)\n",
        "                self.best_epoch = epoch\n",
        "                print(\"Best model updated\")\n",
        "\n",
        "        print(f\"Best model found at epoch {self.best_epoch+1}\")\n",
        "\n",
        "        return (train_loss, train_accuracy,\n",
        "                val_loss, val_accuracy)\n",
        "    \n",
        "    def do_epoch(self, current_epoch):\n",
        "        \"\"\"Trains model for one epoch.\n",
        "        \n",
        "        Args:\n",
        "            current_epoch (int): current epoch number (begins from 1)\n",
        "        Returns:\n",
        "            train_loss: average training loss over all batches of the\n",
        "                current epoch.\n",
        "            train_accuracy: training accuracy of the current epoch over\n",
        "                all samples.\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train()  # Set network in training mode\n",
        "\n",
        "        running_train_loss = 0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        batch_idx = 0\n",
        "\n",
        "        print(f\"Epoch: {current_epoch}, LR: {self.scheduler.get_last_lr()}\")\n",
        "\n",
        "        for images, labels in self.train_dataloader:\n",
        "            loss, corrects = self.do_batch(images, labels)\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "            running_corrects += corrects\n",
        "            total += labels.size(0)\n",
        "            batch_idx += 1\n",
        "\n",
        "        self.scheduler.step()\n",
        "\n",
        "        # Calculate average scores\n",
        "        train_loss = running_train_loss / batch_idx # Average over all batches\n",
        "        train_accuracy = running_corrects / float(total) # Average over all samples\n",
        "\n",
        "        print(f\"Train loss: {train_loss}, Train accuracy: {train_accuracy}\")\n",
        "\n",
        "        return (train_loss, train_accuracy)\n",
        "\n",
        "    def do_batch(self, batch, labels):\n",
        "        batch = batch.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        # Zero-ing the gradients\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        # One-hot encoding of labels of the new training data (new classes)\n",
        "        # Size: batch size (rows) by number of classes seen until now (columns)\n",
        "        #\n",
        "        # e.g., suppose we have four images in a batch, and each incremental\n",
        "        #   step adds three new classes. At the second step, the one-hot\n",
        "        #   encoding may return the following tensor:\n",
        "        #\n",
        "        #       tensor([[0., 0., 0., 1., 0., 0.],   # image 0 (label 3)\n",
        "        #               [0., 0., 0., 0., 1., 0.],   # image 1 (label 4)\n",
        "        #               [0., 0., 0., 0., 0., 1.],   # image 2 (label 5)\n",
        "        #               [0., 0., 0., 0., 1., 0.]])  # image 3 (label 4)\n",
        "        #\n",
        "        #   The first three elements of each vector will always be 0, as the\n",
        "        #   new training batch does not contain images belonging to classes\n",
        "        #   already seen in previous steps.\n",
        "        #\n",
        "        #   The last three elements of each vector will contain the actual\n",
        "        #   information about the class of each image (one-hot encoding of the\n",
        "        #   label). Therefore, we slice the tensor and remove the columns \n",
        "        #   related to old classes (all zeros).# Number of classes seen until now, including new classes\n",
        "        num_classes = self.output_neurons_count() \n",
        "        one_hot_labels = self.to_onehot(labels)[:, num_classes-10:num_classes]\n",
        "\n",
        "        if self.old_net is None: # Network is training for the first time\n",
        "            # We only apply the classification loss\n",
        "            targets = one_hot_labels\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.net(batch)\n",
        "\n",
        "            loss = self.criterion(outputs, targets)\n",
        "\n",
        "        else:\n",
        "            # Old net forward pass. We compute the outputs of the old network\n",
        "            # and apply a sigmoid function. These are used in the distillation\n",
        "            # loss. We discard the output of the new neurons, as they are not\n",
        "            # considered in the distillation loss.\n",
        "            old_net_outputs = sigmoid(self.old_net(batch))[:, :num_classes-10]\n",
        "\n",
        "            # Concatenate the outputs of the old network and the one-hot encoded\n",
        "            # labels along dimension 1 (columns).\n",
        "            # \n",
        "            # Each row refers to an image in the training set, and contains:\n",
        "            # - the output of the old network for that image, used by the\n",
        "            #   distillation loss\n",
        "            # - the one-hot label of the image, used by the classification loss\n",
        "            targets = torch.cat((old_net_outputs, one_hot_labels), dim=1)\n",
        "\n",
        "            # New net forward pass\n",
        "            # Size: batch size (rows) by number of classes seen until now, \n",
        "            # including new ones (columns)\n",
        "            outputs = self.net(batch)\n",
        "\n",
        "            loss = self.criterion(outputs, targets)\n",
        "\n",
        "        # Get predictions\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Accuracy over NEW IMAGES, not over all images\n",
        "        running_corrects = \\\n",
        "            torch.sum(preds == labels.data).data.item() \n",
        "\n",
        "        # Backward pass: computes gradients\n",
        "        loss.backward()\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss, running_corrects\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\"Validate the model.\n",
        "        \n",
        "        Returns:\n",
        "            val_loss: average loss function computed on the network outputs\n",
        "                of the validation set (val_dataloader).\n",
        "            val_accuracy: accuracy computed on the validation set.\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "\n",
        "        running_val_loss = 0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        batch_idx = 0\n",
        "\n",
        "        for images, labels in self.val_dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # One hot encoding of new task labels \n",
        "            one_hot_labels = self.to_onehot(labels)\n",
        "\n",
        "            # New net forward pass\n",
        "            outputs = self.net(images)  \n",
        "            loss = self.criterion(outputs, one_hot_labels) # BCE Loss with sigmoids over outputs\n",
        "\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update the number of correctly classified validation samples\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            batch_idx += 1\n",
        "\n",
        "        # Calculate scores\n",
        "        val_loss = running_val_loss / batch_idx\n",
        "        val_accuracy = running_corrects / float(total)\n",
        "\n",
        "        print(f\"Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n",
        "\n",
        "        return val_loss, val_accuracy\n",
        "\n",
        "    def test(self, test_dataset):\n",
        "        \"\"\"Test the model.\n",
        "\n",
        "        Returns:\n",
        "            accuracy (float): accuracy of the model on the test set\n",
        "        \"\"\"\n",
        "\n",
        "        self.best_net.train(False)  # Set Network to evaluation mode\n",
        "\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "\n",
        "        print(\"Caching mean of exemplars...\")\n",
        "        self.cache_mean_of_exemplars()\n",
        "        \n",
        "        print(\"Testing... \", end=\"\")\n",
        "        for image, label in DataLoader(test_dataset):\n",
        "            image = image.to(self.device)\n",
        "            label = label.to(self.device)\n",
        "\n",
        "            total += 1\n",
        "            \n",
        "            # PROGRESS\n",
        "            if total % 100 == 0:\n",
        "                print(\"#\", end=\"\")\n",
        "\n",
        "            # Classify image of the batch and update corrects\n",
        "            pred = self.classify(image)\n",
        "\n",
        "            if pred == label:\n",
        "                running_corrects += 1\n",
        "\n",
        "        self.clear_cache_mean_of_exemplars()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = running_corrects / float(total)  \n",
        "\n",
        "        print(f\"\\nTest accuracy: {accuracy}\")\n",
        "\n",
        "        return accuracy\n",
        "    \n",
        "    def increment_classes(self, n=10):\n",
        "        \"\"\"Add n classes in the final fully connected layer.\"\"\"\n",
        "\n",
        "        in_features = self.net.fc.in_features  # size of each input sample\n",
        "        out_features = self.net.fc.out_features  # size of each output sample\n",
        "        weight = self.net.fc.weight.data\n",
        "\n",
        "        self.net.fc = nn.Linear(in_features, out_features+n)\n",
        "        self.net.fc.weight.data[:out_features] = weight\n",
        "    \n",
        "    def output_neurons_count(self):\n",
        "        \"\"\"Return the number of output neurons of the current network.\"\"\"\n",
        "\n",
        "        return self.net.fc.out_features\n",
        "    \n",
        "    def feature_neurons_count(self):\n",
        "        \"\"\"Return the number of neurons of the last layer of the feature extractor.\"\"\"\n",
        "\n",
        "        return self.net.fc.in_features\n",
        "    \n",
        "    def to_onehot(self, targets): \n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            targets: dataloader.dataset.targets of the new task images\n",
        "        \"\"\"\n",
        "        num_classes = self.net.fc.out_features\n",
        "        one_hot_targets = torch.eye(num_classes)[targets]\n",
        "\n",
        "        return one_hot_targets.to(self.device)\n",
        "    \n",
        "    def create_exemplars_dataset(self):\n",
        "        exemplars_dataset = Exemplars(transform=self.test_transform)\n",
        "\n",
        "        for y in range(len(self.exemplars)): # Iterate over classes\n",
        "            for i in range(len(self.exemplars[y])): # Iterate over elements\n",
        "                exemplars_dataset.add_exemplar(self.exemplars[y][i], y)\n",
        "        \n",
        "        return exemplars_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF0ypxGognNR",
        "colab_type": "text"
      },
      "source": [
        "### Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4mf04UjEgmPG",
        "colab": {}
      },
      "source": [
        "# Transformations for Learning Without Forgetting\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y9Oq44dxgmPN",
        "outputId": "a92450a6-c2e8-4ce2-e431-3231b5f5fff9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "a7992569bcf043908512003e1288b919",
            "57b29fe6f03445ad8dfca9df4c8e0dca",
            "4ba3a4c50b0b4282bc25265169c04f10",
            "1cfab0cab132415a9d26bcc312d2db48",
            "de0e468df55f4a1497beb3331457ec8c",
            "fbc309f26beb48c5a1412ad75f2c1234",
            "2b9373ee94674c77979a2e4eea50b566",
            "5344bb9d3a3549ae89d948480c5cf695"
          ]
        }
      },
      "source": [
        "train_subsets = [[] for i in range(NUM_RUNS)]\n",
        "val_subsets = [[] for i in range(NUM_RUNS)]\n",
        "test_subsets = [[] for i in range(NUM_RUNS)]\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "    for split_i in range(CLASS_BATCH_SIZE):\n",
        "        if run_i+split_i == 0: # Download dataset only at first instantiation\n",
        "            download = True\n",
        "        else:\n",
        "            download = False\n",
        "\n",
        "        # Create CIFAR100 dataset\n",
        "        train_dataset = Cifar100(DATA_DIR, train=True, download=download, random_state=RANDOM_STATE+run_i, transform=train_transform)\n",
        "        test_dataset = Cifar100(DATA_DIR, train=False, download=False, random_state=RANDOM_STATE+run_i, transform=test_transform)\n",
        "    \n",
        "        # Subspace of CIFAR100 of 10 classes\n",
        "        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i]) \n",
        "        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n",
        "\n",
        "        # Define train and validation indices\n",
        "        train_indices, val_indices = train_dataset.train_val_split(VAL_SIZE, RANDOM_STATE)\n",
        "\n",
        "        # Define subsets\n",
        "        train_subsets[run_i].append(Subset(train_dataset, train_indices))\n",
        "        val_subsets[run_i].append(Subset(train_dataset, val_indices))\n",
        "        test_subsets[run_i].append(test_dataset)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7992569bcf043908512003e1288b919",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmCpRMBKgvDB",
        "colab_type": "text"
      },
      "source": [
        "### Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nRU--zYjmZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# iCaRL hyperparameters\n",
        "LR = 2\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.00001\n",
        "MILESTONES = [49, 63]\n",
        "GAMMA = 0.2\n",
        "NUM_EPOCHS = 70\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrIjsrtSXtMx",
        "colab_type": "code",
        "outputId": "b44c83dc-dd47-4ae5-906b-fa29b7d12a88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Initialize logs\n",
        "logs = []\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "    net = resnet32()\n",
        "    icarl = iCaRL(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS, BATCH_SIZE, train_transform, test_transform)\n",
        "\n",
        "    for split_i in range(10):\n",
        "        print(f\"## Split {split_i} of run {run_i} ##\")\n",
        "        \n",
        "        icarl.incremental_train(split_i, train_subsets[run_i][split_i], val_subsets[run_i][split_i])\n",
        "        icarl.test(test_subsets[run_i][split_i])\n",
        "\n",
        "        # @todo: implement logging and plots"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "## Split 0 of run 0 ##\n",
            "Epoch: 1, LR: [2]\n",
            "Train loss: 0.4107112492833819, Train accuracy: 0.12611607142857142\n",
            "Validation loss: 0.5764664610226949, Validation accuracy: 0.14322916666666666\n",
            "Best model updated\n",
            "Epoch: 2, LR: [2]\n",
            "Train loss: 0.31602041295596534, Train accuracy: 0.15647321428571428\n",
            "Validation loss: 0.3176279564698537, Validation accuracy: 0.1640625\n",
            "Best model updated\n",
            "Epoch: 3, LR: [2]\n",
            "Train loss: 0.3132268394742693, Train accuracy: 0.15290178571428573\n",
            "Validation loss: 0.3172307014465332, Validation accuracy: 0.15364583333333334\n",
            "Best model updated\n",
            "Epoch: 4, LR: [2]\n",
            "Train loss: 0.3112973264285496, Train accuracy: 0.18080357142857142\n",
            "Validation loss: 0.3161837657292684, Validation accuracy: 0.140625\n",
            "Best model updated\n",
            "Epoch: 5, LR: [2]\n",
            "Train loss: 0.30597354088510786, Train accuracy: 0.19732142857142856\n",
            "Validation loss: 0.31889159480730694, Validation accuracy: 0.15885416666666666\n",
            "Epoch: 6, LR: [2]\n",
            "Train loss: 0.2953731954097748, Train accuracy: 0.2234375\n",
            "Validation loss: 0.295241783062617, Validation accuracy: 0.23958333333333334\n",
            "Best model updated\n",
            "Epoch: 7, LR: [2]\n",
            "Train loss: 0.28970831292016164, Train accuracy: 0.25669642857142855\n",
            "Validation loss: 0.28833138942718506, Validation accuracy: 0.2552083333333333\n",
            "Best model updated\n",
            "Epoch: 8, LR: [2]\n",
            "Train loss: 0.28348967858723234, Train accuracy: 0.27433035714285714\n",
            "Validation loss: 0.27842562397321063, Validation accuracy: 0.3177083333333333\n",
            "Best model updated\n",
            "Epoch: 9, LR: [2]\n",
            "Train loss: 0.27965633187975203, Train accuracy: 0.3046875\n",
            "Validation loss: 0.2841976781686147, Validation accuracy: 0.265625\n",
            "Epoch: 10, LR: [2]\n",
            "Train loss: 0.27157952189445494, Train accuracy: 0.32276785714285716\n",
            "Validation loss: 0.2692622145016988, Validation accuracy: 0.3515625\n",
            "Best model updated\n",
            "Epoch: 11, LR: [2]\n",
            "Train loss: 0.2681494423321315, Train accuracy: 0.346875\n",
            "Validation loss: 0.2839881082375844, Validation accuracy: 0.2942708333333333\n",
            "Epoch: 12, LR: [2]\n",
            "Train loss: 0.2648610093763896, Train accuracy: 0.36495535714285715\n",
            "Validation loss: 0.2624234656492869, Validation accuracy: 0.3671875\n",
            "Best model updated\n",
            "Epoch: 13, LR: [2]\n",
            "Train loss: 0.2611625054052898, Train accuracy: 0.38392857142857145\n",
            "Validation loss: 0.27316481868426007, Validation accuracy: 0.3697916666666667\n",
            "Epoch: 14, LR: [2]\n",
            "Train loss: 0.2572452596255711, Train accuracy: 0.38839285714285715\n",
            "Validation loss: 0.2615224818388621, Validation accuracy: 0.40625\n",
            "Best model updated\n",
            "Epoch: 15, LR: [2]\n",
            "Train loss: 0.24914730489253997, Train accuracy: 0.4185267857142857\n",
            "Validation loss: 0.28765106201171875, Validation accuracy: 0.3671875\n",
            "Epoch: 16, LR: [2]\n",
            "Train loss: 0.24482505917549133, Train accuracy: 0.4426339285714286\n",
            "Validation loss: 0.2574913402398427, Validation accuracy: 0.4192708333333333\n",
            "Best model updated\n",
            "Epoch: 17, LR: [2]\n",
            "Train loss: 0.24138778277805872, Train accuracy: 0.45357142857142857\n",
            "Validation loss: 0.24352522691090903, Validation accuracy: 0.4583333333333333\n",
            "Best model updated\n",
            "Epoch: 18, LR: [2]\n",
            "Train loss: 0.23798537424632482, Train accuracy: 0.45558035714285716\n",
            "Validation loss: 0.235903466741244, Validation accuracy: 0.4895833333333333\n",
            "Best model updated\n",
            "Epoch: 19, LR: [2]\n",
            "Train loss: 0.23231981268950871, Train accuracy: 0.4796875\n",
            "Validation loss: 0.24217523137728372, Validation accuracy: 0.4505208333333333\n",
            "Epoch: 20, LR: [2]\n",
            "Train loss: 0.2284110631261553, Train accuracy: 0.48683035714285716\n",
            "Validation loss: 0.22815379003683725, Validation accuracy: 0.5078125\n",
            "Best model updated\n",
            "Epoch: 21, LR: [2]\n",
            "Train loss: 0.2188004148857934, Train accuracy: 0.5165178571428571\n",
            "Validation loss: 0.2701651453971863, Validation accuracy: 0.4895833333333333\n",
            "Epoch: 22, LR: [2]\n",
            "Train loss: 0.21545761270182473, Train accuracy: 0.5238839285714286\n",
            "Validation loss: 0.23685839772224426, Validation accuracy: 0.4635416666666667\n",
            "Epoch: 23, LR: [2]\n",
            "Train loss: 0.21089696713856287, Train accuracy: 0.5479910714285714\n",
            "Validation loss: 0.21524500846862793, Validation accuracy: 0.5572916666666666\n",
            "Best model updated\n",
            "Epoch: 24, LR: [2]\n",
            "Train loss: 0.2054935553244182, Train accuracy: 0.5622767857142857\n",
            "Validation loss: 0.24057529866695404, Validation accuracy: 0.4583333333333333\n",
            "Epoch: 25, LR: [2]\n",
            "Train loss: 0.20337276331015997, Train accuracy: 0.5600446428571428\n",
            "Validation loss: 0.20641638338565826, Validation accuracy: 0.5729166666666666\n",
            "Best model updated\n",
            "Epoch: 26, LR: [2]\n",
            "Train loss: 0.19555208001817975, Train accuracy: 0.5899553571428572\n",
            "Validation loss: 0.1846154679854711, Validation accuracy: 0.5989583333333334\n",
            "Best model updated\n",
            "Epoch: 27, LR: [2]\n",
            "Train loss: 0.18781637166227613, Train accuracy: 0.5924107142857142\n",
            "Validation loss: 0.2310168296098709, Validation accuracy: 0.5130208333333334\n",
            "Epoch: 28, LR: [2]\n",
            "Train loss: 0.1858457999570029, Train accuracy: 0.6100446428571429\n",
            "Validation loss: 0.18772140642007193, Validation accuracy: 0.6015625\n",
            "Epoch: 29, LR: [2]\n",
            "Train loss: 0.18588350670678275, Train accuracy: 0.6060267857142857\n",
            "Validation loss: 0.19422362744808197, Validation accuracy: 0.5546875\n",
            "Epoch: 30, LR: [2]\n",
            "Train loss: 0.177958550623485, Train accuracy: 0.6274553571428572\n",
            "Validation loss: 0.21535570919513702, Validation accuracy: 0.546875\n",
            "Epoch: 31, LR: [2]\n",
            "Train loss: 0.17473829771791186, Train accuracy: 0.6339285714285714\n",
            "Validation loss: 0.1634371280670166, Validation accuracy: 0.6536458333333334\n",
            "Best model updated\n",
            "Epoch: 32, LR: [2]\n",
            "Train loss: 0.16799057636942183, Train accuracy: 0.6470982142857142\n",
            "Validation loss: 0.1890942504008611, Validation accuracy: 0.6197916666666666\n",
            "Epoch: 33, LR: [2]\n",
            "Train loss: 0.16135428547859193, Train accuracy: 0.6678571428571428\n",
            "Validation loss: 0.19095260898272196, Validation accuracy: 0.5807291666666666\n",
            "Epoch: 34, LR: [2]\n",
            "Train loss: 0.15861468144825527, Train accuracy: 0.6830357142857143\n",
            "Validation loss: 0.1964500347773234, Validation accuracy: 0.59375\n",
            "Epoch: 35, LR: [2]\n",
            "Train loss: 0.15949597081967762, Train accuracy: 0.678125\n",
            "Validation loss: 0.19256218274434408, Validation accuracy: 0.6041666666666666\n",
            "Epoch: 36, LR: [2]\n",
            "Train loss: 0.16308876488889967, Train accuracy: 0.6680803571428572\n",
            "Validation loss: 0.2156956891218821, Validation accuracy: 0.5442708333333334\n",
            "Epoch: 37, LR: [2]\n",
            "Train loss: 0.15495508398328509, Train accuracy: 0.6848214285714286\n",
            "Validation loss: 0.1647128015756607, Validation accuracy: 0.6510416666666666\n",
            "Epoch: 38, LR: [2]\n",
            "Train loss: 0.15008887542145594, Train accuracy: 0.6966517857142858\n",
            "Validation loss: 0.17557711402575174, Validation accuracy: 0.65625\n",
            "Epoch: 39, LR: [2]\n",
            "Train loss: 0.14157057063920156, Train accuracy: 0.7176339285714286\n",
            "Validation loss: 0.16600369910399118, Validation accuracy: 0.6510416666666666\n",
            "Epoch: 40, LR: [2]\n",
            "Train loss: 0.14331534760338918, Train accuracy: 0.7104910714285714\n",
            "Validation loss: 0.1462148999174436, Validation accuracy: 0.6901041666666666\n",
            "Best model updated\n",
            "Epoch: 41, LR: [2]\n",
            "Train loss: 0.1351597349558558, Train accuracy: 0.7316964285714286\n",
            "Validation loss: 0.16903951267401376, Validation accuracy: 0.65625\n",
            "Epoch: 42, LR: [2]\n",
            "Train loss: 0.12888651915958949, Train accuracy: 0.7470982142857143\n",
            "Validation loss: 0.15566749374071756, Validation accuracy: 0.6927083333333334\n",
            "Epoch: 43, LR: [2]\n",
            "Train loss: 0.12663051698889052, Train accuracy: 0.7549107142857143\n",
            "Validation loss: 0.21443509062131247, Validation accuracy: 0.6380208333333334\n",
            "Epoch: 44, LR: [2]\n",
            "Train loss: 0.12391597394432341, Train accuracy: 0.7622767857142857\n",
            "Validation loss: 0.15223693350950876, Validation accuracy: 0.7135416666666666\n",
            "Epoch: 45, LR: [2]\n",
            "Train loss: 0.1198174815092768, Train accuracy: 0.771875\n",
            "Validation loss: 0.16996076703071594, Validation accuracy: 0.671875\n",
            "Epoch: 46, LR: [2]\n",
            "Train loss: 0.12170712011201042, Train accuracy: 0.753125\n",
            "Validation loss: 0.14489529033501944, Validation accuracy: 0.71875\n",
            "Best model updated\n",
            "Epoch: 47, LR: [2]\n",
            "Train loss: 0.11494316905736923, Train accuracy: 0.7814732142857143\n",
            "Validation loss: 0.1466601441303889, Validation accuracy: 0.6979166666666666\n",
            "Epoch: 48, LR: [2]\n",
            "Train loss: 0.10939618391650063, Train accuracy: 0.7863839285714286\n",
            "Validation loss: 0.162350927790006, Validation accuracy: 0.6770833333333334\n",
            "Epoch: 49, LR: [2]\n",
            "Train loss: 0.1084424723471914, Train accuracy: 0.7917410714285714\n",
            "Validation loss: 0.1612375577290853, Validation accuracy: 0.7005208333333334\n",
            "Epoch: 50, LR: [0.4]\n",
            "Train loss: 0.09253216300691877, Train accuracy: 0.8294642857142858\n",
            "Validation loss: 0.10918084283669789, Validation accuracy: 0.7838541666666666\n",
            "Best model updated\n",
            "Epoch: 51, LR: [0.4]\n",
            "Train loss: 0.07765994231615747, Train accuracy: 0.8587053571428571\n",
            "Validation loss: 0.1184528296192487, Validation accuracy: 0.7708333333333334\n",
            "Epoch: 52, LR: [0.4]\n",
            "Train loss: 0.07205566614866257, Train accuracy: 0.8705357142857143\n",
            "Validation loss: 0.11172980070114136, Validation accuracy: 0.7890625\n",
            "Epoch: 53, LR: [0.4]\n",
            "Train loss: 0.06974913797208242, Train accuracy: 0.8709821428571428\n",
            "Validation loss: 0.09950268516937892, Validation accuracy: 0.8255208333333334\n",
            "Best model updated\n",
            "Epoch: 54, LR: [0.4]\n",
            "Train loss: 0.0671040803194046, Train accuracy: 0.88125\n",
            "Validation loss: 0.11676251391569774, Validation accuracy: 0.7864583333333334\n",
            "Epoch: 55, LR: [0.4]\n",
            "Train loss: 0.06478628367185593, Train accuracy: 0.8857142857142857\n",
            "Validation loss: 0.11445943762858708, Validation accuracy: 0.7786458333333334\n",
            "Epoch: 56, LR: [0.4]\n",
            "Train loss: 0.06332516255123274, Train accuracy: 0.8912946428571429\n",
            "Validation loss: 0.1149911880493164, Validation accuracy: 0.7942708333333334\n",
            "Epoch: 57, LR: [0.4]\n",
            "Train loss: 0.062359988795859474, Train accuracy: 0.890625\n",
            "Validation loss: 0.11078329632679622, Validation accuracy: 0.8098958333333334\n",
            "Epoch: 58, LR: [0.4]\n",
            "Train loss: 0.058763446020228524, Train accuracy: 0.8973214285714286\n",
            "Validation loss: 0.11238476137320201, Validation accuracy: 0.7890625\n",
            "Epoch: 59, LR: [0.4]\n",
            "Train loss: 0.056723769967045104, Train accuracy: 0.903125\n",
            "Validation loss: 0.10982158780097961, Validation accuracy: 0.8046875\n",
            "Epoch: 60, LR: [0.4]\n",
            "Train loss: 0.05474222568528993, Train accuracy: 0.9046875\n",
            "Validation loss: 0.12938473870356879, Validation accuracy: 0.78125\n",
            "Epoch: 61, LR: [0.4]\n",
            "Train loss: 0.05413185326116426, Train accuracy: 0.9026785714285714\n",
            "Validation loss: 0.12010755638281505, Validation accuracy: 0.8020833333333334\n",
            "Epoch: 62, LR: [0.4]\n",
            "Train loss: 0.04900989096079554, Train accuracy: 0.9174107142857143\n",
            "Validation loss: 0.11959772557020187, Validation accuracy: 0.78125\n",
            "Epoch: 63, LR: [0.4]\n",
            "Train loss: 0.04976448959537915, Train accuracy: 0.9104910714285714\n",
            "Validation loss: 0.1254230464498202, Validation accuracy: 0.765625\n",
            "Epoch: 64, LR: [0.08000000000000002]\n",
            "Train loss: 0.042582470976880615, Train accuracy: 0.9319196428571429\n",
            "Validation loss: 0.11326456318298976, Validation accuracy: 0.8072916666666666\n",
            "Epoch: 65, LR: [0.08000000000000002]\n",
            "Train loss: 0.03799030759504863, Train accuracy: 0.9417410714285714\n",
            "Validation loss: 0.11454403400421143, Validation accuracy: 0.7890625\n",
            "Epoch: 66, LR: [0.08000000000000002]\n",
            "Train loss: 0.036605824742998394, Train accuracy: 0.940625\n",
            "Validation loss: 0.10736020654439926, Validation accuracy: 0.8125\n",
            "Epoch: 67, LR: [0.08000000000000002]\n",
            "Train loss: 0.03660504296422005, Train accuracy: 0.9446428571428571\n",
            "Validation loss: 0.1067924698193868, Validation accuracy: 0.8151041666666666\n",
            "Epoch: 68, LR: [0.08000000000000002]\n",
            "Train loss: 0.0364251864275762, Train accuracy: 0.9455357142857143\n",
            "Validation loss: 0.10439747820297877, Validation accuracy: 0.8098958333333334\n",
            "Epoch: 69, LR: [0.08000000000000002]\n",
            "Train loss: 0.0353454771318606, Train accuracy: 0.9462053571428571\n",
            "Validation loss: 0.11359147230784099, Validation accuracy: 0.7994791666666666\n",
            "Epoch: 70, LR: [0.08000000000000002]\n",
            "Train loss: 0.03339368291199207, Train accuracy: 0.9506696428571428\n",
            "Validation loss: 0.11850465337435405, Validation accuracy: 0.796875\n",
            "Best model found at epoch 53\n",
            "Target number of exemplars per class: 200\n",
            "Total number of exemplars: 2000\n",
            "Randomly extracting exemplars from class 0 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 1 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 2 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 3 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 4 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 5 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 6 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 7 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 8 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 9 of current split... Extracted 200 exemplars.\n",
            "Caching mean of exemplars...\n",
            "Testing... ##########\n",
            "Test accuracy: 0.768\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}